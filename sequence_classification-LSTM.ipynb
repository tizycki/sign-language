{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "from utils.data_augmentation import rotate_keypoints_sequence\n",
    "from utils.keypoints import read_keypoints, rescale_keypoints, keypoints_sequence_padding\n",
    "\n",
    "from model.datasets import SequenceKeypointsDataset\n",
    "from model.transforms import RotateKeypointsSequence, KeypointsSequencePadding\n",
    "from model.models import SequenceRecognitionNetLSTM\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILES_PATH = [\n",
    "    './data/labels/sequence/sequence_labelsSep-16-2020_0516.json',\n",
    "    './data/labels/sequence/sequence_labelsSep-16-2020_1930.json',\n",
    "    './data/labels/sequence/sequence_labelsSep-18-2020_0447.json',\n",
    "    './data/labels/sequence/sequence_labelsSep-18-2020_2351.json'\n",
    "]\n",
    "JSON_KEYPOINTS_BASE_PATH = './data/keypoints'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "N_EPOCHS = 2000\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.00001\n",
    "\n",
    "LABEL_OCCURENCES_MIN = 10\n",
    "SEQUENCE_LENGTH_MAX = 50\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "label_map = {\n",
    "    'Meet': 0,\n",
    "    'Name': 1,\n",
    "    'Good day': 2,\n",
    "    'See you around': 3,\n",
    "    'Thank you': 4,\n",
    "    'Hello': 5,\n",
    "    'Bye bye': 6,\n",
    "    'Tom': 7,\n",
    "    'Nice': 8,\n",
    "    'You': 9,\n",
    "    'My': 10\n",
    "} \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>keypoints_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_path  \\\n",
       "0  ./data/rendered_video/internal_resource_fps10/...   \n",
       "1  ./data/rendered_video/internal_resource_fps10/...   \n",
       "2  ./data/rendered_video/internal_resource_fps10/...   \n",
       "3  ./data/rendered_video/internal_resource_fps10/...   \n",
       "4  ./data/rendered_video/internal_resource_fps10/...   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "1  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "2  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "3  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "4  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "\n",
       "                                      keypoints_path  label  label_id  \n",
       "0  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "1  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "2  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "3  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "4  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack label files\n",
    "labels_json = []\n",
    "\n",
    "for label_file in JSON_FILES_PATH:\n",
    "    with open(label_file, 'r') as f:\n",
    "        for row in f.readlines():\n",
    "            labels_json.append(json.loads(row))\n",
    "\n",
    "            \n",
    "# Fix labels\n",
    "            \n",
    "# Convert into DataFrame\n",
    "df = pd.concat(\n",
    "    [pd.DataFrame(item) for item in labels_json],\n",
    "    axis=0\n",
    ").drop(columns=['id'])\n",
    "\n",
    "# Drop ignore labels\n",
    "df = df[~df.label.isin(['ignore', '<random movements>', 'My name'])]\n",
    "\n",
    "# Drop low freq classes\n",
    "labels_freq = df.label.value_counts()\n",
    "low_freq_labels = labels_freq[(labels_freq < LABEL_OCCURENCES_MIN)].index.tolist()\n",
    "df = df[~(df.label.isin(low_freq_labels))]\n",
    "\n",
    "df = df[df.label.isin(list(label_map.keys()))]\n",
    "\n",
    "# Get label ID\n",
    "df['label_id'] = df.label.map(label_map)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(863, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You               109\n",
      "My                102\n",
      "Name               88\n",
      "Meet               87\n",
      "Nice               79\n",
      "Tom                78\n",
      "Hello              74\n",
      "Good day           72\n",
      "See you around     65\n",
      "Thank you          65\n",
      "Bye bye            44\n",
      "Name: label, dtype: int64\n",
      "9     109\n",
      "10    102\n",
      "1      88\n",
      "0      87\n",
      "8      79\n",
      "7      78\n",
      "5      74\n",
      "2      72\n",
      "4      65\n",
      "3      65\n",
      "6      44\n",
      "Name: label_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.label.value_counts())\n",
    "print(df.label_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(df_train):\n",
    "    class_sample_count = np.array([len(np.where(df_train.label_id==t)[0]) for t in np.unique(df_train.label_id)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in df_train.label_id])\n",
    "\n",
    "    return torch.from_numpy(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Create Transformer\n",
    "keypoints_sequence_transform_train = transforms.Compose([\n",
    "    RotateKeypointsSequence(-30, 30),\n",
    "    KeypointsSequencePadding(SEQUENCE_LENGTH_MAX)\n",
    "])\n",
    "keypoints_sequence_transform_test = transforms.Compose([\n",
    "    KeypointsSequencePadding(SEQUENCE_LENGTH_MAX)\n",
    "])\n",
    "\n",
    "# Initialize Datasets\n",
    "ds_train = SequenceKeypointsDataset(df_train, keypoints_sequence_transform_train)\n",
    "ds_test = SequenceKeypointsDataset(df_test, keypoints_sequence_transform_test)\n",
    "\n",
    "# Initialize Sampler\n",
    "sampler = torch.utils.data.WeightedRandomSampler(get_weights(df_train), df_train.shape[0])\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_dl = torch.utils.data.DataLoader(ds_train, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "test_dl = torch.utils.data.DataLoader(ds_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Model params\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = SequenceRecognitionNetLSTM(\n",
    "    out_class_num=NUM_CLASSES, \n",
    "    input_size=5700,\n",
    "    hidden_size=1024,\n",
    "    num_layers=8, \n",
    "    dropout=0\n",
    ").to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "You               87\n",
       "My                83\n",
       "Name              70\n",
       "Meet              68\n",
       "Tom               66\n",
       "Nice              65\n",
       "Hello             59\n",
       "See you around    56\n",
       "Thank you         54\n",
       "Good day          51\n",
       "Bye bye           31\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "You               22\n",
       "Good day          21\n",
       "My                19\n",
       "Meet              19\n",
       "Name              18\n",
       "Hello             15\n",
       "Nice              14\n",
       "Bye bye           13\n",
       "Tom               12\n",
       "Thank you         11\n",
       "See you around     9\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, data_loader, print_info=False):\n",
    "    \n",
    "    collect_results = []\n",
    "    collect_targets = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        X, y = batch\n",
    "        X = X.float().to(DEVICE)\n",
    "        y = y.float().to(DEVICE).detach().cpu().numpy()\n",
    "\n",
    "        pred = model(X)\n",
    "        pred = F.softmax(pred, dim=0)\n",
    "        collect_results.append(pred.detach().cpu().numpy())\n",
    "        collect_targets.append(y)\n",
    "\n",
    "    preds_proba = np.concatenate(collect_results)\n",
    "    preds = preds_proba.argmax(axis=1)\n",
    "    targets = np.concatenate(collect_targets)\n",
    "\n",
    "    ll = log_loss(targets, preds_proba)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "\n",
    "    \n",
    "    if print_info:\n",
    "        print(\"test log-loss: {}\".format(ll))\n",
    "        print(\"overall accuracy:  {}\".format(acc))\n",
    "\n",
    "    return ll, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH=1\n",
      "train: loss=2.397895336151123 acc=0.08405797101449275\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=2\n",
      "train: loss=2.397895336151123 acc=0.08840579710144927\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=3\n",
      "train: loss=2.397895336151123 acc=0.08550724637681159\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=4\n",
      "train: loss=2.397895336151123 acc=0.06376811594202898\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=5\n",
      "train: loss=2.397895336151123 acc=0.09710144927536232\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=6\n",
      "train: loss=2.397895336151123 acc=0.07971014492753623\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=7\n",
      "train: loss=2.397895336151123 acc=0.08695652173913043\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=8\n",
      "train: loss=2.397895336151123 acc=0.08550724637681159\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=9\n",
      "train: loss=2.397895336151123 acc=0.06956521739130435\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=10\n",
      "train: loss=2.397895336151123 acc=0.08695652173913043\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=11\n",
      "train: loss=2.397895336151123 acc=0.08695652173913043\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=12\n",
      "train: loss=2.397895336151123 acc=0.09710144927536232\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=13\n",
      "train: loss=2.397895336151123 acc=0.09710144927536232\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=14\n",
      "train: loss=2.397895336151123 acc=0.0782608695652174\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=15\n",
      "train: loss=2.397895336151123 acc=0.0927536231884058\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=16\n",
      "train: loss=2.3978953354600545 acc=0.09420289855072464\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=17\n",
      "train: loss=2.397895336151123 acc=0.07246376811594203\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=18\n",
      "train: loss=2.3978953351145207 acc=0.09855072463768116\n",
      "test: loss=2.397895334077918 acc=0.10289855072463767\n",
      "EPOCH=19\n",
      "train: loss=2.39789533338685 acc=0.1\n",
      "test: loss=2.397895333732384 acc=0.10289855072463767\n",
      "EPOCH=20\n",
      "train: loss=2.3978953288949056 acc=0.10289855072463767\n",
      "test: loss=2.3978953326957813 acc=0.11159420289855072\n",
      "EPOCH=21\n",
      "train: loss=2.3978953247484953 acc=0.1318840579710145\n",
      "test: loss=2.397895329931508 acc=0.11884057971014493\n",
      "EPOCH=22\n",
      "train: loss=2.3978953275127686 acc=0.11594202898550725\n",
      "test: loss=2.397895323711893 acc=0.14782608695652175\n",
      "EPOCH=23\n",
      "train: loss=2.397895320256551 acc=0.1289855072463768\n",
      "test: loss=2.3978953257850977 acc=0.14492753623188406\n",
      "EPOCH=24\n",
      "train: loss=2.397895316455675 acc=0.13478260869565217\n",
      "test: loss=2.397895318183346 acc=0.15797101449275364\n",
      "EPOCH=25\n",
      "train: loss=2.3978952991789666 acc=0.16521739130434782\n",
      "test: loss=2.3978953043619793 acc=0.1681159420289855\n",
      "EPOCH=26\n",
      "train: loss=2.397895272572835 acc=0.19420289855072465\n",
      "test: loss=2.3978952715362327 acc=0.21304347826086956\n",
      "EPOCH=27\n",
      "train: loss=2.3978952148686283 acc=0.24202898550724639\n",
      "test: loss=2.3978952445845674 acc=0.20869565217391303\n",
      "EPOCH=28\n",
      "train: loss=2.397895192063373 acc=0.23043478260869565\n",
      "test: loss=2.397895189644634 acc=0.24782608695652175\n",
      "EPOCH=29\n",
      "train: loss=2.3978951789330747 acc=0.23043478260869565\n",
      "test: loss=2.397895192063373 acc=0.21594202898550724\n",
      "EPOCH=30\n",
      "train: loss=2.397895149908204 acc=0.2391304347826087\n",
      "test: loss=2.3978951661483103 acc=0.2217391304347826\n",
      "EPOCH=31\n",
      "train: loss=2.397895158201024 acc=0.22318840579710145\n",
      "test: loss=2.3978951620019 acc=0.22318840579710145\n",
      "EPOCH=32\n",
      "train: loss=2.3978951426519863 acc=0.22753623188405797\n",
      "test: loss=2.397895138160042 acc=0.2579710144927536\n",
      "EPOCH=33\n",
      "train: loss=2.397895135395769 acc=0.24782608695652175\n",
      "test: loss=2.397895137814508 acc=0.23768115942028986\n",
      "EPOCH=34\n",
      "train: loss=2.3978951364323713 acc=0.2536231884057971\n",
      "test: loss=2.3978951367779056 acc=0.2536231884057971\n",
      "EPOCH=35\n",
      "train: loss=2.3978951167369233 acc=0.24782608695652175\n",
      "test: loss=2.3978951208833337 acc=0.2594202898550725\n",
      "EPOCH=36\n",
      "train: loss=2.3978950911673946 acc=0.1826086956521739\n",
      "test: loss=2.397895099460215 acc=0.2072463768115942\n",
      "EPOCH=37\n",
      "train: loss=2.3978950704353443 acc=0.18115942028985507\n",
      "test: loss=2.397895085984382 acc=0.1681159420289855\n",
      "EPOCH=38\n",
      "train: loss=2.397895051776499 acc=0.15942028985507245\n",
      "test: loss=2.397895067671071 acc=0.11739130434782609\n",
      "EPOCH=39\n",
      "train: loss=2.397894994072292 acc=0.19130434782608696\n",
      "test: loss=2.397895035190859 acc=0.14202898550724638\n",
      "EPOCH=40\n",
      "train: loss=2.397894909761954 acc=0.18840579710144928\n",
      "test: loss=2.3978949501894524 acc=0.14492753623188406\n",
      "EPOCH=41\n",
      "train: loss=2.3978948807370837 acc=0.1391304347826087\n",
      "test: loss=2.397894871062127 acc=0.13333333333333333\n",
      "EPOCH=42\n",
      "train: loss=2.3978947587635204 acc=0.1463768115942029\n",
      "test: loss=2.397894793662472 acc=0.14202898550724638\n",
      "EPOCH=43\n",
      "train: loss=2.397894581850024 acc=0.1681159420289855\n",
      "test: loss=2.397894635407821 acc=0.1492753623188406\n",
      "EPOCH=44\n",
      "train: loss=2.3978944339613983 acc=0.1782608695652174\n",
      "test: loss=2.3978944878647295 acc=0.14492753623188406\n",
      "EPOCH=45\n",
      "train: loss=2.397894098793251 acc=0.1318840579710145\n",
      "test: loss=2.3978942535925603 acc=0.12173913043478261\n",
      "EPOCH=46\n",
      "train: loss=2.3978937853937565 acc=0.12753623188405797\n",
      "test: loss=2.3978938002517256 acc=0.11739130434782609\n",
      "EPOCH=47\n",
      "train: loss=2.397892929505611 acc=0.1289855072463768\n",
      "test: loss=2.3978933852651845 acc=0.11304347826086956\n",
      "EPOCH=48\n",
      "train: loss=2.3978915570438772 acc=0.1391304347826087\n",
      "test: loss=2.397891974449158 acc=0.12028985507246377\n",
      "EPOCH=49\n",
      "train: loss=2.397888271013896 acc=0.12028985507246377\n",
      "test: loss=2.3978889375493146 acc=0.12173913043478261\n",
      "EPOCH=50\n",
      "train: loss=2.3978799256725587 acc=0.14202898550724638\n",
      "test: loss=2.39788121762483 acc=0.1246376811594203\n",
      "EPOCH=51\n",
      "train: loss=2.397839977775795 acc=0.1565217391304348\n",
      "test: loss=2.3978483027306154 acc=0.12318840579710146\n",
      "EPOCH=52\n",
      "train: loss=2.397604130662006 acc=0.15072463768115943\n",
      "test: loss=2.3976410627365112 acc=0.13043478260869565\n",
      "EPOCH=53\n",
      "train: loss=2.3946278800135072 acc=0.15507246376811595\n",
      "test: loss=2.39489643988402 acc=0.1391304347826087\n",
      "EPOCH=54\n",
      "train: loss=2.3747760990391606 acc=0.15797101449275364\n",
      "test: loss=2.3761199864788334 acc=0.1608695652173913\n",
      "EPOCH=55\n",
      "train: loss=2.2619937214298527 acc=0.16666666666666666\n",
      "test: loss=2.2759134800537772 acc=0.16956521739130434\n",
      "EPOCH=56\n",
      "train: loss=2.1247533081234367 acc=0.15797101449275364\n",
      "test: loss=2.1469784843748894 acc=0.16956521739130434\n",
      "EPOCH=57\n",
      "train: loss=2.0845981107241864 acc=0.19130434782608696\n",
      "test: loss=2.0890100835026173 acc=0.16666666666666666\n",
      "EPOCH=58\n",
      "train: loss=2.0143228422040527 acc=0.20144927536231885\n",
      "test: loss=2.0409203337586446 acc=0.1681159420289855\n",
      "EPOCH=59\n",
      "train: loss=1.9651365411454353 acc=0.1855072463768116\n",
      "test: loss=2.0062566833219666 acc=0.16956521739130434\n",
      "EPOCH=60\n",
      "train: loss=1.9675643751586693 acc=0.14057971014492754\n",
      "test: loss=1.9867478470871414 acc=0.16666666666666666\n",
      "EPOCH=61\n",
      "train: loss=1.9139116831447767 acc=0.17101449275362318\n",
      "test: loss=1.9630181246909542 acc=0.1681159420289855\n",
      "EPOCH=62\n",
      "train: loss=1.8759091010992077 acc=0.1826086956521739\n",
      "test: loss=1.9401975711186727 acc=0.1681159420289855\n",
      "EPOCH=63\n",
      "train: loss=1.9037789514099341 acc=0.17391304347826086\n",
      "test: loss=1.9204364533009737 acc=0.16956521739130434\n",
      "EPOCH=64\n",
      "train: loss=1.893679450214773 acc=0.16956521739130434\n",
      "test: loss=1.9211190783459207 acc=0.1681159420289855\n",
      "EPOCH=65\n",
      "train: loss=1.8280656890592712 acc=0.19420289855072465\n",
      "test: loss=1.90788814244063 acc=0.17681159420289855\n",
      "EPOCH=66\n",
      "train: loss=1.8684744855631952 acc=0.1826086956521739\n",
      "test: loss=1.9096863404564235 acc=0.1753623188405797\n",
      "EPOCH=67\n",
      "train: loss=1.9271194997041121 acc=0.16666666666666666\n",
      "test: loss=1.922520879386128 acc=0.18115942028985507\n",
      "EPOCH=68\n",
      "train: loss=1.8888438539228578 acc=0.1681159420289855\n",
      "test: loss=1.9585715081380761 acc=0.17246376811594202\n",
      "EPOCH=69\n",
      "train: loss=2.0055512568225033 acc=0.17101449275362318\n",
      "test: loss=2.0270004384759543 acc=0.15797101449275364\n",
      "EPOCH=70\n",
      "train: loss=1.887776826430058 acc=0.19420289855072465\n",
      "test: loss=1.9107137462367183 acc=0.1681159420289855\n",
      "EPOCH=71\n",
      "train: loss=1.8646806188251661 acc=0.18115942028985507\n",
      "test: loss=1.8907174511232239 acc=0.16956521739130434\n",
      "EPOCH=72\n",
      "train: loss=1.8660409163737643 acc=0.17246376811594202\n",
      "test: loss=1.8857550785161448 acc=0.17246376811594202\n",
      "EPOCH=73\n",
      "train: loss=1.8455402147942694 acc=0.1536231884057971\n",
      "test: loss=1.878199305223382 acc=0.1753623188405797\n",
      "EPOCH=74\n",
      "train: loss=1.8785923078440239 acc=0.16666666666666666\n",
      "test: loss=1.88658168816912 acc=0.17391304347826086\n",
      "EPOCH=75\n",
      "train: loss=1.8557752823484117 acc=0.18695652173913044\n",
      "test: loss=1.8750513505244601 acc=0.17101449275362318\n",
      "EPOCH=76\n",
      "train: loss=1.8543850993764572 acc=0.18840579710144928\n",
      "test: loss=1.8890179330024166 acc=0.16956521739130434\n",
      "EPOCH=77\n",
      "train: loss=1.8679893142935158 acc=0.16956521739130434\n",
      "test: loss=1.8924762698187345 acc=0.17246376811594202\n",
      "EPOCH=78\n",
      "train: loss=1.8507920488067295 acc=0.16231884057971013\n",
      "test: loss=1.8910148454749065 acc=0.16666666666666666\n",
      "EPOCH=79\n",
      "train: loss=1.8481141529221465 acc=0.1956521739130435\n",
      "test: loss=1.874910623439844 acc=0.17101449275362318\n",
      "EPOCH=80\n",
      "train: loss=1.8111948859864386 acc=0.19130434782608696\n",
      "test: loss=1.8738589437111564 acc=0.16956521739130434\n",
      "EPOCH=81\n",
      "train: loss=1.8750465809435084 acc=0.19130434782608696\n",
      "test: loss=1.8761328292929609 acc=0.1681159420289855\n",
      "EPOCH=82\n",
      "train: loss=1.8350587444028992 acc=0.19710144927536233\n",
      "test: loss=1.8659895458083222 acc=0.1753623188405797\n",
      "EPOCH=83\n",
      "train: loss=1.8391910850137905 acc=0.17246376811594202\n",
      "test: loss=1.8834645805151566 acc=0.1826086956521739\n",
      "EPOCH=84\n",
      "train: loss=1.8406407218048537 acc=0.16521739130434782\n",
      "test: loss=1.866574870157933 acc=0.17681159420289855\n",
      "EPOCH=85\n",
      "train: loss=1.8392404559729756 acc=0.17681159420289855\n",
      "test: loss=1.8785491506258647 acc=0.16521739130434782\n",
      "EPOCH=86\n",
      "train: loss=1.8405981922495191 acc=0.1782608695652174\n",
      "test: loss=1.8666738746822744 acc=0.16521739130434782\n",
      "EPOCH=87\n",
      "train: loss=1.8147437434265579 acc=0.18405797101449275\n",
      "test: loss=1.867135373060254 acc=0.17101449275362318\n",
      "EPOCH=88\n",
      "train: loss=1.827029840324236 acc=0.1956521739130435\n",
      "test: loss=1.8665526118831357 acc=0.17101449275362318\n",
      "EPOCH=89\n",
      "train: loss=1.851696252822876 acc=0.1681159420289855\n",
      "test: loss=1.8694691208825596 acc=0.16666666666666666\n",
      "EPOCH=90\n",
      "train: loss=1.834430528378141 acc=0.1855072463768116\n",
      "test: loss=1.8661728147147358 acc=0.17391304347826086\n",
      "EPOCH=91\n",
      "train: loss=1.8304633062818776 acc=0.16231884057971013\n",
      "test: loss=1.8657315769057343 acc=0.17246376811594202\n",
      "EPOCH=92\n",
      "train: loss=1.830539922437806 acc=0.17681159420289855\n",
      "test: loss=1.865621911442798 acc=0.17101449275362318\n",
      "EPOCH=93\n",
      "train: loss=1.8711282097775004 acc=0.15942028985507245\n",
      "test: loss=1.8715225712112757 acc=0.16376811594202897\n",
      "EPOCH=94\n",
      "train: loss=1.8247813876124397 acc=0.1565217391304348\n",
      "test: loss=1.8649614420489988 acc=0.16376811594202897\n",
      "EPOCH=95\n",
      "train: loss=1.8258969033973804 acc=0.20869565217391303\n",
      "test: loss=1.8648754418760105 acc=0.16231884057971013\n",
      "EPOCH=96\n",
      "train: loss=1.8463951321615688 acc=0.17101449275362318\n",
      "test: loss=1.8646655865337538 acc=0.16521739130434782\n",
      "EPOCH=97\n",
      "train: loss=1.8235641431117404 acc=0.2028985507246377\n",
      "test: loss=1.8643988928933075 acc=0.18115942028985507\n",
      "EPOCH=98\n",
      "train: loss=1.8406311921451404 acc=0.1826086956521739\n",
      "test: loss=1.8645184256028438 acc=0.17681159420289855\n",
      "EPOCH=99\n",
      "train: loss=1.8097992224969726 acc=0.17681159420289855\n",
      "test: loss=1.8627733444822008 acc=0.17101449275362318\n",
      "EPOCH=100\n",
      "train: loss=1.8392253910285838 acc=0.1565217391304348\n",
      "test: loss=1.8644418035728345 acc=0.1681159420289855\n",
      "EPOCH=101\n",
      "train: loss=1.85213177446006 acc=0.15942028985507245\n",
      "test: loss=1.8638748295065286 acc=0.17246376811594202\n",
      "EPOCH=102\n",
      "train: loss=1.7883482518403426 acc=0.18985507246376812\n",
      "test: loss=1.860992443734321 acc=0.16666666666666666\n",
      "EPOCH=103\n",
      "train: loss=1.8394419898157535 acc=0.16376811594202897\n",
      "test: loss=1.863558941302092 acc=0.17681159420289855\n",
      "EPOCH=104\n",
      "train: loss=1.8393364980600881 acc=0.1681159420289855\n",
      "test: loss=1.8637884012167005 acc=0.18985507246376812\n",
      "EPOCH=105\n",
      "train: loss=1.8542428814846537 acc=0.1681159420289855\n",
      "test: loss=1.8631671376850294 acc=0.16521739130434782\n",
      "EPOCH=106\n",
      "train: loss=1.848906023260476 acc=0.20434782608695654\n",
      "test: loss=1.8679042487904645 acc=0.1681159420289855\n",
      "EPOCH=107\n",
      "train: loss=1.8394892065421395 acc=0.18840579710144928\n",
      "test: loss=1.8683334407599077 acc=0.17971014492753623\n",
      "EPOCH=108\n",
      "train: loss=1.8041247120801953 acc=0.18985507246376812\n",
      "test: loss=1.8625564395517542 acc=0.17391304347826086\n",
      "EPOCH=109\n",
      "train: loss=1.8247963304105013 acc=0.19130434782608696\n",
      "test: loss=1.8624928458877232 acc=0.17246376811594202\n",
      "EPOCH=110\n",
      "train: loss=1.8449168538701708 acc=0.15217391304347827\n",
      "test: loss=1.8622482071752133 acc=0.17391304347826086\n",
      "EPOCH=111\n",
      "train: loss=1.85397735816845 acc=0.17246376811594202\n",
      "test: loss=1.86268959943799 acc=0.16666666666666666\n",
      "EPOCH=112\n",
      "train: loss=1.8652550362158513 acc=0.15217391304347827\n",
      "test: loss=1.8611765204996302 acc=0.1753623188405797\n",
      "EPOCH=113\n",
      "train: loss=1.813360785574153 acc=0.12318840579710146\n",
      "test: loss=1.8621831189031186 acc=0.14492753623188406\n",
      "EPOCH=114\n",
      "train: loss=1.8144165481346242 acc=0.16231884057971013\n",
      "test: loss=1.8622583911038828 acc=0.15507246376811595\n",
      "EPOCH=115\n",
      "train: loss=1.8183612420939017 acc=0.15072463768115943\n",
      "test: loss=1.8624364673227505 acc=0.16376811594202897\n",
      "EPOCH=116\n",
      "train: loss=1.8184761849002562 acc=0.15217391304347827\n",
      "test: loss=1.8622250126755755 acc=0.1681159420289855\n",
      "EPOCH=117\n",
      "train: loss=1.8424536262733349 acc=0.1536231884057971\n",
      "test: loss=1.8617389231488324 acc=0.14492753623188406\n",
      "EPOCH=118\n",
      "train: loss=1.8405993922897008 acc=0.17681159420289855\n",
      "test: loss=1.8584262815074644 acc=0.18695652173913044\n",
      "EPOCH=119\n",
      "train: loss=1.8075630352117014 acc=0.22028985507246376\n",
      "test: loss=1.8613279854041942 acc=0.16956521739130434\n",
      "EPOCH=120\n",
      "train: loss=1.826207521169082 acc=0.19130434782608696\n",
      "test: loss=1.8606977238171343 acc=0.17681159420289855\n",
      "EPOCH=121\n",
      "train: loss=1.810222061302351 acc=0.18115942028985507\n",
      "test: loss=1.8611413465029951 acc=0.18115942028985507\n",
      "EPOCH=122\n",
      "train: loss=1.8382429091826729 acc=0.19710144927536233\n",
      "test: loss=1.8617139660793802 acc=0.18985507246376812\n",
      "EPOCH=123\n",
      "train: loss=1.8515413600465527 acc=0.16956521739130434\n",
      "test: loss=1.86083184843478 acc=0.18985507246376812\n",
      "EPOCH=124\n",
      "train: loss=1.8355598831522293 acc=0.18985507246376812\n",
      "test: loss=1.8607443177181742 acc=0.1826086956521739\n",
      "EPOCH=125\n",
      "train: loss=1.834156540684078 acc=0.17101449275362318\n",
      "test: loss=1.861497680518938 acc=0.1536231884057971\n",
      "EPOCH=126\n",
      "train: loss=1.8317930399507716 acc=0.16231884057971013\n",
      "test: loss=1.8578679770663165 acc=0.1463768115942029\n",
      "EPOCH=127\n",
      "train: loss=1.8195821938307388 acc=0.22318840579710145\n",
      "test: loss=1.8577054326085076 acc=0.17971014492753623\n",
      "EPOCH=128\n",
      "train: loss=1.8213597392690355 acc=0.21594202898550724\n",
      "test: loss=1.8606592503146848 acc=0.18405797101449275\n",
      "EPOCH=129\n",
      "train: loss=1.829011845415917 acc=0.1855072463768116\n",
      "test: loss=1.8577832712643387 acc=0.1826086956521739\n",
      "EPOCH=130\n",
      "train: loss=1.8047486944475035 acc=0.20579710144927535\n",
      "test: loss=1.8601642387500708 acc=0.1826086956521739\n",
      "EPOCH=131\n",
      "train: loss=1.8145999058433202 acc=0.2\n",
      "test: loss=1.859635019302368 acc=0.1753623188405797\n",
      "EPOCH=132\n",
      "train: loss=1.8099795388138813 acc=0.14347826086956522\n",
      "test: loss=1.8597270956937817 acc=0.1463768115942029\n",
      "EPOCH=133\n",
      "train: loss=1.8280725100766058 acc=0.1956521739130435\n",
      "test: loss=1.8574352241944576 acc=0.18695652173913044\n",
      "EPOCH=134\n",
      "train: loss=1.8438990903937298 acc=0.19130434782608696\n",
      "test: loss=1.8591042782949365 acc=0.19130434782608696\n",
      "EPOCH=135\n",
      "train: loss=1.8442956964174906 acc=0.18695652173913044\n",
      "test: loss=1.8591327014176742 acc=0.19710144927536233\n",
      "EPOCH=136\n",
      "train: loss=1.8327100715775422 acc=0.20579710144927535\n",
      "test: loss=1.859428110157234 acc=0.18405797101449275\n",
      "EPOCH=137\n",
      "train: loss=1.8162446493687836 acc=0.23043478260869565\n",
      "test: loss=1.8564868773239247 acc=0.20579710144927535\n",
      "EPOCH=138\n",
      "train: loss=1.8087028372114984 acc=0.18695652173913044\n",
      "test: loss=1.8564765681391178 acc=0.19130434782608696\n",
      "EPOCH=139\n",
      "train: loss=1.8212566627972369 acc=0.21159420289855072\n",
      "test: loss=1.8563073394955067 acc=0.19420289855072465\n",
      "EPOCH=140\n",
      "train: loss=1.812604404359624 acc=0.21014492753623187\n",
      "test: loss=1.8561956039373426 acc=0.1956521739130435\n",
      "EPOCH=141\n",
      "train: loss=1.8087000945340033 acc=0.21014492753623187\n",
      "test: loss=1.8565226568691973 acc=0.1927536231884058\n",
      "EPOCH=142\n",
      "train: loss=1.8355314914731011 acc=0.22318840579710145\n",
      "test: loss=1.868168596426646 acc=0.19130434782608696\n",
      "EPOCH=143\n",
      "train: loss=1.826458944445071 acc=0.22028985507246376\n",
      "test: loss=1.8582747360934382 acc=0.21884057971014492\n",
      "EPOCH=144\n",
      "train: loss=1.814969005273736 acc=0.24782608695652175\n",
      "test: loss=1.8577604945155157 acc=0.21304347826086956\n",
      "EPOCH=145\n",
      "train: loss=1.792854861072872 acc=0.23333333333333334\n",
      "test: loss=1.8558900586072948 acc=0.20869565217391303\n",
      "EPOCH=146\n",
      "train: loss=1.8146116445029992 acc=0.2246376811594203\n",
      "test: loss=1.8560049354166224 acc=0.1927536231884058\n",
      "EPOCH=147\n",
      "train: loss=1.7983139126197152 acc=0.2144927536231884\n",
      "test: loss=1.8558694507764733 acc=0.20869565217391303\n",
      "EPOCH=148\n",
      "train: loss=1.8204155037368552 acc=0.20434782608695654\n",
      "test: loss=1.8591963510582412 acc=0.17971014492753623\n",
      "EPOCH=149\n",
      "train: loss=1.8022273702897886 acc=0.2028985507246377\n",
      "test: loss=1.8556285275929216 acc=0.1753623188405797\n",
      "EPOCH=150\n",
      "train: loss=1.8337549534396849 acc=0.18115942028985507\n",
      "test: loss=1.8554157606069592 acc=0.20144927536231885\n",
      "EPOCH=151\n",
      "train: loss=1.8162604375161986 acc=0.23333333333333334\n",
      "test: loss=1.8553747524385866 acc=0.18695652173913044\n",
      "EPOCH=152\n",
      "train: loss=1.8436872788097547 acc=0.23768115942028986\n",
      "test: loss=1.8552553676176762 acc=0.22608695652173913\n",
      "EPOCH=153\n",
      "train: loss=1.824908879701642 acc=0.21159420289855072\n",
      "test: loss=1.8552822748819986 acc=0.2072463768115942\n",
      "EPOCH=154\n",
      "train: loss=1.8044024279152138 acc=0.2391304347826087\n",
      "test: loss=1.8551542893699977 acc=0.18695652173913044\n",
      "EPOCH=155\n",
      "train: loss=1.8221140327660934 acc=0.21159420289855072\n",
      "test: loss=1.8550540006679037 acc=0.1927536231884058\n",
      "EPOCH=156\n",
      "train: loss=1.8155294568642326 acc=0.2072463768115942\n",
      "test: loss=1.8549312569093013 acc=0.2072463768115942\n",
      "EPOCH=157\n",
      "train: loss=1.7975578123244687 acc=0.22318840579710145\n",
      "test: loss=1.854758764004362 acc=0.2072463768115942\n",
      "EPOCH=158\n",
      "train: loss=1.8026240217512932 acc=0.21739130434782608\n",
      "test: loss=1.8546072949533876 acc=0.18985507246376812\n",
      "EPOCH=159\n",
      "train: loss=1.8228554025940273 acc=0.18985507246376812\n",
      "test: loss=1.8544040750766146 acc=0.21304347826086956\n",
      "EPOCH=160\n",
      "train: loss=1.7790566316549328 acc=0.22318840579710145\n",
      "test: loss=1.8542154282763383 acc=0.2\n",
      "EPOCH=161\n",
      "train: loss=1.8052565775055816 acc=0.21014492753623187\n",
      "test: loss=1.8542483438616213 acc=0.2072463768115942\n",
      "EPOCH=162\n",
      "train: loss=1.8012073338895604 acc=0.2144927536231884\n",
      "test: loss=1.854487184856249 acc=0.19855072463768117\n",
      "EPOCH=163\n",
      "train: loss=1.8074446785277214 acc=0.2608695652173913\n",
      "test: loss=1.8541614622309588 acc=0.20579710144927535\n",
      "EPOCH=164\n",
      "train: loss=1.8287250434142956 acc=0.24492753623188407\n",
      "test: loss=1.8540691750637 acc=0.21594202898550724\n",
      "EPOCH=165\n",
      "train: loss=1.8188803681428882 acc=0.2797101449275362\n",
      "test: loss=1.8540628801221433 acc=0.22753623188405797\n",
      "EPOCH=166\n",
      "train: loss=1.8201968827109405 acc=0.22753623188405797\n",
      "test: loss=1.8540863470754763 acc=0.2246376811594203\n",
      "EPOCH=167\n",
      "train: loss=1.8137908669485563 acc=0.2565217391304348\n",
      "test: loss=1.8538619269495424 acc=0.2579710144927536\n",
      "EPOCH=168\n",
      "train: loss=1.8105519802674004 acc=0.21304347826086956\n",
      "test: loss=1.8540980411612469 acc=0.22608695652173913\n",
      "EPOCH=169\n",
      "train: loss=1.830692672729492 acc=0.25072463768115943\n",
      "test: loss=1.8539771510207135 acc=0.21159420289855072\n",
      "EPOCH=170\n",
      "train: loss=1.8320326276447463 acc=0.21884057971014492\n",
      "test: loss=1.8536910665208015 acc=0.2318840579710145\n",
      "EPOCH=171\n",
      "train: loss=1.8165388090023096 acc=0.2318840579710145\n",
      "test: loss=1.8535831076511438 acc=0.22028985507246376\n",
      "EPOCH=172\n",
      "train: loss=1.803983495719191 acc=0.2318840579710145\n",
      "test: loss=1.853387693736864 acc=0.2289855072463768\n",
      "EPOCH=173\n",
      "train: loss=1.8327536242595617 acc=0.24347826086956523\n",
      "test: loss=1.8532325958860094 acc=0.25072463768115943\n",
      "EPOCH=174\n",
      "train: loss=1.8102975069612697 acc=0.2608695652173913\n",
      "test: loss=1.8533647416294485 acc=0.24202898550724639\n",
      "EPOCH=175\n",
      "train: loss=1.823756544313569 acc=0.2768115942028985\n",
      "test: loss=1.852891869130342 acc=0.2782608695652174\n",
      "EPOCH=176\n",
      "train: loss=1.812640388806661 acc=0.2811594202898551\n",
      "test: loss=1.8526114970013716 acc=0.2811594202898551\n",
      "EPOCH=177\n",
      "train: loss=1.8040004623109016 acc=0.2811594202898551\n",
      "test: loss=1.8523067595302194 acc=0.2855072463768116\n",
      "EPOCH=178\n",
      "train: loss=1.814510722782301 acc=0.2927536231884058\n",
      "test: loss=1.8516353995903678 acc=0.3115942028985507\n",
      "EPOCH=179\n",
      "train: loss=1.8183914286502894 acc=0.2985507246376812\n",
      "test: loss=1.8515219904374385 acc=0.29130434782608694\n",
      "EPOCH=180\n",
      "train: loss=1.8105063035868216 acc=0.3072463768115942\n",
      "test: loss=1.8509881904159766 acc=0.3318840579710145\n",
      "EPOCH=181\n",
      "train: loss=1.8214863694232444 acc=0.3144927536231884\n",
      "test: loss=1.8505253422087518 acc=0.32753623188405795\n",
      "EPOCH=182\n",
      "train: loss=1.81394433543302 acc=0.28405797101449276\n",
      "test: loss=1.848880380305691 acc=0.2942028985507246\n",
      "EPOCH=183\n",
      "train: loss=1.8048772098361583 acc=0.30144927536231886\n",
      "test: loss=1.8467182655265366 acc=0.3159420289855073\n",
      "EPOCH=184\n",
      "train: loss=1.8254013014876325 acc=0.3333333333333333\n",
      "test: loss=1.8440763423408286 acc=0.3333333333333333\n",
      "EPOCH=185\n",
      "train: loss=1.793962199791618 acc=0.3536231884057971\n",
      "test: loss=1.840109145986861 acc=0.3391304347826087\n",
      "EPOCH=186\n",
      "train: loss=1.780320401813673 acc=0.32608695652173914\n",
      "test: loss=1.8360569658486738 acc=0.3\n",
      "EPOCH=187\n",
      "train: loss=1.7973000804583232 acc=0.30579710144927535\n",
      "test: loss=1.8332773878954458 acc=0.35507246376811596\n",
      "EPOCH=188\n",
      "train: loss=1.7993548213571742 acc=0.20579710144927535\n",
      "test: loss=1.8280806724575982 acc=0.24347826086956523\n",
      "EPOCH=189\n",
      "train: loss=1.795725569344949 acc=0.3347826086956522\n",
      "test: loss=1.8195481105127196 acc=0.3695652173913043\n",
      "EPOCH=190\n",
      "train: loss=1.7966496588527292 acc=0.32608695652173914\n",
      "test: loss=1.8096042902573295 acc=0.36086956521739133\n",
      "EPOCH=191\n",
      "train: loss=1.7544346470763719 acc=0.34057971014492755\n",
      "test: loss=1.797259185452392 acc=0.35942028985507246\n",
      "EPOCH=192\n",
      "train: loss=1.7448270386543827 acc=0.3492753623188406\n",
      "test: loss=1.7822313638700955 acc=0.3492753623188406\n",
      "EPOCH=193\n",
      "train: loss=1.7365314656409665 acc=0.34492753623188405\n",
      "test: loss=1.7773476764775704 acc=0.3507246376811594\n",
      "EPOCH=194\n",
      "train: loss=1.7480309270430303 acc=0.3115942028985507\n",
      "test: loss=1.7630219984745634 acc=0.34347826086956523\n",
      "EPOCH=195\n",
      "train: loss=1.7425475471261618 acc=0.3333333333333333\n",
      "test: loss=1.7516903084257374 acc=0.3333333333333333\n",
      "EPOCH=196\n",
      "train: loss=1.7297418324843696 acc=0.3333333333333333\n",
      "test: loss=1.7472054816674496 acc=0.3289855072463768\n",
      "EPOCH=197\n",
      "train: loss=1.7572157092716383 acc=0.2826086956521739\n",
      "test: loss=1.754957617106645 acc=0.3144927536231884\n",
      "EPOCH=198\n",
      "train: loss=1.7613648326500602 acc=0.2768115942028985\n",
      "test: loss=1.7518419562042624 acc=0.30434782608695654\n",
      "EPOCH=199\n",
      "train: loss=1.6927559057007666 acc=0.2971014492753623\n",
      "test: loss=1.7273664198059966 acc=0.32028985507246377\n",
      "EPOCH=200\n",
      "train: loss=1.6800644110942233 acc=0.28840579710144926\n",
      "test: loss=1.6845143856345743 acc=0.336231884057971\n",
      "EPOCH=201\n",
      "train: loss=1.6790811091229536 acc=0.2768115942028985\n",
      "test: loss=1.6775113542874653 acc=0.3188405797101449\n",
      "EPOCH=202\n",
      "train: loss=1.6492567558219466 acc=0.2956521739130435\n",
      "test: loss=1.657834122578303 acc=0.30434782608695654\n",
      "EPOCH=203\n",
      "train: loss=1.5691697991412619 acc=0.3391304347826087\n",
      "test: loss=1.6018969518550927 acc=0.3318840579710145\n",
      "EPOCH=204\n",
      "train: loss=1.5713852588681207 acc=0.3144927536231884\n",
      "test: loss=1.583651994967806 acc=0.3101449275362319\n",
      "EPOCH=205\n",
      "train: loss=1.532694563485574 acc=0.32028985507246377\n",
      "test: loss=1.5421091338862543 acc=0.35797101449275365\n",
      "EPOCH=206\n",
      "train: loss=1.5098707652610281 acc=0.30434782608695654\n",
      "test: loss=1.5352255008358886 acc=0.3391304347826087\n",
      "EPOCH=207\n",
      "train: loss=1.4899134885573733 acc=0.3507246376811594\n",
      "test: loss=1.5120295329370361 acc=0.34492753623188405\n",
      "EPOCH=208\n",
      "train: loss=1.4491400883681533 acc=0.39420289855072466\n",
      "test: loss=1.4654268292413242 acc=0.40144927536231884\n",
      "EPOCH=209\n",
      "train: loss=1.4294128364410954 acc=0.4318840579710145\n",
      "test: loss=1.454280618653781 acc=0.4246376811594203\n",
      "EPOCH=210\n",
      "train: loss=1.4158322609853053 acc=0.4666666666666667\n",
      "test: loss=1.4284586583358654 acc=0.44782608695652176\n",
      "EPOCH=211\n",
      "train: loss=1.3919536644565886 acc=0.43043478260869567\n",
      "test: loss=1.4038160897683407 acc=0.45942028985507244\n",
      "EPOCH=212\n",
      "train: loss=1.356097270872282 acc=0.49130434782608695\n",
      "test: loss=1.3831306478251582 acc=0.48840579710144927\n",
      "EPOCH=213\n",
      "train: loss=1.3340639107901118 acc=0.4391304347826087\n",
      "test: loss=1.3744504347227622 acc=0.45217391304347826\n",
      "EPOCH=214\n",
      "train: loss=1.3080264397937318 acc=0.5072463768115942\n",
      "test: loss=1.3346888735242513 acc=0.4956521739130435\n",
      "EPOCH=215\n",
      "train: loss=1.302222435305948 acc=0.45942028985507244\n",
      "test: loss=1.3121099207928215 acc=0.4956521739130435\n",
      "EPOCH=216\n",
      "train: loss=1.2534679222798002 acc=0.5231884057971015\n",
      "test: loss=1.312550506060538 acc=0.47681159420289854\n",
      "EPOCH=217\n",
      "train: loss=1.2384326497605744 acc=0.5420289855072464\n",
      "test: loss=1.2825850440108257 acc=0.5115942028985507\n",
      "EPOCH=218\n",
      "train: loss=1.1908430422777716 acc=0.5043478260869565\n",
      "test: loss=1.2566796411098777 acc=0.4855072463768116\n",
      "EPOCH=219\n",
      "train: loss=1.1728255895492823 acc=0.5\n",
      "test: loss=1.254671374524849 acc=0.4811594202898551\n",
      "EPOCH=220\n",
      "train: loss=1.172484070894079 acc=0.5159420289855072\n",
      "test: loss=1.2302414667023265 acc=0.5072463768115942\n",
      "EPOCH=221\n",
      "train: loss=1.1616681401037436 acc=0.5159420289855072\n",
      "test: loss=1.2114336011742337 acc=0.48985507246376814\n",
      "EPOCH=222\n",
      "train: loss=1.1021251941558676 acc=0.5739130434782609\n",
      "test: loss=1.2046139381178047 acc=0.49130434782608695\n",
      "EPOCH=223\n",
      "train: loss=1.144320874377761 acc=0.5391304347826087\n",
      "test: loss=1.1847076115400894 acc=0.5231884057971015\n",
      "EPOCH=224\n",
      "train: loss=1.0918092551546683 acc=0.5521739130434783\n",
      "test: loss=1.1770144862649234 acc=0.5173913043478261\n",
      "EPOCH=225\n",
      "train: loss=1.0584103415868638 acc=0.5449275362318841\n",
      "test: loss=1.13566971493453 acc=0.5217391304347826\n",
      "EPOCH=226\n",
      "train: loss=1.081813644848602 acc=0.5231884057971015\n",
      "test: loss=1.144887761335712 acc=0.5101449275362319\n",
      "EPOCH=227\n",
      "train: loss=1.0611471685769873 acc=0.5826086956521739\n",
      "test: loss=1.1096965259157012 acc=0.527536231884058\n",
      "EPOCH=228\n",
      "train: loss=1.0329102035584874 acc=0.563768115942029\n",
      "test: loss=1.106342024449259 acc=0.5217391304347826\n",
      "EPOCH=229\n",
      "train: loss=1.0240089664249208 acc=0.5666666666666667\n",
      "test: loss=1.0906357027320326 acc=0.5231884057971015\n",
      "EPOCH=230\n",
      "train: loss=1.0091828267528689 acc=0.5826086956521739\n",
      "test: loss=1.074305778157398 acc=0.5550724637681159\n",
      "EPOCH=231\n",
      "train: loss=1.0199938156081876 acc=0.5855072463768116\n",
      "test: loss=1.0524778387927707 acc=0.5391304347826087\n",
      "EPOCH=232\n",
      "train: loss=0.949938110411977 acc=0.6130434782608696\n",
      "test: loss=1.04748443407344 acc=0.5463768115942029\n",
      "EPOCH=233\n",
      "train: loss=1.004648173045016 acc=0.5898550724637681\n",
      "test: loss=1.0367464070545807 acc=0.5304347826086957\n",
      "EPOCH=234\n",
      "train: loss=0.9207237085604203 acc=0.6202898550724638\n",
      "test: loss=1.009839734284585 acc=0.5521739130434783\n",
      "EPOCH=235\n",
      "train: loss=0.9627169456049476 acc=0.6086956521739131\n",
      "test: loss=1.0192325499873824 acc=0.5449275362318841\n",
      "EPOCH=236\n",
      "train: loss=0.9207378678165539 acc=0.5927536231884057\n",
      "test: loss=0.9917681256707326 acc=0.5768115942028985\n",
      "EPOCH=237\n",
      "train: loss=0.9119898108954447 acc=0.6130434782608696\n",
      "test: loss=0.9797926802555963 acc=0.6101449275362318\n",
      "EPOCH=238\n",
      "train: loss=0.9065388645443634 acc=0.6159420289855072\n",
      "test: loss=0.9901298351265301 acc=0.5492753623188406\n",
      "EPOCH=239\n",
      "train: loss=0.8955163784180026 acc=0.6695652173913044\n",
      "test: loss=0.96535323633124 acc=0.6043478260869565\n",
      "EPOCH=240\n",
      "train: loss=0.8659726217942502 acc=0.6579710144927536\n",
      "test: loss=0.9471550721621168 acc=0.6057971014492753\n",
      "EPOCH=241\n",
      "train: loss=0.8346736037580674 acc=0.6739130434782609\n",
      "test: loss=0.9435704277015116 acc=0.6043478260869565\n",
      "EPOCH=242\n",
      "train: loss=0.8662041401132887 acc=0.6217391304347826\n",
      "test: loss=0.9317987385037206 acc=0.6217391304347826\n",
      "EPOCH=243\n",
      "train: loss=0.8713916100311916 acc=0.6521739130434783\n",
      "test: loss=0.9216913313817475 acc=0.5884057971014492\n",
      "EPOCH=244\n",
      "train: loss=0.85128219660601 acc=0.6521739130434783\n",
      "test: loss=0.9287539510670509 acc=0.6159420289855072\n",
      "EPOCH=245\n",
      "train: loss=0.8446131701290985 acc=0.6478260869565218\n",
      "test: loss=0.9264806053142293 acc=0.5927536231884057\n",
      "EPOCH=246\n",
      "train: loss=0.8937390585949617 acc=0.6057971014492753\n",
      "test: loss=0.9246938461097012 acc=0.6159420289855072\n",
      "EPOCH=247\n",
      "train: loss=0.7991750109078619 acc=0.6739130434782609\n",
      "test: loss=0.889081134406708 acc=0.6376811594202898\n",
      "EPOCH=248\n",
      "train: loss=0.7987652033192875 acc=0.6985507246376812\n",
      "test: loss=0.8987631138886555 acc=0.6231884057971014\n",
      "EPOCH=249\n",
      "train: loss=0.791579180082772 acc=0.7\n",
      "test: loss=0.8805762824410087 acc=0.6260869565217392\n",
      "EPOCH=250\n",
      "train: loss=0.8026077646573169 acc=0.6695652173913044\n",
      "test: loss=0.8976542664016934 acc=0.6318840579710145\n",
      "EPOCH=251\n",
      "train: loss=0.8010588419271053 acc=0.6376811594202898\n",
      "test: loss=0.8715474002650413 acc=0.6260869565217392\n",
      "EPOCH=252\n",
      "train: loss=0.7822870926042357 acc=0.6666666666666666\n",
      "test: loss=0.8599326065574449 acc=0.6347826086956522\n",
      "EPOCH=253\n",
      "train: loss=0.7890499707404187 acc=0.6579710144927536\n",
      "test: loss=0.8621224509732649 acc=0.6173913043478261\n",
      "EPOCH=254\n",
      "train: loss=0.7672100858766234 acc=0.7101449275362319\n",
      "test: loss=0.864954832966264 acc=0.6420289855072464\n",
      "EPOCH=255\n",
      "train: loss=0.8055760558849822 acc=0.6985507246376812\n",
      "test: loss=0.8729809774362577 acc=0.6420289855072464\n",
      "EPOCH=256\n",
      "train: loss=0.7989994452422674 acc=0.6826086956521739\n",
      "test: loss=0.8654461141830931 acc=0.6275362318840579\n",
      "EPOCH=257\n",
      "train: loss=0.7593035852005098 acc=0.6956521739130435\n",
      "test: loss=0.8440844257230588 acc=0.6463768115942029\n",
      "EPOCH=258\n",
      "train: loss=0.763424012147943 acc=0.6855072463768116\n",
      "test: loss=0.8463834023744246 acc=0.6536231884057971\n",
      "EPOCH=259\n",
      "train: loss=0.7454573701911916 acc=0.6927536231884058\n",
      "test: loss=0.8362156587646113 acc=0.636231884057971\n",
      "EPOCH=260\n",
      "train: loss=0.8286500160531748 acc=0.6594202898550725\n",
      "test: loss=0.8639585629706203 acc=0.636231884057971\n",
      "EPOCH=261\n",
      "train: loss=0.7194474689662457 acc=0.7304347826086957\n",
      "test: loss=0.8268230232887605 acc=0.6710144927536232\n",
      "EPOCH=262\n",
      "train: loss=0.8009223531317505 acc=0.663768115942029\n",
      "test: loss=0.8404374545625667 acc=0.6318840579710145\n",
      "EPOCH=263\n",
      "train: loss=0.7659171778917907 acc=0.6927536231884058\n",
      "test: loss=0.8370362612074645 acc=0.644927536231884\n",
      "EPOCH=264\n",
      "train: loss=0.7533903176638136 acc=0.7043478260869566\n",
      "test: loss=0.8406979093373672 acc=0.6550724637681159\n",
      "EPOCH=265\n",
      "train: loss=0.7418234400118809 acc=0.7231884057971014\n",
      "test: loss=0.8297045264890809 acc=0.6536231884057971\n",
      "EPOCH=266\n",
      "train: loss=0.7672935987324994 acc=0.6782608695652174\n",
      "test: loss=0.8165817829387167 acc=0.6695652173913044\n",
      "EPOCH=267\n",
      "train: loss=0.7158310584952562 acc=0.7434782608695653\n",
      "test: loss=0.8096687781741755 acc=0.6550724637681159\n",
      "EPOCH=268\n",
      "train: loss=0.7214699063470345 acc=0.7057971014492753\n",
      "test: loss=0.8157833095790946 acc=0.6420289855072464\n",
      "EPOCH=269\n",
      "train: loss=0.8418833146703537 acc=0.663768115942029\n",
      "test: loss=0.8554163172257309 acc=0.6333333333333333\n",
      "EPOCH=270\n",
      "train: loss=0.7237950237246865 acc=0.7086956521739131\n",
      "test: loss=0.8233514734690645 acc=0.6405797101449275\n",
      "EPOCH=271\n",
      "train: loss=0.7233868825909061 acc=0.6956521739130435\n",
      "test: loss=0.8339586478814085 acc=0.6318840579710145\n",
      "EPOCH=272\n",
      "train: loss=0.790350505779666 acc=0.6753623188405797\n",
      "test: loss=0.8435779583841171 acc=0.6405797101449275\n",
      "EPOCH=273\n",
      "train: loss=0.762065691611075 acc=0.6739130434782609\n",
      "test: loss=0.8675274418585974 acc=0.6463768115942029\n",
      "EPOCH=274\n",
      "train: loss=0.7172894753854 acc=0.7275362318840579\n",
      "test: loss=0.8195015655088987 acc=0.6420289855072464\n",
      "EPOCH=275\n",
      "train: loss=0.7809878650048843 acc=0.6710144927536232\n",
      "test: loss=0.8566873805914614 acc=0.6304347826086957\n",
      "EPOCH=276\n",
      "train: loss=0.6953363325170147 acc=0.7159420289855073\n",
      "test: loss=0.8096338289577271 acc=0.6478260869565218\n",
      "EPOCH=277\n",
      "train: loss=0.7404856092731833 acc=0.7028985507246377\n",
      "test: loss=0.7974280661181209 acc=0.6463768115942029\n",
      "EPOCH=278\n",
      "train: loss=0.686656446193102 acc=0.691304347826087\n",
      "test: loss=0.8002286349121225 acc=0.6521739130434783\n",
      "EPOCH=279\n",
      "train: loss=0.8133593078908524 acc=0.6579710144927536\n",
      "test: loss=0.8774521737815002 acc=0.6347826086956522\n",
      "EPOCH=280\n",
      "train: loss=0.7404502220754681 acc=0.6565217391304348\n",
      "test: loss=0.8464159094842414 acc=0.6507246376811594\n",
      "EPOCH=281\n",
      "train: loss=0.6868993759335802 acc=0.7028985507246377\n",
      "test: loss=0.7822215104716884 acc=0.6565217391304348\n",
      "EPOCH=282\n",
      "train: loss=0.7936167236361855 acc=0.6782608695652174\n",
      "test: loss=0.8340897907455708 acc=0.6318840579710145\n",
      "EPOCH=283\n",
      "train: loss=0.7562746174701833 acc=0.691304347826087\n",
      "test: loss=0.8177457846687648 acc=0.6318840579710145\n",
      "EPOCH=284\n",
      "train: loss=0.7136672904300739 acc=0.6869565217391305\n",
      "test: loss=0.8063651900160619 acc=0.6507246376811594\n",
      "EPOCH=285\n",
      "train: loss=0.708476684356709 acc=0.6710144927536232\n",
      "test: loss=0.7970055166353552 acc=0.6376811594202898\n",
      "EPOCH=286\n",
      "train: loss=0.7445213787225615 acc=0.6768115942028986\n",
      "test: loss=0.8192962064377397 acc=0.6289855072463768\n",
      "EPOCH=287\n",
      "train: loss=0.7406523910979571 acc=0.672463768115942\n",
      "test: loss=0.8205783906104246 acc=0.6405797101449275\n",
      "EPOCH=288\n",
      "train: loss=0.7574659576152782 acc=0.672463768115942\n",
      "test: loss=0.8040801619533433 acc=0.6550724637681159\n",
      "EPOCH=289\n",
      "train: loss=0.7188820045880899 acc=0.6884057971014492\n",
      "test: loss=0.7949843897645791 acc=0.6478260869565218\n",
      "EPOCH=290\n",
      "train: loss=0.7037903296692885 acc=0.7043478260869566\n",
      "test: loss=0.7832387483885016 acc=0.6492753623188405\n",
      "EPOCH=291\n",
      "train: loss=0.6683892651079762 acc=0.7289855072463768\n",
      "test: loss=0.8041620872624596 acc=0.6420289855072464\n",
      "EPOCH=292\n",
      "train: loss=0.7334265393822494 acc=0.6855072463768116\n",
      "test: loss=0.8463948847342154 acc=0.6405797101449275\n",
      "EPOCH=293\n",
      "train: loss=0.7235793504235504 acc=0.6695652173913044\n",
      "test: loss=0.7953759851111724 acc=0.6478260869565218\n",
      "EPOCH=294\n",
      "train: loss=0.694512831052492 acc=0.717391304347826\n",
      "test: loss=0.7927533889073791 acc=0.6507246376811594\n",
      "EPOCH=295\n",
      "train: loss=0.7134994519163308 acc=0.691304347826087\n",
      "test: loss=0.775458943250714 acc=0.672463768115942\n",
      "EPOCH=296\n",
      "train: loss=0.6351956119241642 acc=0.7724637681159421\n",
      "test: loss=0.7483789448075053 acc=0.6927536231884058\n",
      "EPOCH=297\n",
      "train: loss=0.716350034497378 acc=0.7\n",
      "test: loss=0.7753163984716461 acc=0.6710144927536232\n",
      "EPOCH=298\n",
      "train: loss=0.6938513578900618 acc=0.7130434782608696\n",
      "test: loss=0.7709880283190548 acc=0.6768115942028986\n",
      "EPOCH=299\n",
      "train: loss=0.6161247193073496 acc=0.7594202898550725\n",
      "test: loss=0.7361300221678929 acc=0.6768115942028986\n",
      "EPOCH=300\n",
      "train: loss=0.6520837574622314 acc=0.7420289855072464\n",
      "test: loss=0.7324107719903332 acc=0.6942028985507246\n",
      "EPOCH=301\n",
      "train: loss=0.6696314443842983 acc=0.7507246376811594\n",
      "test: loss=0.7309460846964941 acc=0.6956521739130435\n",
      "EPOCH=302\n",
      "train: loss=0.6147648746200054 acc=0.7420289855072464\n",
      "test: loss=0.722939388895706 acc=0.6927536231884058\n",
      "EPOCH=303\n",
      "train: loss=0.664911609428194 acc=0.7231884057971014\n",
      "test: loss=0.7231229806766825 acc=0.6826086956521739\n",
      "EPOCH=304\n",
      "train: loss=0.6574757953774204 acc=0.7347826086956522\n",
      "test: loss=0.7445425624035019 acc=0.6942028985507246\n",
      "EPOCH=305\n",
      "train: loss=0.6919865985537215 acc=0.7028985507246377\n",
      "test: loss=0.7438947924308182 acc=0.6710144927536232\n",
      "EPOCH=306\n",
      "train: loss=0.5802184242809164 acc=0.7855072463768116\n",
      "test: loss=0.6825485662988681 acc=0.7231884057971014\n",
      "EPOCH=307\n",
      "train: loss=0.6591216258001531 acc=0.7202898550724638\n",
      "test: loss=0.7128773412904263 acc=0.7014492753623188\n",
      "EPOCH=308\n",
      "train: loss=0.6412707546230261 acc=0.7536231884057971\n",
      "test: loss=0.7252473232980845 acc=0.6956521739130435\n",
      "EPOCH=309\n",
      "train: loss=0.5873686495263204 acc=0.7710144927536232\n",
      "test: loss=0.6937109391397351 acc=0.7188405797101449\n",
      "EPOCH=310\n",
      "train: loss=0.5807642545563716 acc=0.7797101449275362\n",
      "test: loss=0.6739205628733465 acc=0.7347826086956522\n",
      "EPOCH=311\n",
      "train: loss=0.6115680760027 acc=0.7478260869565218\n",
      "test: loss=0.6723170942780426 acc=0.7347826086956522\n",
      "EPOCH=312\n",
      "train: loss=0.5854905145768788 acc=0.7840579710144927\n",
      "test: loss=0.6510692297261668 acc=0.7463768115942029\n",
      "EPOCH=313\n",
      "train: loss=0.5929670432301583 acc=0.7579710144927536\n",
      "test: loss=0.6699599415066388 acc=0.7405797101449275\n",
      "EPOCH=314\n",
      "train: loss=0.5985553376367139 acc=0.7782608695652173\n",
      "test: loss=0.6746550459883335 acc=0.7463768115942029\n",
      "EPOCH=315\n",
      "train: loss=0.6314158738383254 acc=0.7492753623188406\n",
      "test: loss=0.6978067919183387 acc=0.717391304347826\n",
      "EPOCH=316\n",
      "train: loss=0.6535314608064557 acc=0.7623188405797101\n",
      "test: loss=0.6628292911201361 acc=0.7420289855072464\n",
      "EPOCH=317\n",
      "train: loss=0.5388752000773768 acc=0.8260869565217391\n",
      "test: loss=0.6132600544884369 acc=0.7753623188405797\n",
      "EPOCH=318\n",
      "train: loss=0.5082363895783404 acc=0.8304347826086956\n",
      "test: loss=0.5972870364232643 acc=0.7811594202898551\n",
      "EPOCH=319\n",
      "train: loss=0.5672213498451089 acc=0.8057971014492754\n",
      "test: loss=0.6162838083477201 acc=0.7521739130434782\n",
      "EPOCH=320\n",
      "train: loss=0.5705633933269976 acc=0.7913043478260869\n",
      "test: loss=0.6363629228430817 acc=0.7492753623188406\n",
      "EPOCH=321\n",
      "train: loss=0.49993055340689707 acc=0.8347826086956521\n",
      "test: loss=0.5745907403260007 acc=0.7695652173913043\n",
      "EPOCH=322\n",
      "train: loss=0.6077980740056365 acc=0.7405797101449275\n",
      "test: loss=0.6557821215791974 acc=0.7434782608695653\n",
      "EPOCH=323\n",
      "train: loss=0.4957376205406879 acc=0.8376811594202899\n",
      "test: loss=0.5652074542269448 acc=0.7956521739130434\n",
      "EPOCH=324\n",
      "train: loss=0.532598475384251 acc=0.8057971014492754\n",
      "test: loss=0.5584782709760053 acc=0.7913043478260869\n",
      "EPOCH=325\n",
      "train: loss=0.49732040825571555 acc=0.8217391304347826\n",
      "test: loss=0.561160236171871 acc=0.7927536231884058\n",
      "EPOCH=326\n",
      "train: loss=0.53937888722871 acc=0.7768115942028986\n",
      "test: loss=0.5794395354237286 acc=0.7768115942028986\n",
      "EPOCH=327\n",
      "train: loss=0.49177324332008343 acc=0.8173913043478261\n",
      "test: loss=0.5576566888662258 acc=0.7884057971014493\n",
      "EPOCH=328\n",
      "train: loss=0.5171986313466534 acc=0.8159420289855073\n",
      "test: loss=0.5525520493290337 acc=0.7956521739130434\n",
      "EPOCH=329\n",
      "train: loss=0.5296975761737617 acc=0.8028985507246377\n",
      "test: loss=0.5593525201646591 acc=0.7797101449275362\n",
      "EPOCH=330\n",
      "train: loss=0.45487665431004565 acc=0.827536231884058\n",
      "test: loss=0.5686398481501796 acc=0.7782608695652173\n",
      "EPOCH=331\n",
      "train: loss=0.5602640937346512 acc=0.7898550724637681\n",
      "test: loss=0.5835735235704899 acc=0.7710144927536232\n",
      "EPOCH=332\n",
      "train: loss=0.5237026213866108 acc=0.8072463768115942\n",
      "test: loss=0.5502718822300253 acc=0.7898550724637681\n",
      "EPOCH=333\n",
      "train: loss=0.498575482482261 acc=0.8144927536231884\n",
      "test: loss=0.5262761398745394 acc=0.808695652173913\n",
      "EPOCH=334\n",
      "train: loss=0.48099494189231917 acc=0.808695652173913\n",
      "test: loss=0.5306925207842774 acc=0.808695652173913\n",
      "EPOCH=335\n",
      "train: loss=0.49112491920962426 acc=0.8217391304347826\n",
      "test: loss=0.5177715078370386 acc=0.8202898550724638\n",
      "EPOCH=336\n",
      "train: loss=0.46754344771915635 acc=0.8507246376811595\n",
      "test: loss=0.5230640868782664 acc=0.8115942028985508\n",
      "EPOCH=337\n",
      "train: loss=0.5090353980282932 acc=0.8289855072463768\n",
      "test: loss=0.5133880007757292 acc=0.8115942028985508\n",
      "EPOCH=338\n",
      "train: loss=0.483741998007273 acc=0.8391304347826087\n",
      "test: loss=0.5372433672363 acc=0.7956521739130434\n",
      "EPOCH=339\n",
      "train: loss=0.46176721179240765 acc=0.8434782608695652\n",
      "test: loss=0.48263175861588997 acc=0.8347826086956521\n",
      "EPOCH=340\n",
      "train: loss=0.4300969768049823 acc=0.8608695652173913\n",
      "test: loss=0.4895400786179749 acc=0.8202898550724638\n",
      "EPOCH=341\n",
      "train: loss=0.4330421205127504 acc=0.855072463768116\n",
      "test: loss=0.4990913781425806 acc=0.8231884057971014\n",
      "EPOCH=342\n",
      "train: loss=0.45174596368118386 acc=0.8391304347826087\n",
      "test: loss=0.4934209252279749 acc=0.8043478260869565\n",
      "EPOCH=343\n",
      "train: loss=0.4440487781934992 acc=0.8521739130434782\n",
      "test: loss=0.4773686629386182 acc=0.8420289855072464\n",
      "EPOCH=344\n",
      "train: loss=0.46049366979189116 acc=0.8449275362318841\n",
      "test: loss=0.4812171290124567 acc=0.8304347826086956\n",
      "EPOCH=345\n",
      "train: loss=0.4725739034643545 acc=0.8304347826086956\n",
      "test: loss=0.48990280560103494 acc=0.8188405797101449\n",
      "EPOCH=346\n",
      "train: loss=0.4441743608122941 acc=0.8376811594202899\n",
      "test: loss=0.47735498902295964 acc=0.8434782608695652\n",
      "EPOCH=347\n",
      "train: loss=0.41556737711109 acc=0.855072463768116\n",
      "test: loss=0.45411994019854374 acc=0.8507246376811595\n",
      "EPOCH=348\n",
      "train: loss=0.4068382903111236 acc=0.8724637681159421\n",
      "test: loss=0.44773724361873685 acc=0.8449275362318841\n",
      "EPOCH=349\n",
      "train: loss=0.44736954205362817 acc=0.8434782608695652\n",
      "test: loss=0.4830435828281317 acc=0.8376811594202899\n",
      "EPOCH=350\n",
      "train: loss=0.45208857403121183 acc=0.8420289855072464\n",
      "test: loss=0.4706990121807684 acc=0.8463768115942029\n",
      "EPOCH=351\n",
      "train: loss=0.41853382001252193 acc=0.8797101449275362\n",
      "test: loss=0.46342970259201655 acc=0.8420289855072464\n",
      "EPOCH=352\n",
      "train: loss=0.44931006439176113 acc=0.8695652173913043\n",
      "test: loss=0.4883525189106982 acc=0.836231884057971\n",
      "EPOCH=353\n",
      "train: loss=0.43830152046980597 acc=0.8579710144927536\n",
      "test: loss=0.4595496354518337 acc=0.8463768115942029\n",
      "EPOCH=354\n",
      "train: loss=0.41860657517384464 acc=0.8623188405797102\n",
      "test: loss=0.43756145368128985 acc=0.8478260869565217\n",
      "EPOCH=355\n",
      "train: loss=0.39232373480445476 acc=0.8855072463768116\n",
      "test: loss=0.41621126930254254 acc=0.8797101449275362\n",
      "EPOCH=356\n",
      "train: loss=0.3809582009871256 acc=0.881159420289855\n",
      "test: loss=0.4246830129979281 acc=0.8710144927536232\n",
      "EPOCH=357\n",
      "train: loss=0.40432006431445294 acc=0.8681159420289855\n",
      "test: loss=0.41115699839879266 acc=0.8724637681159421\n",
      "EPOCH=358\n",
      "train: loss=0.3601559198799436 acc=0.8956521739130435\n",
      "test: loss=0.40581783608961985 acc=0.8666666666666667\n",
      "EPOCH=359\n",
      "train: loss=0.3728841879249544 acc=0.8884057971014493\n",
      "test: loss=0.4344327731805843 acc=0.8652173913043478\n",
      "EPOCH=360\n",
      "train: loss=0.40880902608317926 acc=0.863768115942029\n",
      "test: loss=0.41438919761749804 acc=0.8840579710144928\n",
      "EPOCH=361\n",
      "train: loss=0.33880962294003797 acc=0.9014492753623189\n",
      "test: loss=0.41661456953228004 acc=0.8608695652173913\n",
      "EPOCH=362\n",
      "train: loss=0.3706169033240007 acc=0.8898550724637682\n",
      "test: loss=0.39293570086876467 acc=0.8855072463768116\n",
      "EPOCH=363\n",
      "train: loss=0.3689595630392854 acc=0.8927536231884058\n",
      "test: loss=0.3919380070544835 acc=0.8971014492753623\n",
      "EPOCH=364\n",
      "train: loss=0.3648339891912326 acc=0.8782608695652174\n",
      "test: loss=0.389771132995404 acc=0.8884057971014493\n",
      "EPOCH=365\n",
      "train: loss=0.3496425982472737 acc=0.9057971014492754\n",
      "test: loss=0.39220226766025745 acc=0.8898550724637682\n",
      "EPOCH=366\n",
      "train: loss=0.37975544494927266 acc=0.881159420289855\n",
      "test: loss=0.39574412837950596 acc=0.8855072463768116\n",
      "EPOCH=367\n",
      "train: loss=0.32571161210119803 acc=0.9028985507246376\n",
      "test: loss=0.3879770283771496 acc=0.8927536231884058\n",
      "EPOCH=368\n",
      "train: loss=0.3731021015307161 acc=0.8753623188405797\n",
      "test: loss=0.3886937214400741 acc=0.8913043478260869\n",
      "EPOCH=369\n",
      "train: loss=0.32655063152073444 acc=0.9188405797101449\n",
      "test: loss=0.3708375461931122 acc=0.8956521739130435\n",
      "EPOCH=370\n",
      "train: loss=0.3283438881016227 acc=0.9043478260869565\n",
      "test: loss=0.36883919734292264 acc=0.8927536231884058\n",
      "EPOCH=371\n",
      "train: loss=0.3620068881147587 acc=0.9101449275362319\n",
      "test: loss=0.39336868218980153 acc=0.8869565217391304\n",
      "EPOCH=372\n",
      "train: loss=0.3481715843416379 acc=0.8884057971014493\n",
      "test: loss=0.3521024880762169 acc=0.9043478260869565\n",
      "EPOCH=373\n",
      "train: loss=0.3783929924889125 acc=0.8840579710144928\n",
      "test: loss=0.37473131198542736 acc=0.9014492753623189\n",
      "EPOCH=374\n",
      "train: loss=0.38913903486957324 acc=0.8695652173913043\n",
      "test: loss=0.35809144695609785 acc=0.9014492753623189\n",
      "EPOCH=375\n",
      "train: loss=0.3373812979985656 acc=0.8782608695652174\n",
      "test: loss=0.35558740286478363 acc=0.9014492753623189\n",
      "EPOCH=376\n",
      "train: loss=0.33588287784436216 acc=0.9072463768115943\n",
      "test: loss=0.3384423200201232 acc=0.9144927536231884\n",
      "EPOCH=377\n",
      "train: loss=0.34390801896617 acc=0.9043478260869565\n",
      "test: loss=0.3251568938569846 acc=0.9260869565217391\n",
      "EPOCH=378\n",
      "train: loss=0.4037620970121508 acc=0.8724637681159421\n",
      "test: loss=0.40844251918666563 acc=0.863768115942029\n",
      "EPOCH=379\n",
      "train: loss=0.3787424391541234 acc=0.9130434782608695\n",
      "test: loss=0.3666698928906281 acc=0.9115942028985508\n",
      "EPOCH=380\n",
      "train: loss=0.4322557263126499 acc=0.8347826086956521\n",
      "test: loss=0.396424321234667 acc=0.8536231884057971\n",
      "EPOCH=381\n",
      "train: loss=0.3330427769942933 acc=0.8782608695652174\n",
      "test: loss=0.36455902184471084 acc=0.8913043478260869\n",
      "EPOCH=382\n",
      "train: loss=0.35172197471266914 acc=0.8898550724637682\n",
      "test: loss=0.3529616370060388 acc=0.8884057971014493\n",
      "EPOCH=383\n",
      "train: loss=0.32056136063516877 acc=0.8927536231884058\n",
      "test: loss=0.33013987231461217 acc=0.9072463768115943\n",
      "EPOCH=384\n",
      "train: loss=0.3276782811803455 acc=0.8869565217391304\n",
      "test: loss=0.32295813021783426 acc=0.9246376811594202\n",
      "EPOCH=385\n",
      "train: loss=0.3363804333166521 acc=0.908695652173913\n",
      "test: loss=0.3327641729917546 acc=0.9\n",
      "EPOCH=386\n",
      "train: loss=0.30466450350491947 acc=0.9231884057971015\n",
      "test: loss=0.3149861809833965 acc=0.9246376811594202\n",
      "EPOCH=387\n",
      "train: loss=0.28130907044180564 acc=0.9159420289855073\n",
      "test: loss=0.3264431371757435 acc=0.9159420289855073\n",
      "EPOCH=388\n",
      "train: loss=0.29184930078961346 acc=0.9289855072463769\n",
      "test: loss=0.3133520740755953 acc=0.9202898550724637\n",
      "EPOCH=389\n",
      "train: loss=0.33409552240910423 acc=0.8826086956521739\n",
      "test: loss=0.32923949548403736 acc=0.8985507246376812\n",
      "EPOCH=390\n",
      "train: loss=0.2869730135203007 acc=0.9420289855072463\n",
      "test: loss=0.2952198934629437 acc=0.936231884057971\n",
      "EPOCH=391\n",
      "train: loss=0.3127882435661298 acc=0.9057971014492754\n",
      "test: loss=0.30383928979750585 acc=0.9130434782608695\n",
      "EPOCH=392\n",
      "train: loss=0.2625816878146419 acc=0.9231884057971015\n",
      "test: loss=0.288351342837964 acc=0.9260869565217391\n",
      "EPOCH=393\n",
      "train: loss=0.29526715895211225 acc=0.8956521739130435\n",
      "test: loss=0.29607563130437325 acc=0.9217391304347826\n",
      "EPOCH=394\n",
      "train: loss=0.2953733357363503 acc=0.9144927536231884\n",
      "test: loss=0.2907405523108458 acc=0.9434782608695652\n",
      "EPOCH=395\n",
      "train: loss=0.30257905228048704 acc=0.9144927536231884\n",
      "test: loss=0.29266434272103126 acc=0.9565217391304348\n",
      "EPOCH=396\n",
      "train: loss=0.3093936006825179 acc=0.8840579710144928\n",
      "test: loss=0.31290639999106 acc=0.8956521739130435\n",
      "EPOCH=397\n",
      "train: loss=0.3143815210178508 acc=0.9246376811594202\n",
      "test: loss=0.3071625446196434 acc=0.9376811594202898\n",
      "EPOCH=398\n",
      "train: loss=0.39812231416756244 acc=0.8333333333333334\n",
      "test: loss=0.41750515668092597 acc=0.8159420289855073\n",
      "EPOCH=399\n",
      "train: loss=0.3023454428545728 acc=0.9101449275362319\n",
      "test: loss=0.29499547223668493 acc=0.9231884057971015\n",
      "EPOCH=400\n",
      "train: loss=0.2953466354078652 acc=0.908695652173913\n",
      "test: loss=0.26852047588125033 acc=0.9289855072463769\n",
      "EPOCH=401\n",
      "train: loss=0.24163324673450468 acc=0.9449275362318841\n",
      "test: loss=0.27050408961487477 acc=0.9507246376811594\n",
      "EPOCH=402\n",
      "train: loss=0.2670111889003053 acc=0.9130434782608695\n",
      "test: loss=0.2966605057756617 acc=0.9159420289855073\n",
      "EPOCH=403\n",
      "train: loss=0.2562552453046036 acc=0.9579710144927536\n",
      "test: loss=0.2757514616318656 acc=0.9492753623188406\n",
      "EPOCH=404\n",
      "train: loss=0.26201366705538837 acc=0.9260869565217391\n",
      "test: loss=0.271557652480144 acc=0.9202898550724637\n",
      "EPOCH=405\n",
      "train: loss=0.26945867486001984 acc=0.9202898550724637\n",
      "test: loss=0.253466570432721 acc=0.9492753623188406\n",
      "EPOCH=406\n",
      "train: loss=0.25271744972558446 acc=0.9347826086956522\n",
      "test: loss=0.2585643729179392 acc=0.9405797101449276\n",
      "EPOCH=407\n",
      "train: loss=0.2999521012705214 acc=0.908695652173913\n",
      "test: loss=0.31280681793731396 acc=0.927536231884058\n",
      "EPOCH=408\n",
      "train: loss=0.2795976070812132 acc=0.9231884057971015\n",
      "test: loss=0.2804641983671741 acc=0.9173913043478261\n",
      "EPOCH=409\n",
      "train: loss=0.23313681330288566 acc=0.9289855072463769\n",
      "test: loss=0.25693962461818826 acc=0.9347826086956522\n",
      "EPOCH=410\n",
      "train: loss=0.28928255479295856 acc=0.9260869565217391\n",
      "test: loss=0.2662844372922152 acc=0.9536231884057971\n",
      "EPOCH=411\n",
      "train: loss=0.23470407912034294 acc=0.946376811594203\n",
      "test: loss=0.2385647822353665 acc=0.9579710144927536\n",
      "EPOCH=412\n",
      "train: loss=0.2661352922496904 acc=0.9304347826086956\n",
      "test: loss=0.24704621849688946 acc=0.9405797101449276\n",
      "EPOCH=413\n",
      "train: loss=0.25949420354370595 acc=0.9434782608695652\n",
      "test: loss=0.24660141334348684 acc=0.9376811594202898\n",
      "EPOCH=414\n",
      "train: loss=0.2233036807579175 acc=0.9521739130434783\n",
      "test: loss=0.2382201443979231 acc=0.9536231884057971\n",
      "EPOCH=415\n",
      "train: loss=0.2658959494866671 acc=0.9376811594202898\n",
      "test: loss=0.23181761771680823 acc=0.9579710144927536\n",
      "EPOCH=416\n",
      "train: loss=0.3404873029533137 acc=0.9072463768115943\n",
      "test: loss=0.298945321354135 acc=0.927536231884058\n",
      "EPOCH=417\n",
      "train: loss=0.2858906525062343 acc=0.927536231884058\n",
      "test: loss=0.2677255469544447 acc=0.9289855072463769\n",
      "EPOCH=418\n",
      "train: loss=0.30744050125809946 acc=0.8826086956521739\n",
      "test: loss=0.2875457139052478 acc=0.9231884057971015\n",
      "EPOCH=419\n",
      "train: loss=0.2211757061919132 acc=0.9579710144927536\n",
      "test: loss=0.24485879437655228 acc=0.9565217391304348\n",
      "EPOCH=420\n",
      "train: loss=0.2207467050274937 acc=0.9521739130434783\n",
      "test: loss=0.2278642845073657 acc=0.9608695652173913\n",
      "EPOCH=421\n",
      "train: loss=0.24615452605682642 acc=0.9594202898550724\n",
      "test: loss=0.23213785973161777 acc=0.9507246376811594\n",
      "EPOCH=422\n",
      "train: loss=0.22769018415152012 acc=0.9449275362318841\n",
      "test: loss=0.24337491639557085 acc=0.936231884057971\n",
      "EPOCH=423\n",
      "train: loss=0.20453443550024425 acc=0.9739130434782609\n",
      "test: loss=0.23435346658925876 acc=0.9521739130434783\n",
      "EPOCH=424\n",
      "train: loss=0.23686742748435843 acc=0.9405797101449276\n",
      "test: loss=0.22174252132493733 acc=0.9594202898550724\n",
      "EPOCH=425\n",
      "train: loss=0.21744753156244637 acc=0.9376811594202898\n",
      "test: loss=0.24197238078182715 acc=0.9376811594202898\n",
      "EPOCH=426\n",
      "train: loss=0.24134194132796213 acc=0.9536231884057971\n",
      "test: loss=0.2287052450317234 acc=0.9449275362318841\n",
      "EPOCH=427\n",
      "train: loss=0.22269387417312617 acc=0.9608695652173913\n",
      "test: loss=0.21502486487691425 acc=0.9652173913043478\n",
      "EPOCH=428\n",
      "train: loss=0.26464532777332034 acc=0.9318840579710145\n",
      "test: loss=0.2387747008814546 acc=0.9318840579710145\n",
      "EPOCH=429\n",
      "train: loss=0.24133148601430932 acc=0.9376811594202898\n",
      "test: loss=0.2268517250387484 acc=0.9492753623188406\n",
      "EPOCH=430\n",
      "train: loss=0.24618926719612436 acc=0.9318840579710145\n",
      "test: loss=0.2391754813922054 acc=0.9478260869565217\n",
      "EPOCH=431\n",
      "train: loss=0.2240986595225811 acc=0.9478260869565217\n",
      "test: loss=0.21283763243333417 acc=0.9608695652173913\n",
      "EPOCH=432\n",
      "train: loss=0.2753005738875061 acc=0.9\n",
      "test: loss=0.26333816863231163 acc=0.927536231884058\n",
      "EPOCH=433\n",
      "train: loss=0.18711115081753013 acc=0.9681159420289855\n",
      "test: loss=0.21096849598757547 acc=0.9652173913043478\n",
      "EPOCH=434\n",
      "train: loss=0.19703511254243178 acc=0.946376811594203\n",
      "test: loss=0.21241437868842353 acc=0.9536231884057971\n",
      "EPOCH=435\n",
      "train: loss=0.203563155338154 acc=0.9594202898550724\n",
      "test: loss=0.20452400837369075 acc=0.9695652173913043\n",
      "EPOCH=436\n",
      "train: loss=0.2291631943017358 acc=0.9652173913043478\n",
      "test: loss=0.21398608662629146 acc=0.9608695652173913\n",
      "EPOCH=437\n",
      "train: loss=0.19819489548527877 acc=0.9652173913043478\n",
      "test: loss=0.19728637225916695 acc=0.9594202898550724\n",
      "EPOCH=438\n",
      "train: loss=0.20757951633540211 acc=0.9376811594202898\n",
      "test: loss=0.20870763357933605 acc=0.9565217391304348\n",
      "EPOCH=439\n",
      "train: loss=0.19987032088240506 acc=0.9579710144927536\n",
      "test: loss=0.21236436486544097 acc=0.9594202898550724\n",
      "EPOCH=440\n",
      "train: loss=0.1976961701925377 acc=0.9652173913043478\n",
      "test: loss=0.20167007830075 acc=0.9565217391304348\n",
      "EPOCH=441\n",
      "train: loss=0.21112821913519692 acc=0.972463768115942\n",
      "test: loss=0.19783789016235961 acc=0.972463768115942\n",
      "EPOCH=442\n",
      "train: loss=0.19191758751496862 acc=0.9623188405797102\n",
      "test: loss=0.21517973228922832 acc=0.9594202898550724\n",
      "EPOCH=443\n",
      "train: loss=0.2631714850383698 acc=0.9231884057971015\n",
      "test: loss=0.21188677061895958 acc=0.9492753623188406\n",
      "EPOCH=444\n",
      "train: loss=0.1967730679029204 acc=0.946376811594203\n",
      "test: loss=0.20455494528157125 acc=0.9579710144927536\n",
      "EPOCH=445\n",
      "train: loss=0.19146088032044065 acc=0.9550724637681159\n",
      "test: loss=0.18888553592079943 acc=0.9768115942028985\n",
      "EPOCH=446\n",
      "train: loss=0.19058980140192863 acc=0.972463768115942\n",
      "test: loss=0.19610361973545745 acc=0.9637681159420289\n",
      "EPOCH=447\n",
      "train: loss=0.17171089104308718 acc=0.9710144927536232\n",
      "test: loss=0.17957848332902937 acc=0.972463768115942\n",
      "EPOCH=448\n",
      "train: loss=0.1988795066011216 acc=0.9623188405797102\n",
      "test: loss=0.1985020532850061 acc=0.9666666666666667\n",
      "EPOCH=449\n",
      "train: loss=0.210903425976173 acc=0.9449275362318841\n",
      "test: loss=0.2039474398124672 acc=0.9579710144927536\n",
      "EPOCH=450\n",
      "train: loss=0.18446366338620715 acc=0.9637681159420289\n",
      "test: loss=0.18572377526524825 acc=0.9739130434782609\n",
      "EPOCH=451\n",
      "train: loss=0.18502777287888855 acc=0.9594202898550724\n",
      "test: loss=0.19832228577036576 acc=0.9550724637681159\n",
      "EPOCH=452\n",
      "train: loss=0.1709923575239636 acc=0.972463768115942\n",
      "test: loss=0.19505721998166467 acc=0.9652173913043478\n",
      "EPOCH=453\n",
      "train: loss=0.16928350222427505 acc=0.9579710144927536\n",
      "test: loss=0.18157435352788023 acc=0.9637681159420289\n",
      "EPOCH=454\n",
      "train: loss=0.19770184493087414 acc=0.9666666666666667\n",
      "test: loss=0.18754797505999454 acc=0.9739130434782609\n",
      "EPOCH=455\n",
      "train: loss=0.20901255627870632 acc=0.9666666666666667\n",
      "test: loss=0.17895035013951174 acc=0.9608695652173913\n",
      "EPOCH=456\n",
      "train: loss=0.19885985059612774 acc=0.9608695652173913\n",
      "test: loss=0.19285852109928586 acc=0.9579710144927536\n",
      "EPOCH=457\n",
      "train: loss=0.21843730225761343 acc=0.9536231884057971\n",
      "test: loss=0.2026431884632299 acc=0.9536231884057971\n",
      "EPOCH=458\n",
      "train: loss=0.1604602953723124 acc=0.9608695652173913\n",
      "test: loss=0.1935669896744187 acc=0.9565217391304348\n",
      "EPOCH=459\n",
      "train: loss=0.2541291277698358 acc=0.9449275362318841\n",
      "test: loss=0.1937941678016706 acc=0.9608695652173913\n",
      "EPOCH=460\n",
      "train: loss=0.24708602408174318 acc=0.9304347826086956\n",
      "test: loss=0.23047485126770997 acc=0.927536231884058\n",
      "EPOCH=461\n",
      "train: loss=0.1815012631618352 acc=0.9492753623188406\n",
      "test: loss=0.18783546389798478 acc=0.9681159420289855\n",
      "EPOCH=462\n",
      "train: loss=0.1788577150514607 acc=0.9666666666666667\n",
      "test: loss=0.2063490605055594 acc=0.9579710144927536\n",
      "EPOCH=463\n",
      "train: loss=0.17658085378671534 acc=0.9449275362318841\n",
      "test: loss=0.1830963106758197 acc=0.9753623188405797\n",
      "EPOCH=464\n",
      "train: loss=0.18586944859405882 acc=0.9637681159420289\n",
      "test: loss=0.17854018117178794 acc=0.9579710144927536\n",
      "EPOCH=465\n",
      "train: loss=0.20816025080758677 acc=0.9405797101449276\n",
      "test: loss=0.18834322612634372 acc=0.9536231884057971\n",
      "EPOCH=466\n",
      "train: loss=0.19327525757489375 acc=0.9507246376811594\n",
      "test: loss=0.17587848933856193 acc=0.9637681159420289\n",
      "EPOCH=467\n",
      "train: loss=0.14473563308194992 acc=0.9768115942028985\n",
      "test: loss=0.17398218593980214 acc=0.972463768115942\n",
      "EPOCH=468\n",
      "train: loss=0.19053635982973915 acc=0.9405797101449276\n",
      "test: loss=0.218978946160032 acc=0.9405797101449276\n",
      "EPOCH=469\n",
      "train: loss=0.16490988081064722 acc=0.9681159420289855\n",
      "test: loss=0.1892644141962038 acc=0.9637681159420289\n",
      "EPOCH=470\n",
      "train: loss=0.16868613184842748 acc=0.9637681159420289\n",
      "test: loss=0.18299138932886136 acc=0.9594202898550724\n",
      "EPOCH=471\n",
      "train: loss=0.20820729204113614 acc=0.9507246376811594\n",
      "test: loss=0.17169469922369054 acc=0.9681159420289855\n",
      "EPOCH=472\n",
      "train: loss=0.1728542911650605 acc=0.9623188405797102\n",
      "test: loss=0.16851826106559872 acc=0.9739130434782609\n",
      "EPOCH=473\n",
      "train: loss=0.1728376091246848 acc=0.9710144927536232\n",
      "test: loss=0.15864503222367246 acc=0.9710144927536232\n",
      "EPOCH=474\n",
      "train: loss=0.16946216079245 acc=0.9623188405797102\n",
      "test: loss=0.16830420139457325 acc=0.972463768115942\n",
      "EPOCH=475\n",
      "train: loss=0.19811005530290446 acc=0.9666666666666667\n",
      "test: loss=0.17314937224442026 acc=0.9637681159420289\n",
      "EPOCH=476\n",
      "train: loss=0.14436311545580174 acc=0.9782608695652174\n",
      "test: loss=0.16693251712632853 acc=0.9695652173913043\n",
      "EPOCH=477\n",
      "train: loss=0.1770596325853926 acc=0.9637681159420289\n",
      "test: loss=0.1713622378493644 acc=0.9695652173913043\n",
      "EPOCH=478\n",
      "train: loss=0.15056547620868224 acc=0.9768115942028985\n",
      "test: loss=0.1571213857775863 acc=0.9782608695652174\n",
      "EPOCH=479\n",
      "train: loss=0.17745168936787156 acc=0.9594202898550724\n",
      "test: loss=0.16844656283262166 acc=0.9695652173913043\n",
      "EPOCH=480\n",
      "train: loss=0.1620097965123985 acc=0.972463768115942\n",
      "test: loss=0.16401525815253462 acc=0.9797101449275363\n",
      "EPOCH=481\n",
      "train: loss=0.15074685795093568 acc=0.9753623188405797\n",
      "test: loss=0.160129504633361 acc=0.972463768115942\n",
      "EPOCH=482\n",
      "train: loss=0.1558778766030667 acc=0.9681159420289855\n",
      "test: loss=0.16918445987609196 acc=0.9652173913043478\n",
      "EPOCH=483\n",
      "train: loss=0.17748992158931945 acc=0.9579710144927536\n",
      "test: loss=0.16031049525153968 acc=0.9739130434782609\n",
      "EPOCH=484\n",
      "train: loss=0.14648811914010076 acc=0.9710144927536232\n",
      "test: loss=0.15546063194702794 acc=0.9681159420289855\n",
      "EPOCH=485\n",
      "train: loss=0.15947614368769922 acc=0.9623188405797102\n",
      "test: loss=0.1777023245732077 acc=0.9681159420289855\n",
      "EPOCH=486\n",
      "train: loss=0.14838232723680356 acc=0.9739130434782609\n",
      "test: loss=0.14729317384256754 acc=0.9753623188405797\n",
      "EPOCH=487\n",
      "train: loss=0.191538924939112 acc=0.9594202898550724\n",
      "test: loss=0.16189809722223747 acc=0.9753623188405797\n",
      "EPOCH=488\n",
      "train: loss=0.20684792073770686 acc=0.9391304347826087\n",
      "test: loss=0.22190642870823796 acc=0.9478260869565217\n",
      "EPOCH=489\n",
      "train: loss=0.24391094900715804 acc=0.936231884057971\n",
      "test: loss=0.20393982121792192 acc=0.9420289855072463\n",
      "EPOCH=490\n",
      "train: loss=0.18803640835308413 acc=0.9521739130434783\n",
      "test: loss=0.17791601075163718 acc=0.9550724637681159\n",
      "EPOCH=491\n",
      "train: loss=0.1573134947493705 acc=0.9681159420289855\n",
      "test: loss=0.16624399307079263 acc=0.9652173913043478\n",
      "EPOCH=492\n",
      "train: loss=0.24166682136480855 acc=0.9072463768115943\n",
      "test: loss=0.20328199466530614 acc=0.9376811594202898\n",
      "EPOCH=493\n",
      "train: loss=0.14885639709942103 acc=0.9710144927536232\n",
      "test: loss=0.1537502319493889 acc=0.9594202898550724\n",
      "EPOCH=494\n",
      "train: loss=0.1510505707811203 acc=0.9652173913043478\n",
      "test: loss=0.1643729331217572 acc=0.9681159420289855\n",
      "EPOCH=495\n",
      "train: loss=0.16031891357600123 acc=0.9565217391304348\n",
      "test: loss=0.173630295463444 acc=0.9536231884057971\n",
      "EPOCH=496\n",
      "train: loss=0.16961836559115698 acc=0.9492753623188406\n",
      "test: loss=0.15754565117166736 acc=0.9739130434782609\n",
      "EPOCH=497\n",
      "train: loss=0.11873457573949343 acc=0.9797101449275363\n",
      "test: loss=0.15075928302862518 acc=0.972463768115942\n",
      "EPOCH=498\n",
      "train: loss=0.12326553127207829 acc=0.9710144927536232\n",
      "test: loss=0.1524509534484278 acc=0.9695652173913043\n",
      "EPOCH=499\n",
      "train: loss=0.17045232687373688 acc=0.9710144927536232\n",
      "test: loss=0.15791009357291869 acc=0.9666666666666667\n",
      "EPOCH=500\n",
      "train: loss=0.12797997142066325 acc=0.9695652173913043\n",
      "test: loss=0.14372397582568922 acc=0.9797101449275363\n",
      "EPOCH=501\n",
      "train: loss=0.15080727219912818 acc=0.9695652173913043\n",
      "test: loss=0.14927571649150723 acc=0.9739130434782609\n",
      "EPOCH=502\n",
      "train: loss=0.1702068802295482 acc=0.9695652173913043\n",
      "test: loss=0.15281310959913963 acc=0.9652173913043478\n",
      "EPOCH=503\n",
      "train: loss=0.16345898750876428 acc=0.9623188405797102\n",
      "test: loss=0.1456561078372376 acc=0.9768115942028985\n",
      "EPOCH=504\n",
      "train: loss=0.1670786907667431 acc=0.9623188405797102\n",
      "test: loss=0.15315053233561216 acc=0.9652173913043478\n",
      "EPOCH=505\n",
      "train: loss=0.1590407797323665 acc=0.9652173913043478\n",
      "test: loss=0.14880564524896925 acc=0.9710144927536232\n",
      "EPOCH=506\n",
      "train: loss=0.11942007672172147 acc=0.9782608695652174\n",
      "test: loss=0.15589421257518288 acc=0.9623188405797102\n",
      "EPOCH=507\n",
      "train: loss=0.15140462412589684 acc=0.9797101449275363\n",
      "test: loss=0.16642241202435284 acc=0.9579710144927536\n",
      "EPOCH=508\n",
      "train: loss=0.12606202135799532 acc=0.9739130434782609\n",
      "test: loss=0.14078722924325543 acc=0.9739130434782609\n",
      "EPOCH=509\n",
      "train: loss=0.1530535265040627 acc=0.972463768115942\n",
      "test: loss=0.1455288706080034 acc=0.972463768115942\n",
      "EPOCH=510\n",
      "train: loss=0.12563146555429464 acc=0.9695652173913043\n",
      "test: loss=0.13702188499333398 acc=0.9782608695652174\n",
      "EPOCH=511\n",
      "train: loss=0.1343728301246209 acc=0.9753623188405797\n",
      "test: loss=0.14154875499357134 acc=0.981159420289855\n",
      "EPOCH=512\n",
      "train: loss=0.17841443662560066 acc=0.9579710144927536\n",
      "test: loss=0.14306623564457435 acc=0.9782608695652174\n",
      "EPOCH=513\n",
      "train: loss=0.15428889806718107 acc=0.9536231884057971\n",
      "test: loss=0.14827390219204353 acc=0.9695652173913043\n",
      "EPOCH=514\n",
      "train: loss=0.1732273247445899 acc=0.9594202898550724\n",
      "test: loss=0.14575029345987786 acc=0.972463768115942\n",
      "EPOCH=515\n",
      "train: loss=0.16404912646351122 acc=0.9681159420289855\n",
      "test: loss=0.1346846716939963 acc=0.9753623188405797\n",
      "EPOCH=516\n",
      "train: loss=0.1042803151609827 acc=0.9855072463768116\n",
      "test: loss=0.1365892044361765 acc=0.9797101449275363\n",
      "EPOCH=517\n",
      "train: loss=0.1529542693942423 acc=0.9594202898550724\n",
      "test: loss=0.14207147744847015 acc=0.972463768115942\n",
      "EPOCH=518\n",
      "train: loss=0.12229252288600855 acc=0.9797101449275363\n",
      "test: loss=0.1400202642928131 acc=0.9681159420289855\n",
      "EPOCH=519\n",
      "train: loss=0.14570758013760496 acc=0.9608695652173913\n",
      "test: loss=0.12730939695922078 acc=0.9753623188405797\n",
      "EPOCH=520\n",
      "train: loss=0.11209526975481834 acc=0.981159420289855\n",
      "test: loss=0.13565003102148165 acc=0.9782608695652174\n",
      "EPOCH=521\n",
      "train: loss=0.11698248145442752 acc=0.9739130434782609\n",
      "test: loss=0.13840131086482305 acc=0.972463768115942\n",
      "EPOCH=522\n",
      "train: loss=0.11118192822454465 acc=0.9753623188405797\n",
      "test: loss=0.14388794727140758 acc=0.9753623188405797\n",
      "EPOCH=523\n",
      "train: loss=0.15571672450081336 acc=0.9594202898550724\n",
      "test: loss=0.17115551154656056 acc=0.9608695652173913\n",
      "EPOCH=524\n",
      "train: loss=0.12445105070334345 acc=0.9637681159420289\n",
      "test: loss=0.14450327305134178 acc=0.9710144927536232\n",
      "EPOCH=525\n",
      "train: loss=0.14439281969007028 acc=0.972463768115942\n",
      "test: loss=0.14035307865542235 acc=0.9768115942028985\n",
      "EPOCH=526\n",
      "train: loss=0.15185443038351984 acc=0.9565217391304348\n",
      "test: loss=0.13619606779622198 acc=0.9695652173913043\n",
      "EPOCH=527\n",
      "train: loss=0.14547791031076934 acc=0.9695652173913043\n",
      "test: loss=0.14197924048296334 acc=0.9666666666666667\n",
      "EPOCH=528\n",
      "train: loss=0.2422897014415458 acc=0.9188405797101449\n",
      "test: loss=0.21379447948481053 acc=0.9391304347826087\n",
      "EPOCH=529\n",
      "train: loss=0.1281407138509114 acc=0.9652173913043478\n",
      "test: loss=0.1679738913766157 acc=0.9449275362318841\n",
      "EPOCH=530\n",
      "train: loss=0.2974611084890057 acc=0.9101449275362319\n",
      "test: loss=0.24072277282119736 acc=0.9188405797101449\n",
      "EPOCH=531\n",
      "train: loss=0.14814099648342496 acc=0.9623188405797102\n",
      "test: loss=0.17304535921748143 acc=0.946376811594203\n",
      "EPOCH=532\n",
      "train: loss=0.25949750875554367 acc=0.9246376811594202\n",
      "test: loss=0.2569765720137355 acc=0.9159420289855073\n",
      "EPOCH=533\n",
      "train: loss=0.24644000094195306 acc=0.9\n",
      "test: loss=0.2563946446065898 acc=0.9043478260869565\n",
      "EPOCH=534\n",
      "train: loss=0.2803108223063936 acc=0.9043478260869565\n",
      "test: loss=0.2583966699588822 acc=0.9173913043478261\n",
      "EPOCH=535\n",
      "train: loss=0.1931753263317172 acc=0.9434782608695652\n",
      "test: loss=0.1907976735531566 acc=0.9478260869565217\n",
      "EPOCH=536\n",
      "train: loss=0.21402583672844244 acc=0.9347826086956522\n",
      "test: loss=0.1825411434529575 acc=0.9289855072463769\n",
      "EPOCH=537\n",
      "train: loss=0.163438027712273 acc=0.9565217391304348\n",
      "test: loss=0.15351104560294634 acc=0.9521739130434783\n",
      "EPOCH=538\n",
      "train: loss=0.1309301939291628 acc=0.9579710144927536\n",
      "test: loss=0.1418728376598955 acc=0.9681159420289855\n",
      "EPOCH=539\n",
      "train: loss=0.1983696498036095 acc=0.9405797101449276\n",
      "test: loss=0.1769510134585579 acc=0.9492753623188406\n",
      "EPOCH=540\n",
      "train: loss=0.1284283312445858 acc=0.972463768115942\n",
      "test: loss=0.1541215030784005 acc=0.9565217391304348\n",
      "EPOCH=541\n",
      "train: loss=0.15929272114392165 acc=0.9652173913043478\n",
      "test: loss=0.1379219572306573 acc=0.9782608695652174\n",
      "EPOCH=542\n",
      "train: loss=0.12468517449935158 acc=0.9710144927536232\n",
      "test: loss=0.1337378213524668 acc=0.9710144927536232\n",
      "EPOCH=543\n",
      "train: loss=0.147073221452686 acc=0.9550724637681159\n",
      "test: loss=0.1508937624622449 acc=0.9652173913043478\n",
      "EPOCH=544\n",
      "train: loss=0.13920161389591804 acc=0.9695652173913043\n",
      "test: loss=0.13930123219567736 acc=0.9666666666666667\n",
      "EPOCH=545\n",
      "train: loss=0.15784658131454873 acc=0.9579710144927536\n",
      "test: loss=0.14842380543915906 acc=0.9637681159420289\n",
      "EPOCH=546\n",
      "train: loss=0.1210419385846802 acc=0.9695652173913043\n",
      "test: loss=0.14120507901781384 acc=0.9652173913043478\n",
      "EPOCH=547\n",
      "train: loss=0.17749299872615784 acc=0.9623188405797102\n",
      "test: loss=0.16869440761412027 acc=0.9449275362318841\n",
      "EPOCH=548\n",
      "train: loss=0.12317606449157237 acc=0.9753623188405797\n",
      "test: loss=0.132583632864054 acc=0.9739130434782609\n",
      "EPOCH=549\n",
      "train: loss=0.1606422822830985 acc=0.9608695652173913\n",
      "test: loss=0.1438531529991016 acc=0.9666666666666667\n",
      "EPOCH=550\n",
      "train: loss=0.19698702818645805 acc=0.9478260869565217\n",
      "test: loss=0.15343915795385224 acc=0.9608695652173913\n",
      "EPOCH=551\n",
      "train: loss=0.1666778630244535 acc=0.9579710144927536\n",
      "test: loss=0.14285216296631084 acc=0.9594202898550724\n",
      "EPOCH=552\n",
      "train: loss=0.11024537742969516 acc=0.981159420289855\n",
      "test: loss=0.13248660062143416 acc=0.972463768115942\n",
      "EPOCH=553\n",
      "train: loss=0.15318168408127045 acc=0.9565217391304348\n",
      "test: loss=0.12719356885965838 acc=0.9695652173913043\n",
      "EPOCH=554\n",
      "train: loss=0.1460467032936249 acc=0.9666666666666667\n",
      "test: loss=0.13083169529179256 acc=0.9753623188405797\n",
      "EPOCH=555\n",
      "train: loss=0.16428205683814734 acc=0.9681159420289855\n",
      "test: loss=0.13045529923155486 acc=0.9753623188405797\n",
      "EPOCH=556\n",
      "train: loss=0.1560276543719362 acc=0.9521739130434783\n",
      "test: loss=0.14929069486375685 acc=0.9608695652173913\n",
      "EPOCH=557\n",
      "train: loss=0.12515288327798957 acc=0.9681159420289855\n",
      "test: loss=0.14261311056074905 acc=0.9579710144927536\n",
      "EPOCH=558\n",
      "train: loss=0.17291567541474778 acc=0.9449275362318841\n",
      "test: loss=0.16846964756797483 acc=0.9579710144927536\n",
      "EPOCH=559\n",
      "train: loss=0.10982657394614222 acc=0.9681159420289855\n",
      "test: loss=0.12729332578020952 acc=0.9681159420289855\n",
      "EPOCH=560\n",
      "train: loss=0.11868753597608293 acc=0.9710144927536232\n",
      "test: loss=0.11965692083704989 acc=0.9768115942028985\n",
      "EPOCH=561\n",
      "train: loss=0.1679544017291497 acc=0.9579710144927536\n",
      "test: loss=0.1291180816946191 acc=0.9666666666666667\n",
      "EPOCH=562\n",
      "train: loss=0.0746043031176952 acc=0.9840579710144928\n",
      "test: loss=0.12310918865090097 acc=0.972463768115942\n",
      "EPOCH=563\n",
      "train: loss=0.09994458691073727 acc=0.9797101449275363\n",
      "test: loss=0.12188920216697335 acc=0.9768115942028985\n",
      "EPOCH=564\n",
      "train: loss=0.11112771425704246 acc=0.9623188405797102\n",
      "test: loss=0.1332703608433454 acc=0.9652173913043478\n",
      "EPOCH=565\n",
      "train: loss=0.14344524498982325 acc=0.972463768115942\n",
      "test: loss=0.12527963694653962 acc=0.9695652173913043\n",
      "EPOCH=566\n",
      "train: loss=0.11116675308706797 acc=0.9652173913043478\n",
      "test: loss=0.12822094436296208 acc=0.9652173913043478\n",
      "EPOCH=567\n",
      "train: loss=0.10568257759337886 acc=0.972463768115942\n",
      "test: loss=0.13038564509982847 acc=0.9710144927536232\n",
      "EPOCH=568\n",
      "train: loss=0.11484641999960345 acc=0.972463768115942\n",
      "test: loss=0.12194936937672617 acc=0.9739130434782609\n",
      "EPOCH=569\n",
      "train: loss=0.12618287237269807 acc=0.9623188405797102\n",
      "test: loss=0.12432182852806271 acc=0.9753623188405797\n",
      "EPOCH=570\n",
      "train: loss=0.1485153036386138 acc=0.9565217391304348\n",
      "test: loss=0.11765963252517171 acc=0.9768115942028985\n",
      "EPOCH=571\n",
      "train: loss=0.12320597324305972 acc=0.9637681159420289\n",
      "test: loss=0.12251984729810644 acc=0.9768115942028985\n",
      "EPOCH=572\n",
      "train: loss=0.10195064237113449 acc=0.9768115942028985\n",
      "test: loss=0.12125855114453955 acc=0.9753623188405797\n",
      "EPOCH=573\n",
      "train: loss=0.1298439963946834 acc=0.972463768115942\n",
      "test: loss=0.12638067103528003 acc=0.9739130434782609\n",
      "EPOCH=574\n",
      "train: loss=0.14848430345318644 acc=0.9565217391304348\n",
      "test: loss=0.12888529488704858 acc=0.972463768115942\n",
      "EPOCH=575\n",
      "train: loss=0.18320251685751723 acc=0.9434782608695652\n",
      "test: loss=0.16312290921143516 acc=0.9521739130434783\n",
      "EPOCH=576\n",
      "train: loss=0.1441219338002196 acc=0.9637681159420289\n",
      "test: loss=0.12469422562129746 acc=0.972463768115942\n",
      "EPOCH=577\n",
      "train: loss=0.13887248512978198 acc=0.9623188405797102\n",
      "test: loss=0.12270556042913218 acc=0.9797101449275363\n",
      "EPOCH=578\n",
      "train: loss=0.09124130674419074 acc=0.9869565217391304\n",
      "test: loss=0.12968803850319116 acc=0.972463768115942\n",
      "EPOCH=579\n",
      "train: loss=0.13566940045307027 acc=0.9695652173913043\n",
      "test: loss=0.12020876309165085 acc=0.9739130434782609\n",
      "EPOCH=580\n",
      "train: loss=0.10099616853464624 acc=0.9695652173913043\n",
      "test: loss=0.11683002693622731 acc=0.9797101449275363\n",
      "EPOCH=581\n",
      "train: loss=0.10607851931821827 acc=0.9739130434782609\n",
      "test: loss=0.12340070723446157 acc=0.972463768115942\n",
      "EPOCH=582\n",
      "train: loss=0.12241377473856711 acc=0.9710144927536232\n",
      "test: loss=0.11814261915468363 acc=0.9753623188405797\n",
      "EPOCH=583\n",
      "train: loss=0.14144852138453406 acc=0.9623188405797102\n",
      "test: loss=0.12330658044979696 acc=0.9753623188405797\n",
      "EPOCH=584\n",
      "train: loss=0.08332967475259888 acc=0.9826086956521739\n",
      "test: loss=0.11819509545155571 acc=0.9782608695652174\n",
      "EPOCH=585\n",
      "train: loss=0.09731285082920017 acc=0.972463768115942\n",
      "test: loss=0.11760780067625322 acc=0.9782608695652174\n",
      "EPOCH=586\n",
      "train: loss=0.11876153924243991 acc=0.9768115942028985\n",
      "test: loss=0.12390212655055172 acc=0.9710144927536232\n",
      "EPOCH=587\n",
      "train: loss=0.16758809962505478 acc=0.9594202898550724\n",
      "test: loss=0.11885817845214872 acc=0.9782608695652174\n",
      "EPOCH=588\n",
      "train: loss=0.12262832580881218 acc=0.9753623188405797\n",
      "test: loss=0.12279407813223613 acc=0.9782608695652174\n",
      "EPOCH=589\n",
      "train: loss=0.12951405441126224 acc=0.9608695652173913\n",
      "test: loss=0.12426672302344345 acc=0.9753623188405797\n",
      "EPOCH=590\n",
      "train: loss=0.10684639638787079 acc=0.9710144927536232\n",
      "test: loss=0.11616827549452792 acc=0.9753623188405797\n",
      "EPOCH=591\n",
      "train: loss=0.13405076888648906 acc=0.9666666666666667\n",
      "test: loss=0.12366922084272408 acc=0.9710144927536232\n",
      "EPOCH=592\n",
      "train: loss=0.1156211098990548 acc=0.9710144927536232\n",
      "test: loss=0.1176726828312736 acc=0.9753623188405797\n",
      "EPOCH=593\n",
      "train: loss=0.09903399386912431 acc=0.9739130434782609\n",
      "test: loss=0.11600855920246365 acc=0.9768115942028985\n",
      "EPOCH=594\n",
      "train: loss=0.14557833655342692 acc=0.9579710144927536\n",
      "test: loss=0.11249965127338565 acc=0.9753623188405797\n",
      "EPOCH=595\n",
      "train: loss=0.1070716769742547 acc=0.9782608695652174\n",
      "test: loss=0.10984418122286932 acc=0.9753623188405797\n",
      "EPOCH=596\n",
      "train: loss=0.1444628908586936 acc=0.9739130434782609\n",
      "test: loss=0.11319148135045802 acc=0.972463768115942\n",
      "EPOCH=597\n",
      "train: loss=0.1498585273706178 acc=0.9666666666666667\n",
      "test: loss=0.11934954366136585 acc=0.9753623188405797\n",
      "EPOCH=598\n",
      "train: loss=0.07867412809244217 acc=0.9855072463768116\n",
      "test: loss=0.11290748015781585 acc=0.9753623188405797\n",
      "EPOCH=599\n",
      "train: loss=0.13619940342926687 acc=0.9608695652173913\n",
      "test: loss=0.11008143550876688 acc=0.9739130434782609\n",
      "EPOCH=600\n",
      "train: loss=0.11977519008061882 acc=0.9797101449275363\n",
      "test: loss=0.11266876586642417 acc=0.9782608695652174\n",
      "EPOCH=601\n",
      "train: loss=0.11182337821162788 acc=0.9768115942028985\n",
      "test: loss=0.11343106602923161 acc=0.9782608695652174\n",
      "EPOCH=602\n",
      "train: loss=0.13918372801740983 acc=0.9579710144927536\n",
      "test: loss=0.1403412545898147 acc=0.9710144927536232\n",
      "EPOCH=603\n",
      "train: loss=0.12984325907111033 acc=0.9507246376811594\n",
      "test: loss=0.12185784590788912 acc=0.9768115942028985\n",
      "EPOCH=604\n",
      "train: loss=0.199753932469326 acc=0.9420289855072463\n",
      "test: loss=0.16534725906306225 acc=0.9478260869565217\n",
      "EPOCH=605\n",
      "train: loss=0.17751259522489315 acc=0.9565217391304348\n",
      "test: loss=0.1283676443559908 acc=0.9652173913043478\n",
      "EPOCH=606\n",
      "train: loss=0.15957482568857542 acc=0.9594202898550724\n",
      "test: loss=0.15195054928566856 acc=0.9623188405797102\n",
      "EPOCH=607\n",
      "train: loss=0.1677931058626236 acc=0.9608695652173913\n",
      "test: loss=0.14269730570481312 acc=0.9565217391304348\n",
      "EPOCH=608\n",
      "train: loss=0.1341399195999518 acc=0.9608695652173913\n",
      "test: loss=0.12620246018745113 acc=0.9768115942028985\n",
      "EPOCH=609\n",
      "train: loss=0.15134300611218904 acc=0.9623188405797102\n",
      "test: loss=0.13415861349275526 acc=0.9579710144927536\n",
      "EPOCH=610\n",
      "train: loss=0.3340770630383943 acc=0.9101449275362319\n",
      "test: loss=0.2741997760486982 acc=0.9014492753623189\n",
      "EPOCH=611\n",
      "train: loss=0.17689134585286503 acc=0.9289855072463769\n",
      "test: loss=0.15323125419760825 acc=0.9449275362318841\n",
      "EPOCH=612\n",
      "train: loss=0.24960205797529536 acc=0.9260869565217391\n",
      "test: loss=0.18984655764598968 acc=0.9304347826086956\n",
      "EPOCH=613\n",
      "train: loss=0.23865881363647704 acc=0.908695652173913\n",
      "test: loss=0.25495396972287643 acc=0.9028985507246376\n",
      "EPOCH=614\n",
      "train: loss=0.1419584346810104 acc=0.9565217391304348\n",
      "test: loss=0.14668965094419287 acc=0.9579710144927536\n",
      "EPOCH=615\n",
      "train: loss=0.18657811564133858 acc=0.9376811594202898\n",
      "test: loss=0.16014929800197458 acc=0.9449275362318841\n",
      "EPOCH=616\n",
      "train: loss=0.0925159192898046 acc=0.9768115942028985\n",
      "test: loss=0.12028474699645417 acc=0.9695652173913043\n",
      "EPOCH=617\n",
      "train: loss=0.12864688035400587 acc=0.9666666666666667\n",
      "test: loss=0.14206756995085035 acc=0.946376811594203\n",
      "EPOCH=618\n",
      "train: loss=0.11552713886287982 acc=0.9695652173913043\n",
      "test: loss=0.11758698570110113 acc=0.9666666666666667\n",
      "EPOCH=619\n",
      "train: loss=0.11846673836481268 acc=0.9652173913043478\n",
      "test: loss=0.11929366231064462 acc=0.9681159420289855\n",
      "EPOCH=620\n",
      "train: loss=0.11789495862476344 acc=0.9608695652173913\n",
      "test: loss=0.11128704513758662 acc=0.9768115942028985\n",
      "EPOCH=621\n",
      "train: loss=0.10621775469128253 acc=0.9768115942028985\n",
      "test: loss=0.12064136302042341 acc=0.9782608695652174\n",
      "EPOCH=622\n",
      "train: loss=0.12103221196965458 acc=0.981159420289855\n",
      "test: loss=0.12374582034442483 acc=0.9695652173913043\n",
      "EPOCH=623\n",
      "train: loss=0.10260712200754667 acc=0.9710144927536232\n",
      "test: loss=0.12420539703298346 acc=0.9695652173913043\n",
      "EPOCH=624\n",
      "train: loss=0.09301302010118453 acc=0.9710144927536232\n",
      "test: loss=0.11665265910076586 acc=0.9753623188405797\n",
      "EPOCH=625\n",
      "train: loss=0.13255921610927207 acc=0.9681159420289855\n",
      "test: loss=0.11413076723352075 acc=0.9753623188405797\n",
      "EPOCH=626\n",
      "train: loss=0.10262709340365053 acc=0.9695652173913043\n",
      "test: loss=0.11522976226214585 acc=0.9753623188405797\n",
      "EPOCH=627\n",
      "train: loss=0.12091643675365418 acc=0.9739130434782609\n",
      "test: loss=0.1102640861138581 acc=0.9753623188405797\n",
      "EPOCH=628\n",
      "train: loss=0.11688610486491163 acc=0.9739130434782609\n",
      "test: loss=0.12164394004770611 acc=0.9710144927536232\n",
      "EPOCH=629\n",
      "train: loss=0.15385200166218754 acc=0.9695652173913043\n",
      "test: loss=0.1209270306343467 acc=0.972463768115942\n",
      "EPOCH=630\n",
      "train: loss=0.1768912718471401 acc=0.9550724637681159\n",
      "test: loss=0.11939228646206307 acc=0.9681159420289855\n",
      "EPOCH=631\n",
      "train: loss=0.14174646384359563 acc=0.9579710144927536\n",
      "test: loss=0.12669040506983142 acc=0.9652173913043478\n",
      "EPOCH=632\n",
      "train: loss=0.15235760996601216 acc=0.9579710144927536\n",
      "test: loss=0.11790981381498772 acc=0.9753623188405797\n",
      "EPOCH=633\n",
      "train: loss=0.19778520478604789 acc=0.9289855072463769\n",
      "test: loss=0.15019505265691688 acc=0.9478260869565217\n",
      "EPOCH=634\n",
      "train: loss=0.10541680273783864 acc=0.9739130434782609\n",
      "test: loss=0.1156869835779231 acc=0.972463768115942\n",
      "EPOCH=635\n",
      "train: loss=0.14875840238143306 acc=0.9695652173913043\n",
      "test: loss=0.12410427807872144 acc=0.9695652173913043\n",
      "EPOCH=636\n",
      "train: loss=0.13119614821993164 acc=0.9681159420289855\n",
      "test: loss=0.14397918312098867 acc=0.9637681159420289\n",
      "EPOCH=637\n",
      "train: loss=0.16793988503071214 acc=0.9376811594202898\n",
      "test: loss=0.178099278908673 acc=0.9492753623188406\n",
      "EPOCH=638\n",
      "train: loss=0.25729585061195076 acc=0.9101449275362319\n",
      "test: loss=0.21981628393672265 acc=0.9202898550724637\n",
      "EPOCH=639\n",
      "train: loss=0.08490780776698716 acc=0.9739130434782609\n",
      "test: loss=0.11244719416235274 acc=0.9681159420289855\n",
      "EPOCH=640\n",
      "train: loss=0.14849155214675339 acc=0.9376811594202898\n",
      "test: loss=0.13036834127763383 acc=0.9608695652173913\n",
      "EPOCH=641\n",
      "train: loss=0.18358677848316182 acc=0.946376811594203\n",
      "test: loss=0.17621455057694907 acc=0.9434782608695652\n",
      "EPOCH=642\n",
      "train: loss=0.09904533165323644 acc=0.9739130434782609\n",
      "test: loss=0.12635503862442043 acc=0.9666666666666667\n",
      "EPOCH=643\n",
      "train: loss=0.130770871807626 acc=0.9623188405797102\n",
      "test: loss=0.15642521544996402 acc=0.9492753623188406\n",
      "EPOCH=644\n",
      "train: loss=0.12501464695316952 acc=0.9681159420289855\n",
      "test: loss=0.10923478758487443 acc=0.9782608695652174\n",
      "EPOCH=645\n",
      "train: loss=0.10229599701431066 acc=0.9797101449275363\n",
      "test: loss=0.10814591750725254 acc=0.9768115942028985\n",
      "EPOCH=646\n",
      "train: loss=0.1172359061222498 acc=0.9666666666666667\n",
      "test: loss=0.10920626511430775 acc=0.9652173913043478\n",
      "EPOCH=647\n",
      "train: loss=0.12863380945647898 acc=0.9710144927536232\n",
      "test: loss=0.11192000864329782 acc=0.9782608695652174\n",
      "EPOCH=648\n",
      "train: loss=0.09064661030947224 acc=0.9695652173913043\n",
      "test: loss=0.1115895861661072 acc=0.9681159420289855\n",
      "EPOCH=649\n",
      "train: loss=0.11523248271224751 acc=0.9753623188405797\n",
      "test: loss=0.12105877904330133 acc=0.972463768115942\n",
      "EPOCH=650\n",
      "train: loss=0.08321715639528234 acc=0.9826086956521739\n",
      "test: loss=0.11767402588046097 acc=0.9623188405797102\n",
      "EPOCH=651\n",
      "train: loss=0.11114791455458109 acc=0.9652173913043478\n",
      "test: loss=0.12256293611903758 acc=0.9637681159420289\n",
      "EPOCH=652\n",
      "train: loss=0.14569315101040556 acc=0.9550724637681159\n",
      "test: loss=0.1256272775848787 acc=0.9666666666666667\n",
      "EPOCH=653\n",
      "train: loss=0.18554715307951233 acc=0.9579710144927536\n",
      "test: loss=0.12343636055408877 acc=0.9637681159420289\n",
      "EPOCH=654\n",
      "train: loss=0.1575823089193581 acc=0.9521739130434783\n",
      "test: loss=0.11641427741087748 acc=0.9739130434782609\n",
      "EPOCH=655\n",
      "train: loss=0.21630580443367328 acc=0.9608695652173913\n",
      "test: loss=0.11545472309823075 acc=0.9753623188405797\n",
      "EPOCH=656\n",
      "train: loss=0.15146237381862165 acc=0.9637681159420289\n",
      "test: loss=0.102817017273093 acc=0.981159420289855\n",
      "EPOCH=657\n",
      "train: loss=0.1284892197110208 acc=0.9623188405797102\n",
      "test: loss=0.10743326767836982 acc=0.9637681159420289\n",
      "EPOCH=658\n",
      "train: loss=0.10676521068168432 acc=0.972463768115942\n",
      "test: loss=0.09810473700760397 acc=0.9768115942028985\n",
      "EPOCH=659\n",
      "train: loss=0.09479636737248053 acc=0.9695652173913043\n",
      "test: loss=0.10629455126560007 acc=0.9710144927536232\n",
      "EPOCH=660\n",
      "train: loss=0.12829305067726254 acc=0.9695652173913043\n",
      "test: loss=0.10943118515346958 acc=0.9739130434782609\n",
      "EPOCH=661\n",
      "train: loss=0.09777370924332049 acc=0.9753623188405797\n",
      "test: loss=0.10950572224751505 acc=0.9695652173913043\n",
      "EPOCH=662\n",
      "train: loss=0.08569542889220765 acc=0.9782608695652174\n",
      "test: loss=0.10817693871378875 acc=0.972463768115942\n",
      "EPOCH=663\n",
      "train: loss=0.08533345232017843 acc=0.9797101449275363\n",
      "test: loss=0.10468379713556748 acc=0.9768115942028985\n",
      "EPOCH=664\n",
      "train: loss=0.09026638001996386 acc=0.9797101449275363\n",
      "test: loss=0.1114931257421598 acc=0.972463768115942\n",
      "EPOCH=665\n",
      "train: loss=0.1661375976256631 acc=0.9623188405797102\n",
      "test: loss=0.11369189740927318 acc=0.9652173913043478\n",
      "EPOCH=666\n",
      "train: loss=0.1398599811968822 acc=0.972463768115942\n",
      "test: loss=0.11025276269670854 acc=0.9666666666666667\n",
      "EPOCH=667\n",
      "train: loss=0.09211882442745284 acc=0.9695652173913043\n",
      "test: loss=0.10800430017580548 acc=0.972463768115942\n",
      "EPOCH=668\n",
      "train: loss=0.1254786501422293 acc=0.9681159420289855\n",
      "test: loss=0.12343826208889104 acc=0.9652173913043478\n",
      "EPOCH=669\n",
      "train: loss=0.09666444635971283 acc=0.972463768115942\n",
      "test: loss=0.11990919493304174 acc=0.9753623188405797\n",
      "EPOCH=670\n",
      "train: loss=0.08135471674708657 acc=0.9782608695652174\n",
      "test: loss=0.11354125536874259 acc=0.9681159420289855\n",
      "EPOCH=671\n",
      "train: loss=0.11864529178407489 acc=0.972463768115942\n",
      "test: loss=0.11056899789576137 acc=0.972463768115942\n",
      "EPOCH=672\n",
      "train: loss=0.132663216418876 acc=0.9753623188405797\n",
      "test: loss=0.13964817602975768 acc=0.9579710144927536\n",
      "EPOCH=673\n",
      "train: loss=0.13100978546249345 acc=0.9652173913043478\n",
      "test: loss=0.12231311794366083 acc=0.9695652173913043\n",
      "EPOCH=674\n",
      "train: loss=0.08621960989101657 acc=0.9666666666666667\n",
      "test: loss=0.13325164977802845 acc=0.9652173913043478\n",
      "EPOCH=675\n",
      "train: loss=0.171809459653589 acc=0.9550724637681159\n",
      "test: loss=0.1742122992237712 acc=0.9478260869565217\n",
      "EPOCH=676\n",
      "train: loss=0.08807318985049133 acc=0.981159420289855\n",
      "test: loss=0.10488699565740137 acc=0.9768115942028985\n",
      "EPOCH=677\n",
      "train: loss=0.09920566135314554 acc=0.9739130434782609\n",
      "test: loss=0.11285438635922568 acc=0.9739130434782609\n",
      "EPOCH=678\n",
      "train: loss=0.1570307292086161 acc=0.9536231884057971\n",
      "test: loss=0.14488132889730324 acc=0.9594202898550724\n",
      "EPOCH=679\n",
      "train: loss=0.1270145847135161 acc=0.9710144927536232\n",
      "test: loss=0.12694461590756106 acc=0.9550724637681159\n",
      "EPOCH=680\n",
      "train: loss=0.12661897843376 acc=0.9666666666666667\n",
      "test: loss=0.10515169808154595 acc=0.9681159420289855\n",
      "EPOCH=681\n",
      "train: loss=0.11347532163708852 acc=0.9739130434782609\n",
      "test: loss=0.10117276854962981 acc=0.9753623188405797\n",
      "EPOCH=682\n",
      "train: loss=0.07995150972774305 acc=0.9768115942028985\n",
      "test: loss=0.10475761464365965 acc=0.9681159420289855\n",
      "EPOCH=683\n",
      "train: loss=0.0624366719174647 acc=0.9855072463768116\n",
      "test: loss=0.10272256154903846 acc=0.9782608695652174\n",
      "EPOCH=684\n",
      "train: loss=0.09774482214012176 acc=0.9840579710144928\n",
      "test: loss=0.10365523516617473 acc=0.9652173913043478\n",
      "EPOCH=685\n",
      "train: loss=0.15019337086066703 acc=0.9492753623188406\n",
      "test: loss=0.10864999915930784 acc=0.9666666666666667\n",
      "EPOCH=686\n",
      "train: loss=0.09714054262774524 acc=0.9768115942028985\n",
      "test: loss=0.11511824905387222 acc=0.9710144927536232\n",
      "EPOCH=687\n",
      "train: loss=0.13364498492407564 acc=0.9695652173913043\n",
      "test: loss=0.1164778801046468 acc=0.9637681159420289\n",
      "EPOCH=688\n",
      "train: loss=0.11774089250754474 acc=0.9681159420289855\n",
      "test: loss=0.1062529810903065 acc=0.9710144927536232\n",
      "EPOCH=689\n",
      "train: loss=0.43907590136189034 acc=0.8405797101449275\n",
      "test: loss=0.39213481511230874 acc=0.8391304347826087\n",
      "EPOCH=690\n",
      "train: loss=0.31376419348443235 acc=0.8884057971014493\n",
      "test: loss=0.27376952514099673 acc=0.9130434782608695\n",
      "EPOCH=691\n",
      "train: loss=0.44488600431023057 acc=0.8652173913043478\n",
      "test: loss=0.3788656535822591 acc=0.8927536231884058\n",
      "EPOCH=692\n",
      "train: loss=0.11625841125623465 acc=0.9594202898550724\n",
      "test: loss=0.12787689346936915 acc=0.9637681159420289\n",
      "EPOCH=693\n",
      "train: loss=0.13032744111898725 acc=0.9507246376811594\n",
      "test: loss=0.11415026085903818 acc=0.9681159420289855\n",
      "EPOCH=694\n",
      "train: loss=0.10885426828925274 acc=0.9753623188405797\n",
      "test: loss=0.1160966588172223 acc=0.9652173913043478\n",
      "EPOCH=695\n",
      "train: loss=0.14695684930864517 acc=0.9507246376811594\n",
      "test: loss=0.11599207948504237 acc=0.9637681159420289\n",
      "EPOCH=696\n",
      "train: loss=0.13293204029494787 acc=0.9507246376811594\n",
      "test: loss=0.14168493910071275 acc=0.9492753623188406\n",
      "EPOCH=697\n",
      "train: loss=0.11795121736764037 acc=0.9681159420289855\n",
      "test: loss=0.11156561282809625 acc=0.9652173913043478\n",
      "EPOCH=698\n",
      "train: loss=0.08721398661870636 acc=0.9739130434782609\n",
      "test: loss=0.13515082064178796 acc=0.9521739130434783\n",
      "EPOCH=699\n",
      "train: loss=0.13447733725058367 acc=0.9666666666666667\n",
      "test: loss=0.09904720782247006 acc=0.9753623188405797\n",
      "EPOCH=700\n",
      "train: loss=0.1578385381132559 acc=0.9623188405797102\n",
      "test: loss=0.11317832958411106 acc=0.9710144927536232\n",
      "EPOCH=701\n",
      "train: loss=0.11526370962148173 acc=0.9753623188405797\n",
      "test: loss=0.09766145510127082 acc=0.9739130434782609\n",
      "EPOCH=702\n",
      "train: loss=0.08982633599662387 acc=0.9695652173913043\n",
      "test: loss=0.103835694533007 acc=0.9782608695652174\n",
      "EPOCH=703\n",
      "train: loss=0.12384945480903067 acc=0.9579710144927536\n",
      "test: loss=0.10290241064207281 acc=0.9768115942028985\n",
      "EPOCH=704\n",
      "train: loss=0.08845779238473449 acc=0.972463768115942\n",
      "test: loss=0.10226987780072415 acc=0.9739130434782609\n",
      "EPOCH=705\n",
      "train: loss=0.07381632728211379 acc=0.9840579710144928\n",
      "test: loss=0.10664907244884544 acc=0.9739130434782609\n",
      "EPOCH=706\n",
      "train: loss=0.11422130344171885 acc=0.972463768115942\n",
      "test: loss=0.10543733427951542 acc=0.9710144927536232\n",
      "EPOCH=707\n",
      "train: loss=0.10214684611991572 acc=0.972463768115942\n",
      "test: loss=0.12740674144269792 acc=0.9623188405797102\n",
      "EPOCH=708\n",
      "train: loss=0.12695614044499554 acc=0.9594202898550724\n",
      "test: loss=0.11607383933389982 acc=0.9652173913043478\n",
      "EPOCH=709\n",
      "train: loss=0.05452154050549358 acc=0.9869565217391304\n",
      "test: loss=0.10697477184345054 acc=0.9753623188405797\n",
      "EPOCH=710\n",
      "train: loss=0.12613856832126263 acc=0.9652173913043478\n",
      "test: loss=0.10613791803713925 acc=0.9753623188405797\n",
      "EPOCH=711\n",
      "train: loss=0.10506576468527275 acc=0.972463768115942\n",
      "test: loss=0.09887843139950364 acc=0.9695652173913043\n",
      "EPOCH=712\n",
      "train: loss=0.10987795434327738 acc=0.9710144927536232\n",
      "test: loss=0.11278665890890642 acc=0.9652173913043478\n",
      "EPOCH=713\n",
      "train: loss=0.08874366956676989 acc=0.972463768115942\n",
      "test: loss=0.0990619121909108 acc=0.9666666666666667\n",
      "EPOCH=714\n",
      "train: loss=0.11435599543507664 acc=0.9739130434782609\n",
      "test: loss=0.1021708329564806 acc=0.9753623188405797\n",
      "EPOCH=715\n",
      "train: loss=0.11050973159434814 acc=0.9623188405797102\n",
      "test: loss=0.1030390206696061 acc=0.9739130434782609\n",
      "EPOCH=716\n",
      "train: loss=0.08748919238810725 acc=0.9753623188405797\n",
      "test: loss=0.09932217863197629 acc=0.9739130434782609\n",
      "EPOCH=717\n",
      "train: loss=0.08939298293414101 acc=0.9753623188405797\n",
      "test: loss=0.09989638610917628 acc=0.9782608695652174\n",
      "EPOCH=718\n",
      "train: loss=0.11291758945738932 acc=0.9695652173913043\n",
      "test: loss=0.09296898215494148 acc=0.981159420289855\n",
      "EPOCH=719\n",
      "train: loss=0.09118158718461578 acc=0.9695652173913043\n",
      "test: loss=0.09185622776756883 acc=0.9782608695652174\n",
      "EPOCH=720\n",
      "train: loss=0.07634376396665597 acc=0.9855072463768116\n",
      "test: loss=0.09604416315011204 acc=0.9739130434782609\n",
      "EPOCH=721\n",
      "train: loss=0.07568797656476521 acc=0.9753623188405797\n",
      "test: loss=0.09402953163737147 acc=0.981159420289855\n",
      "EPOCH=722\n",
      "train: loss=0.09474609533694178 acc=0.9710144927536232\n",
      "test: loss=0.09570688532910876 acc=0.9768115942028985\n",
      "EPOCH=723\n",
      "train: loss=0.08541306562307738 acc=0.9753623188405797\n",
      "test: loss=0.09283125407770565 acc=0.9768115942028985\n",
      "EPOCH=724\n",
      "train: loss=0.14897362710166256 acc=0.9753623188405797\n",
      "test: loss=0.09448583243513044 acc=0.9768115942028985\n",
      "EPOCH=725\n",
      "train: loss=0.09688159650642174 acc=0.9695652173913043\n",
      "test: loss=0.09354407621647037 acc=0.9753623188405797\n",
      "EPOCH=726\n",
      "train: loss=0.12593943994039228 acc=0.9782608695652174\n",
      "test: loss=0.09408750076326547 acc=0.981159420289855\n",
      "EPOCH=727\n",
      "train: loss=0.08649030030855319 acc=0.9753623188405797\n",
      "test: loss=0.10808452075909497 acc=0.9710144927536232\n",
      "EPOCH=728\n",
      "train: loss=0.10076273872112776 acc=0.972463768115942\n",
      "test: loss=0.12174918507971366 acc=0.9623188405797102\n",
      "EPOCH=729\n",
      "train: loss=0.20691794557212473 acc=0.9130434782608695\n",
      "test: loss=0.18697285605771455 acc=0.9246376811594202\n",
      "EPOCH=730\n",
      "train: loss=0.20649268641408677 acc=0.9130434782608695\n",
      "test: loss=0.20685954093042955 acc=0.9101449275362319\n",
      "EPOCH=731\n",
      "train: loss=0.2839968767424257 acc=0.9057971014492754\n",
      "test: loss=0.23073522668923843 acc=0.9202898550724637\n",
      "EPOCH=732\n",
      "train: loss=0.3520021997077234 acc=0.8579710144927536\n",
      "test: loss=0.28177780912807604 acc=0.8855072463768116\n",
      "EPOCH=733\n",
      "train: loss=0.1475760368064927 acc=0.946376811594203\n",
      "test: loss=0.19536021120905064 acc=0.9304347826086956\n",
      "EPOCH=734\n",
      "train: loss=0.16116626005758822 acc=0.9608695652173913\n",
      "test: loss=0.132587761728347 acc=0.9637681159420289\n",
      "EPOCH=735\n",
      "train: loss=0.2672338163442916 acc=0.9173913043478261\n",
      "test: loss=0.21010386546902646 acc=0.9318840579710145\n",
      "EPOCH=736\n",
      "train: loss=0.17301641941493145 acc=0.936231884057971\n",
      "test: loss=0.19182083348804194 acc=0.927536231884058\n",
      "EPOCH=737\n",
      "train: loss=0.12481230240811052 acc=0.9637681159420289\n",
      "test: loss=0.11587482146342516 acc=0.9637681159420289\n",
      "EPOCH=738\n",
      "train: loss=0.10803780857091341 acc=0.972463768115942\n",
      "test: loss=0.12805590380165158 acc=0.9565217391304348\n",
      "EPOCH=739\n",
      "train: loss=0.12391068256816962 acc=0.9550724637681159\n",
      "test: loss=0.12275538404593248 acc=0.9536231884057971\n",
      "EPOCH=740\n",
      "train: loss=0.18434527075533233 acc=0.9536231884057971\n",
      "test: loss=0.11343372238365475 acc=0.9623188405797102\n",
      "EPOCH=741\n",
      "train: loss=0.09694169496489034 acc=0.9681159420289855\n",
      "test: loss=0.10219605374756703 acc=0.9681159420289855\n",
      "EPOCH=742\n",
      "train: loss=0.10461835638638911 acc=0.972463768115942\n",
      "test: loss=0.09943383219980993 acc=0.9768115942028985\n",
      "EPOCH=743\n",
      "train: loss=0.08496116157419277 acc=0.9739130434782609\n",
      "test: loss=0.1084445497050842 acc=0.9739130434782609\n",
      "EPOCH=744\n",
      "train: loss=0.11647934969109851 acc=0.9652173913043478\n",
      "test: loss=0.10688754084283267 acc=0.9666666666666667\n",
      "EPOCH=745\n",
      "train: loss=0.06723405292706687 acc=0.9884057971014493\n",
      "test: loss=0.0925403332508704 acc=0.9695652173913043\n",
      "EPOCH=746\n",
      "train: loss=0.1010648333905417 acc=0.9637681159420289\n",
      "test: loss=0.09577822535973089 acc=0.9666666666666667\n",
      "EPOCH=747\n",
      "train: loss=0.09568357226288411 acc=0.9681159420289855\n",
      "test: loss=0.09243213137367481 acc=0.9768115942028985\n",
      "EPOCH=748\n",
      "train: loss=0.09241502568058382 acc=0.9710144927536232\n",
      "test: loss=0.09779676753479111 acc=0.972463768115942\n",
      "EPOCH=749\n",
      "train: loss=0.10471366585395539 acc=0.9768115942028985\n",
      "test: loss=0.09108469751965402 acc=0.9768115942028985\n",
      "EPOCH=750\n",
      "train: loss=0.11508955835309352 acc=0.9608695652173913\n",
      "test: loss=0.10694632546097238 acc=0.9652173913043478\n",
      "EPOCH=751\n",
      "train: loss=0.11612634920277913 acc=0.9710144927536232\n",
      "test: loss=0.11780327925823911 acc=0.9608695652173913\n",
      "EPOCH=752\n",
      "train: loss=0.13386499616643913 acc=0.9797101449275363\n",
      "test: loss=0.09348781654576023 acc=0.981159420289855\n",
      "EPOCH=753\n",
      "train: loss=0.10484441849871226 acc=0.9710144927536232\n",
      "test: loss=0.09164285100717698 acc=0.9797101449275363\n",
      "EPOCH=754\n",
      "train: loss=0.07920379623615685 acc=0.981159420289855\n",
      "test: loss=0.09476659971218729 acc=0.9739130434782609\n",
      "EPOCH=755\n",
      "train: loss=0.08489616214095791 acc=0.9695652173913043\n",
      "test: loss=0.09387999103649539 acc=0.9681159420289855\n",
      "EPOCH=756\n",
      "train: loss=0.08965493232066649 acc=0.9768115942028985\n",
      "test: loss=0.12597776215733464 acc=0.9565217391304348\n",
      "EPOCH=757\n",
      "train: loss=0.08793556829272829 acc=0.981159420289855\n",
      "test: loss=0.0934255015258371 acc=0.9739130434782609\n",
      "EPOCH=758\n",
      "train: loss=0.06366649736824612 acc=0.9884057971014493\n",
      "test: loss=0.08958506541163325 acc=0.9826086956521739\n",
      "EPOCH=759\n",
      "train: loss=0.11624513095183289 acc=0.972463768115942\n",
      "test: loss=0.09186550600002572 acc=0.9782608695652174\n",
      "EPOCH=760\n",
      "train: loss=0.09261724092345397 acc=0.9739130434782609\n",
      "test: loss=0.09262394685256153 acc=0.9768115942028985\n",
      "EPOCH=761\n",
      "train: loss=0.09003292938469165 acc=0.9840579710144928\n",
      "test: loss=0.09322330113944988 acc=0.9710144927536232\n",
      "EPOCH=762\n",
      "train: loss=0.13140967356077776 acc=0.9565217391304348\n",
      "test: loss=0.08972553977087178 acc=0.9753623188405797\n",
      "EPOCH=763\n",
      "train: loss=0.10862354379004915 acc=0.981159420289855\n",
      "test: loss=0.10435167729050084 acc=0.972463768115942\n",
      "EPOCH=764\n",
      "train: loss=0.11990071663133897 acc=0.9637681159420289\n",
      "test: loss=0.1341540280682161 acc=0.9492753623188406\n",
      "EPOCH=765\n",
      "train: loss=0.13827199858862654 acc=0.9550724637681159\n",
      "test: loss=0.14942980322161203 acc=0.9434782608695652\n",
      "EPOCH=766\n",
      "train: loss=0.10666343864938518 acc=0.9753623188405797\n",
      "test: loss=0.09275842398627311 acc=0.9753623188405797\n",
      "EPOCH=767\n",
      "train: loss=0.11424508644003373 acc=0.9681159420289855\n",
      "test: loss=0.13784327381082515 acc=0.9579710144927536\n",
      "EPOCH=768\n",
      "train: loss=0.14937861631982433 acc=0.9579710144927536\n",
      "test: loss=0.12758878905610058 acc=0.9623188405797102\n",
      "EPOCH=769\n",
      "train: loss=0.12365513982123785 acc=0.9521739130434783\n",
      "test: loss=0.11227092793352672 acc=0.9652173913043478\n",
      "EPOCH=770\n",
      "train: loss=0.10229369132646891 acc=0.9782608695652174\n",
      "test: loss=0.10161195186479625 acc=0.9782608695652174\n",
      "EPOCH=771\n",
      "train: loss=0.1528893476715834 acc=0.9492753623188406\n",
      "test: loss=0.10129803668124703 acc=0.972463768115942\n",
      "EPOCH=772\n",
      "train: loss=0.08421823324724888 acc=0.9753623188405797\n",
      "test: loss=0.1105986443475054 acc=0.9681159420289855\n",
      "EPOCH=773\n",
      "train: loss=0.1385022779643528 acc=0.9666666666666667\n",
      "test: loss=0.10692896581088666 acc=0.9739130434782609\n",
      "EPOCH=774\n",
      "train: loss=0.1427668411900805 acc=0.9695652173913043\n",
      "test: loss=0.1059226811264359 acc=0.9739130434782609\n",
      "EPOCH=775\n",
      "train: loss=0.12453924892718457 acc=0.9565217391304348\n",
      "test: loss=0.09518864077537095 acc=0.9753623188405797\n",
      "EPOCH=776\n",
      "train: loss=0.15304495772715945 acc=0.9536231884057971\n",
      "test: loss=0.12889061928570372 acc=0.9550724637681159\n",
      "EPOCH=777\n",
      "train: loss=0.11941070669990736 acc=0.9681159420289855\n",
      "test: loss=0.10400207070606188 acc=0.972463768115942\n",
      "EPOCH=778\n",
      "train: loss=0.10045853551793016 acc=0.9666666666666667\n",
      "test: loss=0.09525285576098502 acc=0.9681159420289855\n",
      "EPOCH=779\n",
      "train: loss=0.10464981166813057 acc=0.9652173913043478\n",
      "test: loss=0.12162592592929078 acc=0.9739130434782609\n",
      "EPOCH=780\n",
      "train: loss=0.13454896002405847 acc=0.9637681159420289\n",
      "test: loss=0.11976624315220695 acc=0.9695652173913043\n",
      "EPOCH=781\n",
      "train: loss=0.09727220669754873 acc=0.9637681159420289\n",
      "test: loss=0.11698795532124744 acc=0.9536231884057971\n",
      "EPOCH=782\n",
      "train: loss=0.1134320667450937 acc=0.9550724637681159\n",
      "test: loss=0.08861660434810399 acc=0.9782608695652174\n",
      "EPOCH=783\n",
      "train: loss=0.1747014898110739 acc=0.9434782608695652\n",
      "test: loss=0.15624444243461144 acc=0.9536231884057971\n",
      "EPOCH=784\n",
      "train: loss=0.08653337353866977 acc=0.9681159420289855\n",
      "test: loss=0.11018043914401968 acc=0.9652173913043478\n",
      "EPOCH=785\n",
      "train: loss=0.13264225949973021 acc=0.9536231884057971\n",
      "test: loss=0.12476076652051019 acc=0.9579710144927536\n",
      "EPOCH=786\n",
      "train: loss=0.17463386376008314 acc=0.9246376811594202\n",
      "test: loss=0.19517015377677624 acc=0.9188405797101449\n",
      "EPOCH=787\n",
      "train: loss=0.16038580895392138 acc=0.9347826086956522\n",
      "test: loss=0.13547390226091255 acc=0.9550724637681159\n",
      "EPOCH=788\n",
      "train: loss=0.11317411475702513 acc=0.9681159420289855\n",
      "test: loss=0.10787214919061552 acc=0.9594202898550724\n",
      "EPOCH=789\n",
      "train: loss=0.10714118275397425 acc=0.9623188405797102\n",
      "test: loss=0.097465554980741 acc=0.9710144927536232\n",
      "EPOCH=790\n",
      "train: loss=0.101407033179927 acc=0.9695652173913043\n",
      "test: loss=0.09664942643766584 acc=0.9637681159420289\n",
      "EPOCH=791\n",
      "train: loss=0.08151910617901778 acc=0.9753623188405797\n",
      "test: loss=0.10397394747128107 acc=0.9710144927536232\n",
      "EPOCH=792\n",
      "train: loss=0.11309811223231474 acc=0.9637681159420289\n",
      "test: loss=0.10328671844844892 acc=0.9594202898550724\n",
      "EPOCH=793\n",
      "train: loss=0.1164113216952173 acc=0.9681159420289855\n",
      "test: loss=0.092597655668511 acc=0.972463768115942\n",
      "EPOCH=794\n",
      "train: loss=0.1197399096580406 acc=0.9681159420289855\n",
      "test: loss=0.09092361933285 acc=0.972463768115942\n",
      "EPOCH=795\n",
      "train: loss=0.11494383284476149 acc=0.972463768115942\n",
      "test: loss=0.10957311264469387 acc=0.9623188405797102\n",
      "EPOCH=796\n",
      "train: loss=0.11821699239481098 acc=0.9768115942028985\n",
      "test: loss=0.09892642762108113 acc=0.9666666666666667\n",
      "EPOCH=797\n",
      "train: loss=0.07675391394114309 acc=0.9768115942028985\n",
      "test: loss=0.08767669499617307 acc=0.9768115942028985\n",
      "EPOCH=798\n",
      "train: loss=0.07380191138837089 acc=0.972463768115942\n",
      "test: loss=0.0952783454227611 acc=0.9681159420289855\n",
      "EPOCH=799\n",
      "train: loss=0.11585584320697437 acc=0.9623188405797102\n",
      "test: loss=0.11072862534216615 acc=0.9652173913043478\n",
      "EPOCH=800\n",
      "train: loss=0.0754483414313168 acc=0.9826086956521739\n",
      "test: loss=0.09246017441889498 acc=0.9797101449275363\n",
      "EPOCH=801\n",
      "train: loss=0.09824903058219786 acc=0.9782608695652174\n",
      "test: loss=0.0967841193063645 acc=0.9695652173913043\n",
      "EPOCH=802\n",
      "train: loss=0.11692612745887543 acc=0.9681159420289855\n",
      "test: loss=0.08568219702518232 acc=0.9768115942028985\n",
      "EPOCH=803\n",
      "train: loss=0.10198280972513248 acc=0.9739130434782609\n",
      "test: loss=0.09188198968185385 acc=0.9710144927536232\n",
      "EPOCH=804\n",
      "train: loss=0.11058225476300104 acc=0.9652173913043478\n",
      "test: loss=0.08681283008542244 acc=0.981159420289855\n",
      "EPOCH=805\n",
      "train: loss=0.08546155034848653 acc=0.9840579710144928\n",
      "test: loss=0.09808118793071133 acc=0.9637681159420289\n",
      "EPOCH=806\n",
      "train: loss=0.07153368509980661 acc=0.9768115942028985\n",
      "test: loss=0.08726912223472075 acc=0.9768115942028985\n",
      "EPOCH=807\n",
      "train: loss=0.15965013447834497 acc=0.9478260869565217\n",
      "test: loss=0.12876444754391803 acc=0.9492753623188406\n",
      "EPOCH=808\n",
      "train: loss=0.0877692110700454 acc=0.9782608695652174\n",
      "test: loss=0.11079342633199799 acc=0.9623188405797102\n",
      "EPOCH=809\n",
      "train: loss=0.19434789110516293 acc=0.9420289855072463\n",
      "test: loss=0.15147654745916403 acc=0.9478260869565217\n",
      "EPOCH=810\n",
      "train: loss=0.16362039658175817 acc=0.9492753623188406\n",
      "test: loss=0.12334323437138683 acc=0.9550724637681159\n",
      "EPOCH=811\n",
      "train: loss=0.09383875019409466 acc=0.9695652173913043\n",
      "test: loss=0.10072991695999932 acc=0.972463768115942\n",
      "EPOCH=812\n",
      "train: loss=0.1295623445221259 acc=0.9695652173913043\n",
      "test: loss=0.10247197755498413 acc=0.9739130434782609\n",
      "EPOCH=813\n",
      "train: loss=0.12926136562193172 acc=0.9637681159420289\n",
      "test: loss=0.11952404959238809 acc=0.9478260869565217\n",
      "EPOCH=814\n",
      "train: loss=0.11061711102262148 acc=0.972463768115942\n",
      "test: loss=0.08826910374727465 acc=0.9782608695652174\n",
      "EPOCH=815\n",
      "train: loss=0.09493813411266347 acc=0.9710144927536232\n",
      "test: loss=0.09052096974295738 acc=0.9782608695652174\n",
      "EPOCH=816\n",
      "train: loss=0.10660508704803665 acc=0.9637681159420289\n",
      "test: loss=0.08897560835841384 acc=0.9695652173913043\n",
      "EPOCH=817\n",
      "train: loss=0.15254539311122375 acc=0.9492753623188406\n",
      "test: loss=0.1418991038624527 acc=0.946376811594203\n",
      "EPOCH=818\n",
      "train: loss=0.1220386152231342 acc=0.9579710144927536\n",
      "test: loss=0.09490874152725493 acc=0.972463768115942\n",
      "EPOCH=819\n",
      "train: loss=0.07306048813720975 acc=0.9869565217391304\n",
      "test: loss=0.08839094530633272 acc=0.9753623188405797\n",
      "EPOCH=820\n",
      "train: loss=0.08979790995618891 acc=0.9666666666666667\n",
      "test: loss=0.08780168454483278 acc=0.9710144927536232\n",
      "EPOCH=821\n",
      "train: loss=0.06742844658877886 acc=0.9869565217391304\n",
      "test: loss=0.10168844584134092 acc=0.9753623188405797\n",
      "EPOCH=822\n",
      "train: loss=0.08834677035086504 acc=0.972463768115942\n",
      "test: loss=0.08771601296924544 acc=0.9782608695652174\n",
      "EPOCH=823\n",
      "train: loss=0.08781507486872127 acc=0.972463768115942\n",
      "test: loss=0.08380099187429925 acc=0.9768115942028985\n",
      "EPOCH=824\n",
      "train: loss=0.1033965922791606 acc=0.9666666666666667\n",
      "test: loss=0.094140047569837 acc=0.9695652173913043\n",
      "EPOCH=825\n",
      "train: loss=0.07361914224520022 acc=0.9782608695652174\n",
      "test: loss=0.10717243864340217 acc=0.9710144927536232\n",
      "EPOCH=826\n",
      "train: loss=0.10942140523287262 acc=0.9710144927536232\n",
      "test: loss=0.09575914683380361 acc=0.9710144927536232\n",
      "EPOCH=827\n",
      "train: loss=0.06984428501176 acc=0.9782608695652174\n",
      "test: loss=0.09396176270129156 acc=0.9681159420289855\n",
      "EPOCH=828\n",
      "train: loss=0.08077747154311185 acc=0.9739130434782609\n",
      "test: loss=0.08859898499766411 acc=0.9782608695652174\n",
      "EPOCH=829\n",
      "train: loss=0.08630692143829156 acc=0.9652173913043478\n",
      "test: loss=0.08479218611722354 acc=0.9797101449275363\n",
      "EPOCH=830\n",
      "train: loss=0.08940269134567452 acc=0.9753623188405797\n",
      "test: loss=0.08895347194967783 acc=0.9710144927536232\n",
      "EPOCH=831\n",
      "train: loss=0.08377446704358653 acc=0.9797101449275363\n",
      "test: loss=0.08221783789158651 acc=0.981159420289855\n",
      "EPOCH=832\n",
      "train: loss=0.0684002391509741 acc=0.9840579710144928\n",
      "test: loss=0.08523106447742461 acc=0.981159420289855\n",
      "EPOCH=833\n",
      "train: loss=0.048828890224612356 acc=0.9898550724637681\n",
      "test: loss=0.09062215283012387 acc=0.9826086956521739\n",
      "EPOCH=834\n",
      "train: loss=0.12560670926954376 acc=0.9652173913043478\n",
      "test: loss=0.0822953964305452 acc=0.9782608695652174\n",
      "EPOCH=835\n",
      "train: loss=0.0877392998990391 acc=0.9782608695652174\n",
      "test: loss=0.10772052309827115 acc=0.9594202898550724\n",
      "EPOCH=836\n",
      "train: loss=0.11313804340238726 acc=0.9666666666666667\n",
      "test: loss=0.11182854439061778 acc=0.9608695652173913\n",
      "EPOCH=837\n",
      "train: loss=0.1001226304081346 acc=0.9565217391304348\n",
      "test: loss=0.09271196128599482 acc=0.9753623188405797\n",
      "EPOCH=838\n",
      "train: loss=0.21872453206289055 acc=0.8971014492753623\n",
      "test: loss=0.189621484765756 acc=0.9231884057971015\n",
      "EPOCH=839\n",
      "train: loss=0.13788074868596156 acc=0.9478260869565217\n",
      "test: loss=0.1534593192421181 acc=0.9449275362318841\n",
      "EPOCH=840\n",
      "train: loss=0.4633538465527092 acc=0.8202898550724638\n",
      "test: loss=0.3929092971292854 acc=0.8492753623188406\n",
      "EPOCH=841\n",
      "train: loss=0.24669705156389085 acc=0.9188405797101449\n",
      "test: loss=0.19840797101540308 acc=0.9289855072463769\n",
      "EPOCH=842\n",
      "train: loss=0.1783733679032432 acc=0.9507246376811594\n",
      "test: loss=0.15980196172168432 acc=0.9521739130434783\n",
      "EPOCH=843\n",
      "train: loss=0.0954784062343405 acc=0.9681159420289855\n",
      "test: loss=0.11225426408416166 acc=0.9608695652173913\n",
      "EPOCH=844\n",
      "train: loss=0.10070883423210081 acc=0.9710144927536232\n",
      "test: loss=0.0990140227810054 acc=0.9666666666666667\n",
      "EPOCH=845\n",
      "train: loss=0.12381808587519005 acc=0.9579710144927536\n",
      "test: loss=0.09631570685872007 acc=0.972463768115942\n",
      "EPOCH=846\n",
      "train: loss=0.20492057625857124 acc=0.9434782608695652\n",
      "test: loss=0.13035631616311535 acc=0.9434782608695652\n",
      "EPOCH=847\n",
      "train: loss=0.07388000386545415 acc=0.9753623188405797\n",
      "test: loss=0.10086767023255439 acc=0.9710144927536232\n",
      "EPOCH=848\n",
      "train: loss=0.12679823930432954 acc=0.9637681159420289\n",
      "test: loss=0.11767649013533012 acc=0.9565217391304348\n",
      "EPOCH=849\n",
      "train: loss=0.08290802605547255 acc=0.9739130434782609\n",
      "test: loss=0.11080601647196194 acc=0.9695652173913043\n",
      "EPOCH=850\n",
      "train: loss=0.08790291202692384 acc=0.9826086956521739\n",
      "test: loss=0.1121469121729789 acc=0.9579710144927536\n",
      "EPOCH=851\n",
      "train: loss=0.1724921608962775 acc=0.9449275362318841\n",
      "test: loss=0.17784782121545745 acc=0.9202898550724637\n",
      "EPOCH=852\n",
      "train: loss=0.09447100622890493 acc=0.9753623188405797\n",
      "test: loss=0.11766969113879139 acc=0.9637681159420289\n",
      "EPOCH=853\n",
      "train: loss=0.08528286628249994 acc=0.9695652173913043\n",
      "test: loss=0.11300771050242076 acc=0.9666666666666667\n",
      "EPOCH=854\n",
      "train: loss=0.09116199746091137 acc=0.9652173913043478\n",
      "test: loss=0.10649599940314335 acc=0.9666666666666667\n",
      "EPOCH=855\n",
      "train: loss=0.14774969041516461 acc=0.9710144927536232\n",
      "test: loss=0.10808907240350765 acc=0.9637681159420289\n",
      "EPOCH=856\n",
      "train: loss=0.09593173807078062 acc=0.9652173913043478\n",
      "test: loss=0.09269440118616834 acc=0.9681159420289855\n",
      "EPOCH=857\n",
      "train: loss=0.10627958845586827 acc=0.9579710144927536\n",
      "test: loss=0.09866001582778043 acc=0.9695652173913043\n",
      "EPOCH=858\n",
      "train: loss=0.12136971827096675 acc=0.972463768115942\n",
      "test: loss=0.10566483248381038 acc=0.9637681159420289\n",
      "EPOCH=859\n",
      "train: loss=0.0776781360981404 acc=0.972463768115942\n",
      "test: loss=0.10018165662308937 acc=0.9695652173913043\n",
      "EPOCH=860\n",
      "train: loss=0.12671693947904863 acc=0.9536231884057971\n",
      "test: loss=0.10128369180893834 acc=0.972463768115942\n",
      "EPOCH=861\n",
      "train: loss=0.10649665187134198 acc=0.972463768115942\n",
      "test: loss=0.09846928111316161 acc=0.9710144927536232\n",
      "EPOCH=862\n",
      "train: loss=0.1316784096402811 acc=0.9536231884057971\n",
      "test: loss=0.09986988185064333 acc=0.9652173913043478\n",
      "EPOCH=863\n",
      "train: loss=0.0855188244553583 acc=0.9666666666666667\n",
      "test: loss=0.09328264331995526 acc=0.972463768115942\n",
      "EPOCH=864\n",
      "train: loss=0.1454687843067045 acc=0.9594202898550724\n",
      "test: loss=0.08994956411632926 acc=0.9768115942028985\n",
      "EPOCH=865\n",
      "train: loss=0.07392233164608962 acc=0.9768115942028985\n",
      "test: loss=0.09396105938234421 acc=0.972463768115942\n",
      "EPOCH=866\n",
      "train: loss=0.10731053469909012 acc=0.9623188405797102\n",
      "test: loss=0.10194461701795342 acc=0.972463768115942\n",
      "EPOCH=867\n",
      "train: loss=0.09536383050260956 acc=0.9681159420289855\n",
      "test: loss=0.09191441482435829 acc=0.9782608695652174\n",
      "EPOCH=868\n",
      "train: loss=0.09801816181270603 acc=0.9768115942028985\n",
      "test: loss=0.09712303812444725 acc=0.972463768115942\n",
      "EPOCH=869\n",
      "train: loss=0.11867498139639021 acc=0.9594202898550724\n",
      "test: loss=0.10573661554241862 acc=0.9666666666666667\n",
      "EPOCH=870\n",
      "train: loss=0.09393956144403655 acc=0.972463768115942\n",
      "test: loss=0.09751496140664236 acc=0.9681159420289855\n",
      "EPOCH=871\n",
      "train: loss=0.13463925037052152 acc=0.9579710144927536\n",
      "test: loss=0.11442502881224283 acc=0.9536231884057971\n",
      "EPOCH=872\n",
      "train: loss=0.08732888866780807 acc=0.9797101449275363\n",
      "test: loss=0.10118102963133482 acc=0.9652173913043478\n",
      "EPOCH=873\n",
      "train: loss=0.09594469435466776 acc=0.9666666666666667\n",
      "test: loss=0.09035306006249874 acc=0.972463768115942\n",
      "EPOCH=874\n",
      "train: loss=0.13888357939639095 acc=0.9739130434782609\n",
      "test: loss=0.08903716593006422 acc=0.9753623188405797\n",
      "EPOCH=875\n",
      "train: loss=0.08870913393374064 acc=0.9666666666666667\n",
      "test: loss=0.10697233582973026 acc=0.9652173913043478\n",
      "EPOCH=876\n",
      "train: loss=0.08795734583472747 acc=0.9695652173913043\n",
      "test: loss=0.0890127079746542 acc=0.9739130434782609\n",
      "EPOCH=877\n",
      "train: loss=0.08633041412489492 acc=0.9666666666666667\n",
      "test: loss=0.09985648103400636 acc=0.9623188405797102\n",
      "EPOCH=878\n",
      "train: loss=0.07618491663818662 acc=0.9884057971014493\n",
      "test: loss=0.09002291487961214 acc=0.9710144927536232\n",
      "EPOCH=879\n",
      "train: loss=0.06483852565599568 acc=0.981159420289855\n",
      "test: loss=0.09045477212220467 acc=0.9768115942028985\n",
      "EPOCH=880\n",
      "train: loss=0.11676825605472403 acc=0.9623188405797102\n",
      "test: loss=0.0906839447508357 acc=0.9681159420289855\n",
      "EPOCH=881\n",
      "train: loss=0.07315858055814049 acc=0.9782608695652174\n",
      "test: loss=0.0969002184327805 acc=0.972463768115942\n",
      "EPOCH=882\n",
      "train: loss=0.12130352656754546 acc=0.9594202898550724\n",
      "test: loss=0.13417666196070138 acc=0.9536231884057971\n",
      "EPOCH=883\n",
      "train: loss=0.17043891179826257 acc=0.9347826086956522\n",
      "test: loss=0.1552801704331822 acc=0.9405797101449276\n",
      "EPOCH=884\n",
      "train: loss=0.14197357516765774 acc=0.9478260869565217\n",
      "test: loss=0.14553658509670644 acc=0.9434782608695652\n",
      "EPOCH=885\n",
      "train: loss=0.09665142283886902 acc=0.9739130434782609\n",
      "test: loss=0.11675098325590624 acc=0.9565217391304348\n",
      "EPOCH=886\n",
      "train: loss=0.0981210518880253 acc=0.9666666666666667\n",
      "test: loss=0.11965413507617961 acc=0.9565217391304348\n",
      "EPOCH=887\n",
      "train: loss=0.14762146577455695 acc=0.9420289855072463\n",
      "test: loss=0.1374042487190433 acc=0.946376811594203\n",
      "EPOCH=888\n",
      "train: loss=0.09555471773540074 acc=0.9695652173913043\n",
      "test: loss=0.1033183593770452 acc=0.9681159420289855\n",
      "EPOCH=889\n",
      "train: loss=0.12068631257951294 acc=0.9623188405797102\n",
      "test: loss=0.10195580640315377 acc=0.9695652173913043\n",
      "EPOCH=890\n",
      "train: loss=0.08144234786775517 acc=0.9739130434782609\n",
      "test: loss=0.10530552326308944 acc=0.9637681159420289\n",
      "EPOCH=891\n",
      "train: loss=0.09082878852272455 acc=0.9695652173913043\n",
      "test: loss=0.0939579726208574 acc=0.9695652173913043\n",
      "EPOCH=892\n",
      "train: loss=0.11220826130304637 acc=0.9449275362318841\n",
      "test: loss=0.09102463222286851 acc=0.9579710144927536\n",
      "EPOCH=893\n",
      "train: loss=0.08259620555358192 acc=0.9753623188405797\n",
      "test: loss=0.09641912114229371 acc=0.9637681159420289\n",
      "EPOCH=894\n",
      "train: loss=0.054995075269782014 acc=0.9826086956521739\n",
      "test: loss=0.09189594687944258 acc=0.9710144927536232\n",
      "EPOCH=895\n",
      "train: loss=0.07732151113801947 acc=0.9753623188405797\n",
      "test: loss=0.08632998697966969 acc=0.9782608695652174\n",
      "EPOCH=896\n",
      "train: loss=0.051739744152745765 acc=0.9840579710144928\n",
      "test: loss=0.08817316262332059 acc=0.9768115942028985\n",
      "EPOCH=897\n",
      "train: loss=0.08732168672759213 acc=0.972463768115942\n",
      "test: loss=0.0967997229128762 acc=0.972463768115942\n",
      "EPOCH=898\n",
      "train: loss=0.06824344706478291 acc=0.9840579710144928\n",
      "test: loss=0.08810802249792829 acc=0.9782608695652174\n",
      "EPOCH=899\n",
      "train: loss=0.1328341654206125 acc=0.9565217391304348\n",
      "test: loss=0.08827394073957827 acc=0.9753623188405797\n",
      "EPOCH=900\n",
      "train: loss=0.1066409893185516 acc=0.9739130434782609\n",
      "test: loss=0.08795411495150347 acc=0.9782608695652174\n",
      "EPOCH=901\n",
      "train: loss=0.09235674897682637 acc=0.9710144927536232\n",
      "test: loss=0.08231920568606974 acc=0.9753623188405797\n",
      "EPOCH=902\n",
      "train: loss=0.07476458824497818 acc=0.9753623188405797\n",
      "test: loss=0.08277149403142113 acc=0.9768115942028985\n",
      "EPOCH=903\n",
      "train: loss=0.1365652461763529 acc=0.9637681159420289\n",
      "test: loss=0.08738425217944958 acc=0.9710144927536232\n",
      "EPOCH=904\n",
      "train: loss=0.07744852164069543 acc=0.9782608695652174\n",
      "test: loss=0.08832034346480458 acc=0.9782608695652174\n",
      "EPOCH=905\n",
      "train: loss=0.08074452048177672 acc=0.9695652173913043\n",
      "test: loss=0.08835080593938358 acc=0.972463768115942\n",
      "EPOCH=906\n",
      "train: loss=0.0735030392210384 acc=0.981159420289855\n",
      "test: loss=0.08439523119144074 acc=0.9753623188405797\n",
      "EPOCH=907\n",
      "train: loss=0.0940303294617573 acc=0.9623188405797102\n",
      "test: loss=0.08831089370857133 acc=0.9782608695652174\n",
      "EPOCH=908\n",
      "train: loss=0.1350849628926018 acc=0.9623188405797102\n",
      "test: loss=0.08648908424626538 acc=0.972463768115942\n",
      "EPOCH=909\n",
      "train: loss=0.09177974047222819 acc=0.9739130434782609\n",
      "test: loss=0.09292695897602256 acc=0.9710144927536232\n",
      "EPOCH=910\n",
      "train: loss=0.13172505453280373 acc=0.9521739130434783\n",
      "test: loss=0.13039127093890662 acc=0.9652173913043478\n",
      "EPOCH=911\n",
      "train: loss=0.20162890638333963 acc=0.9347826086956522\n",
      "test: loss=0.2170971730772813 acc=0.9260869565217391\n",
      "EPOCH=912\n",
      "train: loss=0.11448697039036497 acc=0.9666666666666667\n",
      "test: loss=0.1289859236606624 acc=0.9521739130434783\n",
      "EPOCH=913\n",
      "train: loss=0.1031189481003048 acc=0.9623188405797102\n",
      "test: loss=0.11247407867685819 acc=0.9637681159420289\n",
      "EPOCH=914\n",
      "train: loss=0.1706794865631162 acc=0.9434782608695652\n",
      "test: loss=0.21401683461013457 acc=0.9188405797101449\n",
      "EPOCH=915\n",
      "train: loss=0.14031896213225561 acc=0.936231884057971\n",
      "test: loss=0.12789014088197406 acc=0.9594202898550724\n",
      "EPOCH=916\n",
      "train: loss=0.13065077905457134 acc=0.9507246376811594\n",
      "test: loss=0.13493394036222703 acc=0.9608695652173913\n",
      "EPOCH=917\n",
      "train: loss=0.10151341745527714 acc=0.9623188405797102\n",
      "test: loss=0.09525573375848044 acc=0.9623188405797102\n",
      "EPOCH=918\n",
      "train: loss=0.0776213901545005 acc=0.9681159420289855\n",
      "test: loss=0.0977786997307552 acc=0.9695652173913043\n",
      "EPOCH=919\n",
      "train: loss=0.12073363168350068 acc=0.9710144927536232\n",
      "test: loss=0.09210117070595687 acc=0.9710144927536232\n",
      "EPOCH=920\n",
      "train: loss=0.07577580802813334 acc=0.9797101449275363\n",
      "test: loss=0.1047620138051319 acc=0.9623188405797102\n",
      "EPOCH=921\n",
      "train: loss=0.0777445759703541 acc=0.9739130434782609\n",
      "test: loss=0.08433391694132641 acc=0.9753623188405797\n",
      "EPOCH=922\n",
      "train: loss=0.130845266340007 acc=0.9550724637681159\n",
      "test: loss=0.09688123452545026 acc=0.9652173913043478\n",
      "EPOCH=923\n",
      "train: loss=0.09038300594658576 acc=0.9681159420289855\n",
      "test: loss=0.09777721335690973 acc=0.9768115942028985\n",
      "EPOCH=924\n",
      "train: loss=0.11304066170847414 acc=0.9608695652173913\n",
      "test: loss=0.08757944730324203 acc=0.972463768115942\n",
      "EPOCH=925\n",
      "train: loss=0.10995723951836653 acc=0.9478260869565217\n",
      "test: loss=0.10597733911136277 acc=0.9637681159420289\n",
      "EPOCH=926\n",
      "train: loss=0.1347787292539433 acc=0.9608695652173913\n",
      "test: loss=0.08524625093804614 acc=0.9753623188405797\n",
      "EPOCH=927\n",
      "train: loss=0.11266252549887086 acc=0.9666666666666667\n",
      "test: loss=0.09525082259410898 acc=0.9623188405797102\n",
      "EPOCH=928\n",
      "train: loss=0.07442452586502438 acc=0.972463768115942\n",
      "test: loss=0.08062094070641852 acc=0.9666666666666667\n",
      "EPOCH=929\n",
      "train: loss=0.1488239802454067 acc=0.9478260869565217\n",
      "test: loss=0.12403354710600387 acc=0.9579710144927536\n",
      "EPOCH=930\n",
      "train: loss=0.14062903479534794 acc=0.9536231884057971\n",
      "test: loss=0.15814597124374766 acc=0.9420289855072463\n",
      "EPOCH=931\n",
      "train: loss=0.09566705490879175 acc=0.9681159420289855\n",
      "test: loss=0.10674186391878732 acc=0.9579710144927536\n",
      "EPOCH=932\n",
      "train: loss=0.13247265547730055 acc=0.9681159420289855\n",
      "test: loss=0.0943425787316392 acc=0.9753623188405797\n",
      "EPOCH=933\n",
      "train: loss=0.09300684769249132 acc=0.972463768115942\n",
      "test: loss=0.08521998613241291 acc=0.9681159420289855\n",
      "EPOCH=934\n",
      "train: loss=0.12078631179469607 acc=0.9710144927536232\n",
      "test: loss=0.0963757299786176 acc=0.9666666666666667\n",
      "EPOCH=935\n",
      "train: loss=0.08408016046854341 acc=0.9797101449275363\n",
      "test: loss=0.08169133919214612 acc=0.9782608695652174\n",
      "EPOCH=936\n",
      "train: loss=0.08005340384913956 acc=0.9739130434782609\n",
      "test: loss=0.08466762106059689 acc=0.9768115942028985\n",
      "EPOCH=937\n",
      "train: loss=0.07354644641656838 acc=0.9797101449275363\n",
      "test: loss=0.09505976050913566 acc=0.9666666666666667\n",
      "EPOCH=938\n",
      "train: loss=0.07486515795967193 acc=0.9739130434782609\n",
      "test: loss=0.0832694464930654 acc=0.9710144927536232\n",
      "EPOCH=939\n",
      "train: loss=0.07219150116584552 acc=0.9710144927536232\n",
      "test: loss=0.09604055894131268 acc=0.9710144927536232\n",
      "EPOCH=940\n",
      "train: loss=0.0956621575088177 acc=0.9637681159420289\n",
      "test: loss=0.08433604209124486 acc=0.9739130434782609\n",
      "EPOCH=941\n",
      "train: loss=0.07575688394746154 acc=0.9710144927536232\n",
      "test: loss=0.08163856995374281 acc=0.9840579710144928\n",
      "EPOCH=942\n",
      "train: loss=0.08222332622768097 acc=0.9753623188405797\n",
      "test: loss=0.08764017793476483 acc=0.972463768115942\n",
      "EPOCH=943\n",
      "train: loss=0.07499180926109872 acc=0.9797101449275363\n",
      "test: loss=0.08477422653780424 acc=0.972463768115942\n",
      "EPOCH=944\n",
      "train: loss=0.09249670625978654 acc=0.972463768115942\n",
      "test: loss=0.09902067957354364 acc=0.9681159420289855\n",
      "EPOCH=945\n",
      "train: loss=0.056030128746638035 acc=0.9840579710144928\n",
      "test: loss=0.0798002794811676 acc=0.9797101449275363\n",
      "EPOCH=946\n",
      "train: loss=0.06188855937551964 acc=0.9797101449275363\n",
      "test: loss=0.08230670877732653 acc=0.9768115942028985\n",
      "EPOCH=947\n",
      "train: loss=0.07076201750228059 acc=0.9768115942028985\n",
      "test: loss=0.08219388585558425 acc=0.981159420289855\n",
      "EPOCH=948\n",
      "train: loss=0.09315778057563733 acc=0.9695652173913043\n",
      "test: loss=0.07905646899027383 acc=0.981159420289855\n",
      "EPOCH=949\n",
      "train: loss=0.08457896270221708 acc=0.9739130434782609\n",
      "test: loss=0.0843225580720206 acc=0.9797101449275363\n",
      "EPOCH=950\n",
      "train: loss=0.10032506882650893 acc=0.9710144927536232\n",
      "test: loss=0.08341628239446111 acc=0.9782608695652174\n",
      "EPOCH=951\n",
      "train: loss=0.08342305998782588 acc=0.9782608695652174\n",
      "test: loss=0.10427246437326126 acc=0.9579710144927536\n",
      "EPOCH=952\n",
      "train: loss=0.10378329024185666 acc=0.9739130434782609\n",
      "test: loss=0.08704167414562425 acc=0.9739130434782609\n",
      "EPOCH=953\n",
      "train: loss=0.05723083599335992 acc=0.9840579710144928\n",
      "test: loss=0.08209889320790444 acc=0.9797101449275363\n",
      "EPOCH=954\n",
      "train: loss=0.05656861400245367 acc=0.9782608695652174\n",
      "test: loss=0.08015486595745304 acc=0.981159420289855\n",
      "EPOCH=955\n",
      "train: loss=0.07642997644346151 acc=0.972463768115942\n",
      "test: loss=0.07952209192798552 acc=0.9753623188405797\n",
      "EPOCH=956\n",
      "train: loss=0.12523353813831448 acc=0.9507246376811594\n",
      "test: loss=0.08899891899370793 acc=0.9637681159420289\n",
      "EPOCH=957\n",
      "train: loss=0.08619444219448252 acc=0.9681159420289855\n",
      "test: loss=0.10272243852282412 acc=0.9652173913043478\n",
      "EPOCH=958\n",
      "train: loss=0.14093976013763562 acc=0.9521739130434783\n",
      "test: loss=0.09667828588924204 acc=0.9666666666666667\n",
      "EPOCH=959\n",
      "train: loss=0.12298719545029377 acc=0.9652173913043478\n",
      "test: loss=0.0818657694341932 acc=0.981159420289855\n",
      "EPOCH=960\n",
      "train: loss=0.06492672060233268 acc=0.9840579710144928\n",
      "test: loss=0.08870566702011493 acc=0.9739130434782609\n",
      "EPOCH=961\n",
      "train: loss=0.11140964504052418 acc=0.9666666666666667\n",
      "test: loss=0.0783404635468647 acc=0.9739130434782609\n",
      "EPOCH=962\n",
      "train: loss=0.08274326455840682 acc=0.972463768115942\n",
      "test: loss=0.08555730083073242 acc=0.972463768115942\n",
      "EPOCH=963\n",
      "train: loss=0.07855237738098891 acc=0.9782608695652174\n",
      "test: loss=0.07654616866903786 acc=0.9768115942028985\n",
      "EPOCH=964\n",
      "train: loss=0.065288367928093 acc=0.9782608695652174\n",
      "test: loss=0.07632848019890669 acc=0.981159420289855\n",
      "EPOCH=965\n",
      "train: loss=0.1272650669201653 acc=0.9521739130434783\n",
      "test: loss=0.12967460849140172 acc=0.9507246376811594\n",
      "EPOCH=966\n",
      "train: loss=0.48407998113385586 acc=0.8057971014492754\n",
      "test: loss=0.5280287299186966 acc=0.8043478260869565\n",
      "EPOCH=967\n",
      "train: loss=0.23586463772638908 acc=0.9318840579710145\n",
      "test: loss=0.17466878826532367 acc=0.936231884057971\n",
      "EPOCH=968\n",
      "train: loss=0.14670947009731475 acc=0.9652173913043478\n",
      "test: loss=0.10048148042236665 acc=0.9681159420289855\n",
      "EPOCH=969\n",
      "train: loss=0.07222653071470099 acc=0.9782608695652174\n",
      "test: loss=0.09909687060152504 acc=0.9666666666666667\n",
      "EPOCH=970\n",
      "train: loss=0.0823259893464584 acc=0.9652173913043478\n",
      "test: loss=0.10946862168034521 acc=0.9652173913043478\n",
      "EPOCH=971\n",
      "train: loss=0.10298380250304891 acc=0.9681159420289855\n",
      "test: loss=0.11712619704031091 acc=0.9666666666666667\n",
      "EPOCH=972\n",
      "train: loss=0.12712979542066472 acc=0.9637681159420289\n",
      "test: loss=0.10153968084086243 acc=0.9652173913043478\n",
      "EPOCH=973\n",
      "train: loss=0.089575950403015 acc=0.9637681159420289\n",
      "test: loss=0.09736158093991747 acc=0.972463768115942\n",
      "EPOCH=974\n",
      "train: loss=0.12552484039979958 acc=0.9594202898550724\n",
      "test: loss=0.08923719954409134 acc=0.9739130434782609\n",
      "EPOCH=975\n",
      "train: loss=0.12289551744071435 acc=0.9521739130434783\n",
      "test: loss=0.10573646133508027 acc=0.9666666666666667\n",
      "EPOCH=976\n",
      "train: loss=0.11259049731230382 acc=0.9695652173913043\n",
      "test: loss=0.09826598620102009 acc=0.9666666666666667\n",
      "EPOCH=977\n",
      "train: loss=0.07167970692225652 acc=0.9768115942028985\n",
      "test: loss=0.09339666704065486 acc=0.9681159420289855\n",
      "EPOCH=978\n",
      "train: loss=0.09244190627538879 acc=0.9753623188405797\n",
      "test: loss=0.09614152605870149 acc=0.9782608695652174\n",
      "EPOCH=979\n",
      "train: loss=0.10183658724997083 acc=0.972463768115942\n",
      "test: loss=0.09295327134393343 acc=0.9753623188405797\n",
      "EPOCH=980\n",
      "train: loss=0.07205181806378996 acc=0.9782608695652174\n",
      "test: loss=0.09172142930979757 acc=0.9695652173913043\n",
      "EPOCH=981\n",
      "train: loss=0.06534535510802668 acc=0.9782608695652174\n",
      "test: loss=0.09554258204392026 acc=0.9695652173913043\n",
      "EPOCH=982\n",
      "train: loss=0.06609551037190102 acc=0.981159420289855\n",
      "test: loss=0.08600504829552552 acc=0.9753623188405797\n",
      "EPOCH=983\n",
      "train: loss=0.09204013243651392 acc=0.9695652173913043\n",
      "test: loss=0.0914991149783803 acc=0.9710144927536232\n",
      "EPOCH=984\n",
      "train: loss=0.07713583445151728 acc=0.9753623188405797\n",
      "test: loss=0.11194900236882063 acc=0.9666666666666667\n",
      "EPOCH=985\n",
      "train: loss=0.06568406531433882 acc=0.9840579710144928\n",
      "test: loss=0.09850435566095574 acc=0.9710144927536232\n",
      "EPOCH=986\n",
      "train: loss=0.1130555065607235 acc=0.9666666666666667\n",
      "test: loss=0.08874581415416938 acc=0.972463768115942\n",
      "EPOCH=987\n",
      "train: loss=0.09965075618159998 acc=0.9739130434782609\n",
      "test: loss=0.09672417039303463 acc=0.9681159420289855\n",
      "EPOCH=988\n",
      "train: loss=0.09242075619938232 acc=0.9652173913043478\n",
      "test: loss=0.09690782727408816 acc=0.9652173913043478\n",
      "EPOCH=989\n",
      "train: loss=0.11527253001611568 acc=0.9652173913043478\n",
      "test: loss=0.09387317837763443 acc=0.9652173913043478\n",
      "EPOCH=990\n",
      "train: loss=0.10057161415135647 acc=0.9623188405797102\n",
      "test: loss=0.08921916954288242 acc=0.9710144927536232\n",
      "EPOCH=991\n",
      "train: loss=0.05275281935849639 acc=0.981159420289855\n",
      "test: loss=0.09056389083457238 acc=0.9681159420289855\n",
      "EPOCH=992\n",
      "train: loss=0.06213120269040368 acc=0.981159420289855\n",
      "test: loss=0.08518170506439479 acc=0.9768115942028985\n",
      "EPOCH=993\n",
      "train: loss=0.17468982104342376 acc=0.9391304347826087\n",
      "test: loss=0.1398403095846635 acc=0.9507246376811594\n",
      "EPOCH=994\n",
      "train: loss=0.09987216913199987 acc=0.9797101449275363\n",
      "test: loss=0.09131487271944314 acc=0.9710144927536232\n",
      "EPOCH=995\n",
      "train: loss=0.1124194751695057 acc=0.9608695652173913\n",
      "test: loss=0.11313589944567666 acc=0.9695652173913043\n",
      "EPOCH=996\n",
      "train: loss=0.09062463089469232 acc=0.9753623188405797\n",
      "test: loss=0.11227694542316846 acc=0.9666666666666667\n",
      "EPOCH=997\n",
      "train: loss=0.08116598093649716 acc=0.9666666666666667\n",
      "test: loss=0.10716736345110416 acc=0.9623188405797102\n",
      "EPOCH=998\n",
      "train: loss=0.12007608455379049 acc=0.9681159420289855\n",
      "test: loss=0.10439460612552723 acc=0.9637681159420289\n",
      "EPOCH=999\n",
      "train: loss=0.12325183177978179 acc=0.9507246376811594\n",
      "test: loss=0.11201859562283732 acc=0.9521739130434783\n",
      "EPOCH=1000\n",
      "train: loss=0.05688231271900466 acc=0.9826086956521739\n",
      "test: loss=0.09400381711389733 acc=0.9681159420289855\n",
      "EPOCH=1001\n",
      "train: loss=0.07453179270616027 acc=0.9782608695652174\n",
      "test: loss=0.10934995608100702 acc=0.9565217391304348\n",
      "EPOCH=1002\n",
      "train: loss=0.14527736118200016 acc=0.9434782608695652\n",
      "test: loss=0.11499017575709725 acc=0.9521739130434783\n",
      "EPOCH=1003\n",
      "train: loss=0.07282275193733981 acc=0.9695652173913043\n",
      "test: loss=0.08537044020486963 acc=0.9739130434782609\n",
      "EPOCH=1004\n",
      "train: loss=0.0716457926050665 acc=0.9739130434782609\n",
      "test: loss=0.08742105170492052 acc=0.972463768115942\n",
      "EPOCH=1005\n",
      "train: loss=0.09867252789740003 acc=0.9666666666666667\n",
      "test: loss=0.0846265910457183 acc=0.9753623188405797\n",
      "EPOCH=1006\n",
      "train: loss=0.07887844754788785 acc=0.9695652173913043\n",
      "test: loss=0.07950576681060509 acc=0.972463768115942\n",
      "EPOCH=1007\n",
      "train: loss=0.08196426636436557 acc=0.972463768115942\n",
      "test: loss=0.07830853513698582 acc=0.9782608695652174\n",
      "EPOCH=1008\n",
      "train: loss=0.08357952843237616 acc=0.9681159420289855\n",
      "test: loss=0.089935968490408 acc=0.9753623188405797\n",
      "EPOCH=1009\n",
      "train: loss=0.0672291593697592 acc=0.9753623188405797\n",
      "test: loss=0.08886758400993508 acc=0.9637681159420289\n",
      "EPOCH=1010\n",
      "train: loss=0.15389317752363643 acc=0.9536231884057971\n",
      "test: loss=0.1037766769060707 acc=0.9652173913043478\n",
      "EPOCH=1011\n",
      "train: loss=0.11962564770757186 acc=0.9768115942028985\n",
      "test: loss=0.08476715342615998 acc=0.9695652173913043\n",
      "EPOCH=1012\n",
      "train: loss=0.08424214116758492 acc=0.9681159420289855\n",
      "test: loss=0.0851441213441489 acc=0.9710144927536232\n",
      "EPOCH=1013\n",
      "train: loss=0.09916810375910522 acc=0.9710144927536232\n",
      "test: loss=0.08805211968523691 acc=0.9782608695652174\n",
      "EPOCH=1014\n",
      "train: loss=0.08461485946108445 acc=0.9666666666666667\n",
      "test: loss=0.08346055157986616 acc=0.9710144927536232\n",
      "EPOCH=1015\n",
      "train: loss=0.10689927036726637 acc=0.972463768115942\n",
      "test: loss=0.08041923301467334 acc=0.981159420289855\n",
      "EPOCH=1016\n",
      "train: loss=0.10870292602286767 acc=0.9695652173913043\n",
      "test: loss=0.08347249972688928 acc=0.9710144927536232\n",
      "EPOCH=1017\n",
      "train: loss=0.07743425816990696 acc=0.9782608695652174\n",
      "test: loss=0.08404900180746598 acc=0.9739130434782609\n",
      "EPOCH=1018\n",
      "train: loss=0.07340328402892624 acc=0.9797101449275363\n",
      "test: loss=0.08415072366289507 acc=0.9753623188405797\n",
      "EPOCH=1019\n",
      "train: loss=0.08532438784370158 acc=0.9739130434782609\n",
      "test: loss=0.08684405053979177 acc=0.9710144927536232\n",
      "EPOCH=1020\n",
      "train: loss=0.14489787222602793 acc=0.9637681159420289\n",
      "test: loss=0.09758666902674529 acc=0.9666666666666667\n",
      "EPOCH=1021\n",
      "train: loss=0.12103595531878605 acc=0.9608695652173913\n",
      "test: loss=0.09382732497363978 acc=0.9666666666666667\n",
      "EPOCH=1022\n",
      "train: loss=0.06922078064010989 acc=0.9797101449275363\n",
      "test: loss=0.08293067475990866 acc=0.9768115942028985\n",
      "EPOCH=1023\n",
      "train: loss=0.06512235988717892 acc=0.9782608695652174\n",
      "test: loss=0.0956503513287636 acc=0.9652173913043478\n",
      "EPOCH=1024\n",
      "train: loss=0.10050610689670603 acc=0.9695652173913043\n",
      "test: loss=0.10209022434691573 acc=0.9637681159420289\n",
      "EPOCH=1025\n",
      "train: loss=0.14042635373573886 acc=0.9594202898550724\n",
      "test: loss=0.10096090630370623 acc=0.9652173913043478\n",
      "EPOCH=1026\n",
      "train: loss=0.11937742520213668 acc=0.9550724637681159\n",
      "test: loss=0.09583661867539342 acc=0.972463768115942\n",
      "EPOCH=1027\n",
      "train: loss=0.06493309937418915 acc=0.981159420289855\n",
      "test: loss=0.08426257064292776 acc=0.9695652173913043\n",
      "EPOCH=1028\n",
      "train: loss=0.10668242834899959 acc=0.9753623188405797\n",
      "test: loss=0.10318018206997807 acc=0.9623188405797102\n",
      "EPOCH=1029\n",
      "train: loss=0.08344733643732051 acc=0.9652173913043478\n",
      "test: loss=0.0916470896889607 acc=0.972463768115942\n",
      "EPOCH=1030\n",
      "train: loss=0.07290286772101988 acc=0.9768115942028985\n",
      "test: loss=0.09711197692003902 acc=0.9753623188405797\n",
      "EPOCH=1031\n",
      "train: loss=0.09813378043285861 acc=0.9652173913043478\n",
      "test: loss=0.09285974061266418 acc=0.972463768115942\n",
      "EPOCH=1032\n",
      "train: loss=0.09886400300401064 acc=0.972463768115942\n",
      "test: loss=0.09459900785877552 acc=0.9753623188405797\n",
      "EPOCH=1033\n",
      "train: loss=0.0560800103871649 acc=0.9797101449275363\n",
      "test: loss=0.07992640288915756 acc=0.9753623188405797\n",
      "EPOCH=1034\n",
      "train: loss=0.0878888921359502 acc=0.9768115942028985\n",
      "test: loss=0.08650815061006549 acc=0.9681159420289855\n",
      "EPOCH=1035\n",
      "train: loss=0.07537862372799951 acc=0.9753623188405797\n",
      "test: loss=0.07949608944402516 acc=0.9782608695652174\n",
      "EPOCH=1036\n",
      "train: loss=0.08679479502714711 acc=0.9826086956521739\n",
      "test: loss=0.08404450240052248 acc=0.9753623188405797\n",
      "EPOCH=1037\n",
      "train: loss=0.08710997083366498 acc=0.9768115942028985\n",
      "test: loss=0.0841090602534027 acc=0.972463768115942\n",
      "EPOCH=1038\n",
      "train: loss=0.08279325801703602 acc=0.9695652173913043\n",
      "test: loss=0.08455332651354833 acc=0.9710144927536232\n",
      "EPOCH=1039\n",
      "train: loss=0.13515302742995589 acc=0.9681159420289855\n",
      "test: loss=0.08396439231416312 acc=0.9695652173913043\n",
      "EPOCH=1040\n",
      "train: loss=0.08896008424521476 acc=0.9666666666666667\n",
      "test: loss=0.08629815894336433 acc=0.9768115942028985\n",
      "EPOCH=1041\n",
      "train: loss=0.0782532074614211 acc=0.9782608695652174\n",
      "test: loss=0.09006818170537914 acc=0.9623188405797102\n",
      "EPOCH=1042\n",
      "train: loss=0.13318701004511196 acc=0.946376811594203\n",
      "test: loss=0.10591155003510751 acc=0.9608695652173913\n",
      "EPOCH=1043\n",
      "train: loss=0.0760731074000699 acc=0.9782608695652174\n",
      "test: loss=0.08756854390985425 acc=0.972463768115942\n",
      "EPOCH=1044\n",
      "train: loss=0.07702667587847736 acc=0.981159420289855\n",
      "test: loss=0.07966068046614665 acc=0.9753623188405797\n",
      "EPOCH=1045\n",
      "train: loss=0.08009540825790651 acc=0.9768115942028985\n",
      "test: loss=0.0801859760715993 acc=0.9753623188405797\n",
      "EPOCH=1046\n",
      "train: loss=0.06182674731840561 acc=0.9826086956521739\n",
      "test: loss=0.08522093649810997 acc=0.9695652173913043\n",
      "EPOCH=1047\n",
      "train: loss=0.06951550182447257 acc=0.9782608695652174\n",
      "test: loss=0.07975124326360222 acc=0.9782608695652174\n",
      "EPOCH=1048\n",
      "train: loss=0.05343052620228245 acc=0.9840579710144928\n",
      "test: loss=0.08873257977598516 acc=0.9782608695652174\n",
      "EPOCH=1049\n",
      "train: loss=0.06545713488708832 acc=0.9753623188405797\n",
      "test: loss=0.08120490935435361 acc=0.972463768115942\n",
      "EPOCH=1050\n",
      "train: loss=0.08937214498159501 acc=0.9739130434782609\n",
      "test: loss=0.08194892939313737 acc=0.9695652173913043\n",
      "EPOCH=1051\n",
      "train: loss=0.09073039055529693 acc=0.9782608695652174\n",
      "test: loss=0.07936930212972038 acc=0.9739130434782609\n",
      "EPOCH=1052\n",
      "train: loss=0.10896555753341282 acc=0.9637681159420289\n",
      "test: loss=0.07825259595927313 acc=0.9710144927536232\n",
      "EPOCH=1053\n",
      "train: loss=0.06561588467264536 acc=0.9855072463768116\n",
      "test: loss=0.0762486923185771 acc=0.9768115942028985\n",
      "EPOCH=1054\n",
      "train: loss=0.06609143008652736 acc=0.9797101449275363\n",
      "test: loss=0.07930893659463434 acc=0.9710144927536232\n",
      "EPOCH=1055\n",
      "train: loss=0.05193540640011683 acc=0.9855072463768116\n",
      "test: loss=0.07884054772301836 acc=0.9753623188405797\n",
      "EPOCH=1056\n",
      "train: loss=0.07142684363128911 acc=0.972463768115942\n",
      "test: loss=0.08123800892076974 acc=0.9782608695652174\n",
      "EPOCH=1057\n",
      "train: loss=0.058782255095886714 acc=0.9768115942028985\n",
      "test: loss=0.08747609050692826 acc=0.9695652173913043\n",
      "EPOCH=1058\n",
      "train: loss=0.09271002564634637 acc=0.9753623188405797\n",
      "test: loss=0.07942292611247635 acc=0.9782608695652174\n",
      "EPOCH=1059\n",
      "train: loss=0.09374983541372155 acc=0.9666666666666667\n",
      "test: loss=0.08028022194209261 acc=0.9710144927536232\n",
      "EPOCH=1060\n",
      "train: loss=0.10260292924779696 acc=0.9652173913043478\n",
      "test: loss=0.08696915622273763 acc=0.9695652173913043\n",
      "EPOCH=1061\n",
      "train: loss=0.14952466253047195 acc=0.9304347826086956\n",
      "test: loss=0.1147054078767622 acc=0.9492753623188406\n",
      "EPOCH=1062\n",
      "train: loss=0.08483086297251184 acc=0.972463768115942\n",
      "test: loss=0.08062822143588239 acc=0.9826086956521739\n",
      "EPOCH=1063\n",
      "train: loss=0.08121826791680285 acc=0.9681159420289855\n",
      "test: loss=0.078631340964253 acc=0.9782608695652174\n",
      "EPOCH=1064\n",
      "train: loss=0.06416217021279508 acc=0.9753623188405797\n",
      "test: loss=0.07980155595675831 acc=0.9768115942028985\n",
      "EPOCH=1065\n",
      "train: loss=0.08676840156020067 acc=0.972463768115942\n",
      "test: loss=0.07649048716991771 acc=0.9797101449275363\n",
      "EPOCH=1066\n",
      "train: loss=0.10102814030437024 acc=0.9695652173913043\n",
      "test: loss=0.08716121024619061 acc=0.9695652173913043\n",
      "EPOCH=1067\n",
      "train: loss=0.06093259550648862 acc=0.9840579710144928\n",
      "test: loss=0.08185965640420294 acc=0.9753623188405797\n",
      "EPOCH=1068\n",
      "train: loss=0.05055255549363971 acc=0.9855072463768116\n",
      "test: loss=0.0802495454487557 acc=0.9768115942028985\n",
      "EPOCH=1069\n",
      "train: loss=0.07243354514195668 acc=0.9826086956521739\n",
      "test: loss=0.0756694143436782 acc=0.9782608695652174\n",
      "EPOCH=1070\n",
      "train: loss=0.05932961940848384 acc=0.9768115942028985\n",
      "test: loss=0.07845174893376085 acc=0.9768115942028985\n",
      "EPOCH=1071\n",
      "train: loss=0.09868252684451928 acc=0.972463768115942\n",
      "test: loss=0.07776343205746943 acc=0.9739130434782609\n",
      "EPOCH=1072\n",
      "train: loss=0.07531604730782913 acc=0.9840579710144928\n",
      "test: loss=0.08190528751921804 acc=0.9768115942028985\n",
      "EPOCH=1073\n",
      "train: loss=0.07979865585839545 acc=0.9753623188405797\n",
      "test: loss=0.07571456323058956 acc=0.9753623188405797\n",
      "EPOCH=1074\n",
      "train: loss=0.08350350944853545 acc=0.9797101449275363\n",
      "test: loss=0.08134549225620526 acc=0.981159420289855\n",
      "EPOCH=1075\n",
      "train: loss=0.08640749357949089 acc=0.9695652173913043\n",
      "test: loss=0.08017146465385827 acc=0.9768115942028985\n",
      "EPOCH=1076\n",
      "train: loss=0.10229285103636225 acc=0.9637681159420289\n",
      "test: loss=0.0853303036022193 acc=0.9753623188405797\n",
      "EPOCH=1077\n",
      "train: loss=0.08551865907690982 acc=0.9681159420289855\n",
      "test: loss=0.08077743129251615 acc=0.9753623188405797\n",
      "EPOCH=1078\n",
      "train: loss=0.08102432797218835 acc=0.9710144927536232\n",
      "test: loss=0.0736687832591492 acc=0.9753623188405797\n",
      "EPOCH=1079\n",
      "train: loss=0.09538302645355379 acc=0.9652173913043478\n",
      "test: loss=0.08944729012422396 acc=0.9710144927536232\n",
      "EPOCH=1080\n",
      "train: loss=0.05584583041779095 acc=0.9797101449275363\n",
      "test: loss=0.08537657676757174 acc=0.9753623188405797\n",
      "EPOCH=1081\n",
      "train: loss=0.07280425072555002 acc=0.9652173913043478\n",
      "test: loss=0.092720038212162 acc=0.9695652173913043\n",
      "EPOCH=1082\n",
      "train: loss=0.05948867457378553 acc=0.9826086956521739\n",
      "test: loss=0.0845105695627934 acc=0.9782608695652174\n",
      "EPOCH=1083\n",
      "train: loss=0.11538373600660663 acc=0.9579710144927536\n",
      "test: loss=0.09944117442999659 acc=0.9594202898550724\n",
      "EPOCH=1084\n",
      "train: loss=0.10656255666696804 acc=0.9840579710144928\n",
      "test: loss=0.08290433053898223 acc=0.9768115942028985\n",
      "EPOCH=1085\n",
      "train: loss=0.15264757946657437 acc=0.9376811594202898\n",
      "test: loss=0.15973093190010576 acc=0.9289855072463769\n",
      "EPOCH=1086\n",
      "train: loss=0.11775235388233166 acc=0.946376811594203\n",
      "test: loss=0.17411544084489694 acc=0.9420289855072463\n",
      "EPOCH=1087\n",
      "train: loss=0.1689240126970562 acc=0.9492753623188406\n",
      "test: loss=0.14599987905970763 acc=0.9507246376811594\n",
      "EPOCH=1088\n",
      "train: loss=0.16892148985855238 acc=0.9434782608695652\n",
      "test: loss=0.1699301776286209 acc=0.9347826086956522\n",
      "EPOCH=1089\n",
      "train: loss=0.2970038629100735 acc=0.9014492753623189\n",
      "test: loss=0.2442630578403039 acc=0.9159420289855073\n",
      "EPOCH=1090\n",
      "train: loss=0.22258424226311035 acc=0.9130434782608695\n",
      "test: loss=0.1913692250801948 acc=0.9434782608695652\n",
      "EPOCH=1091\n",
      "train: loss=0.10006489401939735 acc=0.9608695652173913\n",
      "test: loss=0.0820552390554449 acc=0.9753623188405797\n",
      "EPOCH=1092\n",
      "train: loss=0.2251072216193162 acc=0.9333333333333333\n",
      "test: loss=0.17981205249535948 acc=0.9391304347826087\n",
      "EPOCH=1093\n",
      "train: loss=0.12405858233854654 acc=0.9579710144927536\n",
      "test: loss=0.10558134085224945 acc=0.9579710144927536\n",
      "EPOCH=1094\n",
      "train: loss=0.08617064259909997 acc=0.9797101449275363\n",
      "test: loss=0.09038258960328027 acc=0.9710144927536232\n",
      "EPOCH=1095\n",
      "train: loss=0.1701106945936356 acc=0.9405797101449276\n",
      "test: loss=0.15022479793715493 acc=0.9478260869565217\n",
      "EPOCH=1096\n",
      "train: loss=0.11717983642984989 acc=0.9579710144927536\n",
      "test: loss=0.13383146593830444 acc=0.9594202898550724\n",
      "EPOCH=1097\n",
      "train: loss=0.06615520491001814 acc=0.9739130434782609\n",
      "test: loss=0.10842363939157473 acc=0.9710144927536232\n",
      "EPOCH=1098\n",
      "train: loss=0.09335449344638387 acc=0.9666666666666667\n",
      "test: loss=0.08835400133545374 acc=0.972463768115942\n",
      "EPOCH=1099\n",
      "train: loss=0.0785138569859528 acc=0.972463768115942\n",
      "test: loss=0.07762415550451326 acc=0.9739130434782609\n",
      "EPOCH=1100\n",
      "train: loss=0.09070972764275359 acc=0.9637681159420289\n",
      "test: loss=0.09680670293570935 acc=0.9710144927536232\n",
      "EPOCH=1101\n",
      "train: loss=0.09255908829961146 acc=0.9637681159420289\n",
      "test: loss=0.08691520445692212 acc=0.9739130434782609\n",
      "EPOCH=1102\n",
      "train: loss=0.09499763008860619 acc=0.9695652173913043\n",
      "test: loss=0.08520003247408348 acc=0.9710144927536232\n",
      "EPOCH=1103\n",
      "train: loss=0.10905531395338343 acc=0.9681159420289855\n",
      "test: loss=0.10604955038909877 acc=0.9666666666666667\n",
      "EPOCH=1104\n",
      "train: loss=0.1441270319760836 acc=0.9623188405797102\n",
      "test: loss=0.14474080609747172 acc=0.9434782608695652\n",
      "EPOCH=1105\n",
      "train: loss=0.07070427601965754 acc=0.972463768115942\n",
      "test: loss=0.09133912814746027 acc=0.972463768115942\n",
      "EPOCH=1106\n",
      "train: loss=0.12225522637781516 acc=0.9579710144927536\n",
      "test: loss=0.10326478541605891 acc=0.9637681159420289\n",
      "EPOCH=1107\n",
      "train: loss=0.09405819763742074 acc=0.9710144927536232\n",
      "test: loss=0.0995458561091314 acc=0.9710144927536232\n",
      "EPOCH=1108\n",
      "train: loss=0.06442566740898545 acc=0.9782608695652174\n",
      "test: loss=0.08215302572387008 acc=0.9782608695652174\n",
      "EPOCH=1109\n",
      "train: loss=0.11511526400635613 acc=0.9652173913043478\n",
      "test: loss=0.09529383678867939 acc=0.9681159420289855\n",
      "EPOCH=1110\n",
      "train: loss=0.10281175289116261 acc=0.9695652173913043\n",
      "test: loss=0.08494161498485797 acc=0.9739130434782609\n",
      "EPOCH=1111\n",
      "train: loss=0.087877073122539 acc=0.9782608695652174\n",
      "test: loss=0.08324225742445548 acc=0.9797101449275363\n",
      "EPOCH=1112\n",
      "train: loss=0.06303135083066237 acc=0.9768115942028985\n",
      "test: loss=0.07896401195150199 acc=0.9768115942028985\n",
      "EPOCH=1113\n",
      "train: loss=0.10731953811868872 acc=0.9797101449275363\n",
      "test: loss=0.08085281056019798 acc=0.9710144927536232\n",
      "EPOCH=1114\n",
      "train: loss=0.08972415456876602 acc=0.9681159420289855\n",
      "test: loss=0.08128023724329794 acc=0.972463768115942\n",
      "EPOCH=1115\n",
      "train: loss=0.11208609689223739 acc=0.9565217391304348\n",
      "test: loss=0.07686366648241624 acc=0.9797101449275363\n",
      "EPOCH=1116\n",
      "train: loss=0.1156438867909464 acc=0.9695652173913043\n",
      "test: loss=0.08242406816372602 acc=0.972463768115942\n",
      "EPOCH=1117\n",
      "train: loss=0.1030198606360707 acc=0.9579710144927536\n",
      "test: loss=0.08223319442969586 acc=0.9739130434782609\n",
      "EPOCH=1118\n",
      "train: loss=0.07470803109261469 acc=0.972463768115942\n",
      "test: loss=0.09311064873057419 acc=0.9637681159420289\n",
      "EPOCH=1119\n",
      "train: loss=0.1018114383296012 acc=0.9492753623188406\n",
      "test: loss=0.07879781587647317 acc=0.9782608695652174\n",
      "EPOCH=1120\n",
      "train: loss=0.08804672051483886 acc=0.9739130434782609\n",
      "test: loss=0.07676103497846012 acc=0.9782608695652174\n",
      "EPOCH=1121\n",
      "train: loss=0.08645409676893592 acc=0.9652173913043478\n",
      "test: loss=0.08921522612719684 acc=0.9666666666666667\n",
      "EPOCH=1122\n",
      "train: loss=0.07431736342700059 acc=0.9710144927536232\n",
      "test: loss=0.07609767572301648 acc=0.9768115942028985\n",
      "EPOCH=1123\n",
      "train: loss=0.08606689745752687 acc=0.9710144927536232\n",
      "test: loss=0.0853360767959939 acc=0.9782608695652174\n",
      "EPOCH=1124\n",
      "train: loss=0.08932945046801903 acc=0.972463768115942\n",
      "test: loss=0.07723976554718905 acc=0.9753623188405797\n",
      "EPOCH=1125\n",
      "train: loss=0.05947261939273799 acc=0.9797101449275363\n",
      "test: loss=0.07234473967572251 acc=0.9753623188405797\n",
      "EPOCH=1126\n",
      "train: loss=0.05969370598158402 acc=0.981159420289855\n",
      "test: loss=0.07274388359038499 acc=0.9768115942028985\n",
      "EPOCH=1127\n",
      "train: loss=0.07857467640763828 acc=0.9652173913043478\n",
      "test: loss=0.07428164870518633 acc=0.981159420289855\n",
      "EPOCH=1128\n",
      "train: loss=0.09427627005026007 acc=0.9681159420289855\n",
      "test: loss=0.0783439335181404 acc=0.972463768115942\n",
      "EPOCH=1129\n",
      "train: loss=0.05844174034523223 acc=0.9782608695652174\n",
      "test: loss=0.07509316598331141 acc=0.9739130434782609\n",
      "EPOCH=1130\n",
      "train: loss=0.1385337464488411 acc=0.9507246376811594\n",
      "test: loss=0.07626255935927335 acc=0.9753623188405797\n",
      "EPOCH=1131\n",
      "train: loss=0.08347800013345909 acc=0.9695652173913043\n",
      "test: loss=0.08024619517160636 acc=0.9782608695652174\n",
      "EPOCH=1132\n",
      "train: loss=0.0892898362211202 acc=0.9681159420289855\n",
      "test: loss=0.07986572971491912 acc=0.9710144927536232\n",
      "EPOCH=1133\n",
      "train: loss=0.08099026307338487 acc=0.9739130434782609\n",
      "test: loss=0.08108340270468245 acc=0.972463768115942\n",
      "EPOCH=1134\n",
      "train: loss=0.08526738356593958 acc=0.9739130434782609\n",
      "test: loss=0.08856814832626707 acc=0.972463768115942\n",
      "EPOCH=1135\n",
      "train: loss=0.08422643614420385 acc=0.972463768115942\n",
      "test: loss=0.09195560408775992 acc=0.9666666666666667\n",
      "EPOCH=1136\n",
      "train: loss=0.12682156835505057 acc=0.9695652173913043\n",
      "test: loss=0.09617794278007892 acc=0.9652173913043478\n",
      "EPOCH=1137\n",
      "train: loss=0.19690273143368958 acc=0.9260869565217391\n",
      "test: loss=0.235629211832863 acc=0.908695652173913\n",
      "EPOCH=1138\n",
      "train: loss=0.13597871635934183 acc=0.9536231884057971\n",
      "test: loss=0.10347490916518336 acc=0.9623188405797102\n",
      "EPOCH=1139\n",
      "train: loss=0.12608640575747906 acc=0.9521739130434783\n",
      "test: loss=0.13433341749959948 acc=0.9507246376811594\n",
      "EPOCH=1140\n",
      "train: loss=0.15128127192625987 acc=0.9449275362318841\n",
      "test: loss=0.14995663872044868 acc=0.9333333333333333\n",
      "EPOCH=1141\n",
      "train: loss=0.16286267243148894 acc=0.9434782608695652\n",
      "test: loss=0.14226176483140537 acc=0.9507246376811594\n",
      "EPOCH=1142\n",
      "train: loss=0.181533214275029 acc=0.9391304347826087\n",
      "test: loss=0.12196301673328988 acc=0.9652173913043478\n",
      "EPOCH=1143\n",
      "train: loss=0.07648027800180643 acc=0.972463768115942\n",
      "test: loss=0.10657933831629242 acc=0.9623188405797102\n",
      "EPOCH=1144\n",
      "train: loss=0.11902067244543522 acc=0.9579710144927536\n",
      "test: loss=0.1255582593249872 acc=0.9565217391304348\n",
      "EPOCH=1145\n",
      "train: loss=0.08561571487451397 acc=0.9652173913043478\n",
      "test: loss=0.07859378430626285 acc=0.9753623188405797\n",
      "EPOCH=1146\n",
      "train: loss=0.07786838705154572 acc=0.981159420289855\n",
      "test: loss=0.08843264854595229 acc=0.9652173913043478\n",
      "EPOCH=1147\n",
      "train: loss=0.08029148283046891 acc=0.9710144927536232\n",
      "test: loss=0.11337907571927365 acc=0.9623188405797102\n",
      "EPOCH=1148\n",
      "train: loss=0.4594683247397625 acc=0.8376811594202899\n",
      "test: loss=0.39366120267645116 acc=0.8478260869565217\n",
      "EPOCH=1149\n",
      "train: loss=0.13038253182663456 acc=0.9623188405797102\n",
      "test: loss=0.12563920706640816 acc=0.9594202898550724\n",
      "EPOCH=1150\n",
      "train: loss=0.11225830941574462 acc=0.9666666666666667\n",
      "test: loss=0.10797073594166831 acc=0.9695652173913043\n",
      "EPOCH=1151\n",
      "train: loss=0.08509194329164356 acc=0.9681159420289855\n",
      "test: loss=0.08910913029691603 acc=0.9681159420289855\n",
      "EPOCH=1152\n",
      "train: loss=0.08261662404885406 acc=0.9768115942028985\n",
      "test: loss=0.0953529277707275 acc=0.9637681159420289\n",
      "EPOCH=1153\n",
      "train: loss=0.09603784324632214 acc=0.9608695652173913\n",
      "test: loss=0.08371304382887429 acc=0.9695652173913043\n",
      "EPOCH=1154\n",
      "train: loss=0.08454934143183601 acc=0.9681159420289855\n",
      "test: loss=0.09199386383136604 acc=0.9695652173913043\n",
      "EPOCH=1155\n",
      "train: loss=0.07495643085825462 acc=0.9739130434782609\n",
      "test: loss=0.080416824779893 acc=0.9739130434782609\n",
      "EPOCH=1156\n",
      "train: loss=0.10983713657879136 acc=0.9565217391304348\n",
      "test: loss=0.09233768819913847 acc=0.9623188405797102\n",
      "EPOCH=1157\n",
      "train: loss=0.13214826203629715 acc=0.9579710144927536\n",
      "test: loss=0.1157348017401498 acc=0.9550724637681159\n",
      "EPOCH=1158\n",
      "train: loss=0.07344751971610454 acc=0.9695652173913043\n",
      "test: loss=0.09122763510471082 acc=0.9637681159420289\n",
      "EPOCH=1159\n",
      "train: loss=0.09365969353117296 acc=0.9608695652173913\n",
      "test: loss=0.0913701909408972 acc=0.9652173913043478\n",
      "EPOCH=1160\n",
      "train: loss=0.06492276153802812 acc=0.9768115942028985\n",
      "test: loss=0.08781041152560146 acc=0.9666666666666667\n",
      "EPOCH=1161\n",
      "train: loss=0.09857536023794863 acc=0.9652173913043478\n",
      "test: loss=0.09244713930229981 acc=0.9681159420289855\n",
      "EPOCH=1162\n",
      "train: loss=0.0511401011659384 acc=0.9826086956521739\n",
      "test: loss=0.0740185862435486 acc=0.9797101449275363\n",
      "EPOCH=1163\n",
      "train: loss=0.07824435945246708 acc=0.972463768115942\n",
      "test: loss=0.08073280588707402 acc=0.9753623188405797\n",
      "EPOCH=1164\n",
      "train: loss=0.08385060070202806 acc=0.9753623188405797\n",
      "test: loss=0.08846849442757569 acc=0.972463768115942\n",
      "EPOCH=1165\n",
      "train: loss=0.11008921879580351 acc=0.9681159420289855\n",
      "test: loss=0.07630746834542082 acc=0.972463768115942\n",
      "EPOCH=1166\n",
      "train: loss=0.10545543261301893 acc=0.9782608695652174\n",
      "test: loss=0.07344882129796274 acc=0.9797101449275363\n",
      "EPOCH=1167\n",
      "train: loss=0.0731582645299942 acc=0.981159420289855\n",
      "test: loss=0.07314830086538507 acc=0.9782608695652174\n",
      "EPOCH=1168\n",
      "train: loss=0.07633546770524237 acc=0.9753623188405797\n",
      "test: loss=0.07215326921571881 acc=0.9782608695652174\n",
      "EPOCH=1169\n",
      "train: loss=0.06743453527610194 acc=0.9768115942028985\n",
      "test: loss=0.07978798933220556 acc=0.9739130434782609\n",
      "EPOCH=1170\n",
      "train: loss=0.13210766824043685 acc=0.9710144927536232\n",
      "test: loss=0.07850605319000152 acc=0.9739130434782609\n",
      "EPOCH=1171\n",
      "train: loss=0.11248657918157756 acc=0.972463768115942\n",
      "test: loss=0.07861175148134159 acc=0.981159420289855\n",
      "EPOCH=1172\n",
      "train: loss=0.07774529467337525 acc=0.9840579710144928\n",
      "test: loss=0.09905668061314953 acc=0.9666666666666667\n",
      "EPOCH=1173\n",
      "train: loss=0.06527498440066275 acc=0.9739130434782609\n",
      "test: loss=0.07326870700357406 acc=0.981159420289855\n",
      "EPOCH=1174\n",
      "train: loss=0.08769551622899796 acc=0.9753623188405797\n",
      "test: loss=0.07673480116042092 acc=0.972463768115942\n",
      "EPOCH=1175\n",
      "train: loss=0.06295648791138571 acc=0.972463768115942\n",
      "test: loss=0.08245268277237885 acc=0.9681159420289855\n",
      "EPOCH=1176\n",
      "train: loss=0.09261473307397812 acc=0.9695652173913043\n",
      "test: loss=0.08043721597035208 acc=0.9753623188405797\n",
      "EPOCH=1177\n",
      "train: loss=0.0886036887674184 acc=0.9637681159420289\n",
      "test: loss=0.07513982066103811 acc=0.972463768115942\n",
      "EPOCH=1178\n",
      "train: loss=0.08261461840810035 acc=0.9840579710144928\n",
      "test: loss=0.08275363428871739 acc=0.9739130434782609\n",
      "EPOCH=1179\n",
      "train: loss=0.05288362904962712 acc=0.981159420289855\n",
      "test: loss=0.07125670348351254 acc=0.981159420289855\n",
      "EPOCH=1180\n",
      "train: loss=0.06767640376483045 acc=0.9840579710144928\n",
      "test: loss=0.07743294316117999 acc=0.9739130434782609\n",
      "EPOCH=1181\n",
      "train: loss=0.09208670111996348 acc=0.972463768115942\n",
      "test: loss=0.07495878652035226 acc=0.9695652173913043\n",
      "EPOCH=1182\n",
      "train: loss=0.07693191669864094 acc=0.981159420289855\n",
      "test: loss=0.07745211373826334 acc=0.9768115942028985\n",
      "EPOCH=1183\n",
      "train: loss=0.07739792215440504 acc=0.9695652173913043\n",
      "test: loss=0.07777747148632615 acc=0.981159420289855\n",
      "EPOCH=1184\n",
      "train: loss=0.06107061733484563 acc=0.9826086956521739\n",
      "test: loss=0.07510636975853298 acc=0.9753623188405797\n",
      "EPOCH=1185\n",
      "train: loss=0.06791654265730894 acc=0.9826086956521739\n",
      "test: loss=0.07360257339800909 acc=0.981159420289855\n",
      "EPOCH=1186\n",
      "train: loss=0.06758521710434835 acc=0.981159420289855\n",
      "test: loss=0.07667845708889962 acc=0.9768115942028985\n",
      "EPOCH=1187\n",
      "train: loss=0.07235845841951012 acc=0.9797101449275363\n",
      "test: loss=0.07451666774493204 acc=0.981159420289855\n",
      "EPOCH=1188\n",
      "train: loss=0.1153504409232254 acc=0.9666666666666667\n",
      "test: loss=0.08340326127249165 acc=0.9652173913043478\n",
      "EPOCH=1189\n",
      "train: loss=0.07212362879717506 acc=0.9666666666666667\n",
      "test: loss=0.0769249500126822 acc=0.9826086956521739\n",
      "EPOCH=1190\n",
      "train: loss=0.09541737282898864 acc=0.9608695652173913\n",
      "test: loss=0.07979041109202917 acc=0.9710144927536232\n",
      "EPOCH=1191\n",
      "train: loss=0.0706292697671253 acc=0.981159420289855\n",
      "test: loss=0.08703582841855327 acc=0.9695652173913043\n",
      "EPOCH=1192\n",
      "train: loss=0.083886210851325 acc=0.9710144927536232\n",
      "test: loss=0.07528667200775879 acc=0.9797101449275363\n",
      "EPOCH=1193\n",
      "train: loss=0.0756390401224529 acc=0.9797101449275363\n",
      "test: loss=0.07728389048366852 acc=0.972463768115942\n",
      "EPOCH=1194\n",
      "train: loss=0.06898553945792645 acc=0.9782608695652174\n",
      "test: loss=0.0733571560698992 acc=0.9739130434782609\n",
      "EPOCH=1195\n",
      "train: loss=0.09091469730606434 acc=0.9739130434782609\n",
      "test: loss=0.07478455936525694 acc=0.9797101449275363\n",
      "EPOCH=1196\n",
      "train: loss=0.22981112517597568 acc=0.9217391304347826\n",
      "test: loss=0.1827505788666225 acc=0.9289855072463769\n",
      "EPOCH=1197\n",
      "train: loss=0.09891221106074473 acc=0.9826086956521739\n",
      "test: loss=0.07583365688609137 acc=0.9782608695652174\n",
      "EPOCH=1198\n",
      "train: loss=0.15504722720056618 acc=0.9623188405797102\n",
      "test: loss=0.07066228786957598 acc=0.9782608695652174\n",
      "EPOCH=1199\n",
      "train: loss=0.04532204202917654 acc=0.9855072463768116\n",
      "test: loss=0.0771212137611196 acc=0.9768115942028985\n",
      "EPOCH=1200\n",
      "train: loss=0.07598111011212799 acc=0.972463768115942\n",
      "test: loss=0.07293404163805345 acc=0.9753623188405797\n",
      "EPOCH=1201\n",
      "train: loss=0.055441522001497656 acc=0.9797101449275363\n",
      "test: loss=0.07160207741683301 acc=0.9768115942028985\n",
      "EPOCH=1202\n",
      "train: loss=0.11043873839791794 acc=0.9681159420289855\n",
      "test: loss=0.08101243119534264 acc=0.9797101449275363\n",
      "EPOCH=1203\n",
      "train: loss=0.06710849262476737 acc=0.9695652173913043\n",
      "test: loss=0.07356159421311648 acc=0.9782608695652174\n",
      "EPOCH=1204\n",
      "train: loss=0.1025316631670573 acc=0.9565217391304348\n",
      "test: loss=0.07128047905329274 acc=0.9768115942028985\n",
      "EPOCH=1205\n",
      "train: loss=0.06092107254639204 acc=0.9797101449275363\n",
      "test: loss=0.08659337759714118 acc=0.9695652173913043\n",
      "EPOCH=1206\n",
      "train: loss=0.07558136095715848 acc=0.9753623188405797\n",
      "test: loss=0.08950749754132825 acc=0.9681159420289855\n",
      "EPOCH=1207\n",
      "train: loss=0.09712544235113646 acc=0.9681159420289855\n",
      "test: loss=0.08984607296612201 acc=0.9681159420289855\n",
      "EPOCH=1208\n",
      "train: loss=0.10133816184421165 acc=0.9768115942028985\n",
      "test: loss=0.07819905160698515 acc=0.9666666666666667\n",
      "EPOCH=1209\n",
      "train: loss=0.10112216960622822 acc=0.9652173913043478\n",
      "test: loss=0.08108769705149667 acc=0.9666666666666667\n",
      "EPOCH=1210\n",
      "train: loss=0.07395129375939405 acc=0.9739130434782609\n",
      "test: loss=0.08750765743882796 acc=0.9768115942028985\n",
      "EPOCH=1211\n",
      "train: loss=0.049386143735133045 acc=0.9855072463768116\n",
      "test: loss=0.09053941814446914 acc=0.9768115942028985\n",
      "EPOCH=1212\n",
      "train: loss=0.06200266126652412 acc=0.9840579710144928\n",
      "test: loss=0.07951212844491852 acc=0.9753623188405797\n",
      "EPOCH=1213\n",
      "train: loss=0.06182155270299641 acc=0.9826086956521739\n",
      "test: loss=0.07205800375620237 acc=0.9753623188405797\n",
      "EPOCH=1214\n",
      "train: loss=0.06713284683088348 acc=0.9782608695652174\n",
      "test: loss=0.07626794224423239 acc=0.9753623188405797\n",
      "EPOCH=1215\n",
      "train: loss=0.05919693733079459 acc=0.9840579710144928\n",
      "test: loss=0.06516775347548792 acc=0.9768115942028985\n",
      "EPOCH=1216\n",
      "train: loss=0.06352588072863954 acc=0.9855072463768116\n",
      "test: loss=0.06960728662257733 acc=0.981159420289855\n",
      "EPOCH=1217\n",
      "train: loss=0.09897237285882102 acc=0.9637681159420289\n",
      "test: loss=0.07264322921532179 acc=0.9768115942028985\n",
      "EPOCH=1218\n",
      "train: loss=0.0596672764213843 acc=0.9782608695652174\n",
      "test: loss=0.07827956555267139 acc=0.9768115942028985\n",
      "EPOCH=1219\n",
      "train: loss=0.05995542759999466 acc=0.972463768115942\n",
      "test: loss=0.07221950397059879 acc=0.9768115942028985\n",
      "EPOCH=1220\n",
      "train: loss=0.060519656023519525 acc=0.9768115942028985\n",
      "test: loss=0.06768524755020518 acc=0.9797101449275363\n",
      "EPOCH=1221\n",
      "train: loss=0.08823780972823943 acc=0.9594202898550724\n",
      "test: loss=0.06918224439794995 acc=0.9768115942028985\n",
      "EPOCH=1222\n",
      "train: loss=0.08645148553352448 acc=0.9666666666666667\n",
      "test: loss=0.08090618386979453 acc=0.9797101449275363\n",
      "EPOCH=1223\n",
      "train: loss=0.05083444283535407 acc=0.9869565217391304\n",
      "test: loss=0.07073019324884006 acc=0.981159420289855\n",
      "EPOCH=1224\n",
      "train: loss=0.05856508870833553 acc=0.9768115942028985\n",
      "test: loss=0.07054994635239004 acc=0.9840579710144928\n",
      "EPOCH=1225\n",
      "train: loss=0.1096328913957064 acc=0.9623188405797102\n",
      "test: loss=0.09157926422229744 acc=0.9695652173913043\n",
      "EPOCH=1226\n",
      "train: loss=0.06560338528381515 acc=0.9768115942028985\n",
      "test: loss=0.0769229346863975 acc=0.9782608695652174\n",
      "EPOCH=1227\n",
      "train: loss=0.07192154541633126 acc=0.9681159420289855\n",
      "test: loss=0.07484277273857959 acc=0.9768115942028985\n",
      "EPOCH=1228\n",
      "train: loss=0.0942339260669354 acc=0.9768115942028985\n",
      "test: loss=0.0976567491777383 acc=0.9608695652173913\n",
      "EPOCH=1229\n",
      "train: loss=0.05061929471542497 acc=0.9840579710144928\n",
      "test: loss=0.08532219563681767 acc=0.9739130434782609\n",
      "EPOCH=1230\n",
      "train: loss=0.09023090222388416 acc=0.9753623188405797\n",
      "test: loss=0.07952801359786243 acc=0.9782608695652174\n",
      "EPOCH=1231\n",
      "train: loss=0.07048513031421033 acc=0.9782608695652174\n",
      "test: loss=0.07651430225460733 acc=0.9797101449275363\n",
      "EPOCH=1232\n",
      "train: loss=0.06623112017685746 acc=0.9782608695652174\n",
      "test: loss=0.09091332340038534 acc=0.9710144927536232\n",
      "EPOCH=1233\n",
      "train: loss=0.1269559700272752 acc=0.9623188405797102\n",
      "test: loss=0.07471656963598815 acc=0.9826086956521739\n",
      "EPOCH=1234\n",
      "train: loss=0.10084163084943515 acc=0.9637681159420289\n",
      "test: loss=0.08848214502632618 acc=0.9797101449275363\n",
      "EPOCH=1235\n",
      "train: loss=0.05316680623514716 acc=0.9840579710144928\n",
      "test: loss=0.08896464516367121 acc=0.9695652173913043\n",
      "EPOCH=1236\n",
      "train: loss=0.06649426581751969 acc=0.9739130434782609\n",
      "test: loss=0.070941970554628 acc=0.9768115942028985\n",
      "EPOCH=1237\n",
      "train: loss=0.08353618289260036 acc=0.972463768115942\n",
      "test: loss=0.07656711133088344 acc=0.972463768115942\n",
      "EPOCH=1238\n",
      "train: loss=0.09152378870527127 acc=0.9753623188405797\n",
      "test: loss=0.07322737104566962 acc=0.9797101449275363\n",
      "EPOCH=1239\n",
      "train: loss=0.05245233812321696 acc=0.9869565217391304\n",
      "test: loss=0.07694286104177311 acc=0.9739130434782609\n",
      "EPOCH=1240\n",
      "train: loss=0.06148079630959211 acc=0.9768115942028985\n",
      "test: loss=0.0738573451877728 acc=0.9782608695652174\n",
      "EPOCH=1241\n",
      "train: loss=0.09221073132345668 acc=0.9739130434782609\n",
      "test: loss=0.0746238128085861 acc=0.9797101449275363\n",
      "EPOCH=1242\n",
      "train: loss=0.10669089721550307 acc=0.9666666666666667\n",
      "test: loss=0.07352121111227498 acc=0.9768115942028985\n",
      "EPOCH=1243\n",
      "train: loss=0.04518828634512682 acc=0.9869565217391304\n",
      "test: loss=0.07031956677207572 acc=0.9782608695652174\n",
      "EPOCH=1244\n",
      "train: loss=0.07875313251405061 acc=0.9666666666666667\n",
      "test: loss=0.07209858953772337 acc=0.9739130434782609\n",
      "EPOCH=1245\n",
      "train: loss=0.07377539743457052 acc=0.972463768115942\n",
      "test: loss=0.07299926344355119 acc=0.9840579710144928\n",
      "EPOCH=1246\n",
      "train: loss=0.12084861447582183 acc=0.9768115942028985\n",
      "test: loss=0.07248319630409059 acc=0.981159420289855\n",
      "EPOCH=1247\n",
      "train: loss=0.10879928141136665 acc=0.9565217391304348\n",
      "test: loss=0.10038607840221662 acc=0.9652173913043478\n",
      "EPOCH=1248\n",
      "train: loss=0.07110128528396761 acc=0.972463768115942\n",
      "test: loss=0.07534283837028875 acc=0.9797101449275363\n",
      "EPOCH=1249\n",
      "train: loss=0.09341954682687442 acc=0.981159420289855\n",
      "test: loss=0.07255827476530928 acc=0.9768115942028985\n",
      "EPOCH=1250\n",
      "train: loss=0.04879617872679446 acc=0.9869565217391304\n",
      "test: loss=0.0725938568666412 acc=0.9753623188405797\n",
      "EPOCH=1251\n",
      "train: loss=0.049828851633869346 acc=0.9840579710144928\n",
      "test: loss=0.07568251326354902 acc=0.9768115942028985\n",
      "EPOCH=1252\n",
      "train: loss=0.08081898770871282 acc=0.9797101449275363\n",
      "test: loss=0.07547927231791002 acc=0.9739130434782609\n",
      "EPOCH=1253\n",
      "train: loss=0.0921372137237685 acc=0.9710144927536232\n",
      "test: loss=0.07899422413830053 acc=0.972463768115942\n",
      "EPOCH=1254\n",
      "train: loss=0.10399084441598083 acc=0.9652173913043478\n",
      "test: loss=0.08763750239104752 acc=0.9637681159420289\n",
      "EPOCH=1255\n",
      "train: loss=0.1641465763695871 acc=0.9449275362318841\n",
      "test: loss=0.2001061282914126 acc=0.9260869565217391\n",
      "EPOCH=1256\n",
      "train: loss=0.33579818428024005 acc=0.8855072463768116\n",
      "test: loss=0.26741087792196283 acc=0.9\n",
      "EPOCH=1257\n",
      "train: loss=0.15333178831953553 acc=0.9536231884057971\n",
      "test: loss=0.12780119856838543 acc=0.9579710144927536\n",
      "EPOCH=1258\n",
      "train: loss=0.15786728290212568 acc=0.9391304347826087\n",
      "test: loss=0.15481037742123144 acc=0.9449275362318841\n",
      "EPOCH=1259\n",
      "train: loss=0.12076802086912396 acc=0.9666666666666667\n",
      "test: loss=0.12761051215506963 acc=0.9594202898550724\n",
      "EPOCH=1260\n",
      "train: loss=0.13035814912597274 acc=0.9594202898550724\n",
      "test: loss=0.12508784288881228 acc=0.9536231884057971\n",
      "EPOCH=1261\n",
      "train: loss=0.16016791834451918 acc=0.9594202898550724\n",
      "test: loss=0.10639290360549959 acc=0.9637681159420289\n",
      "EPOCH=1262\n",
      "train: loss=0.10390893612607856 acc=0.9695652173913043\n",
      "test: loss=0.08734549570091857 acc=0.9666666666666667\n",
      "EPOCH=1263\n",
      "train: loss=0.17362499965795383 acc=0.946376811594203\n",
      "test: loss=0.12396960723608064 acc=0.9550724637681159\n",
      "EPOCH=1264\n",
      "train: loss=0.06548629666543315 acc=0.9826086956521739\n",
      "test: loss=0.08209873156054451 acc=0.9739130434782609\n",
      "EPOCH=1265\n",
      "train: loss=0.5486538779656346 acc=0.7956521739130434\n",
      "test: loss=0.44396911398100186 acc=0.8405797101449275\n",
      "EPOCH=1266\n",
      "train: loss=0.0662800513874268 acc=0.9782608695652174\n",
      "test: loss=0.07405491521050049 acc=0.9666666666666667\n",
      "EPOCH=1267\n",
      "train: loss=0.09235038884731227 acc=0.9666666666666667\n",
      "test: loss=0.07975636821564082 acc=0.9739130434782609\n",
      "EPOCH=1268\n",
      "train: loss=0.059153948344477 acc=0.9739130434782609\n",
      "test: loss=0.08985819352036886 acc=0.9768115942028985\n",
      "EPOCH=1269\n",
      "train: loss=0.07400174363598731 acc=0.9753623188405797\n",
      "test: loss=0.0830933004703041 acc=0.972463768115942\n",
      "EPOCH=1270\n",
      "train: loss=0.06100371932432439 acc=0.9782608695652174\n",
      "test: loss=0.07272778199592053 acc=0.981159420289855\n",
      "EPOCH=1271\n",
      "train: loss=0.09469741965785117 acc=0.9637681159420289\n",
      "test: loss=0.08686022779121942 acc=0.9652173913043478\n",
      "EPOCH=1272\n",
      "train: loss=0.1152208009352797 acc=0.9536231884057971\n",
      "test: loss=0.0920221498112309 acc=0.9768115942028985\n",
      "EPOCH=1273\n",
      "train: loss=0.08163522511214145 acc=0.972463768115942\n",
      "test: loss=0.07327507146412965 acc=0.9782608695652174\n",
      "EPOCH=1274\n",
      "train: loss=0.14179686952720139 acc=0.9536231884057971\n",
      "test: loss=0.12583662693077996 acc=0.9550724637681159\n",
      "EPOCH=1275\n",
      "train: loss=0.10311142204192644 acc=0.9739130434782609\n",
      "test: loss=0.0934172472234294 acc=0.9710144927536232\n",
      "EPOCH=1276\n",
      "train: loss=0.1004399216034289 acc=0.9608695652173913\n",
      "test: loss=0.10471436026951668 acc=0.9536231884057971\n",
      "EPOCH=1277\n",
      "train: loss=0.10259661402246066 acc=0.9739130434782609\n",
      "test: loss=0.07996250225831206 acc=0.9739130434782609\n",
      "EPOCH=1278\n",
      "train: loss=0.06196449777576375 acc=0.9797101449275363\n",
      "test: loss=0.08696103832485144 acc=0.9739130434782609\n",
      "EPOCH=1279\n",
      "train: loss=0.054832613942727024 acc=0.981159420289855\n",
      "test: loss=0.0820663217750535 acc=0.9710144927536232\n",
      "EPOCH=1280\n",
      "train: loss=0.09632796732316716 acc=0.9768115942028985\n",
      "test: loss=0.08061176370405894 acc=0.972463768115942\n",
      "EPOCH=1281\n",
      "train: loss=0.061921553270858104 acc=0.9753623188405797\n",
      "test: loss=0.080897018610819 acc=0.9739130434782609\n",
      "EPOCH=1282\n",
      "train: loss=0.09789111208329605 acc=0.9695652173913043\n",
      "test: loss=0.09115958568512127 acc=0.9695652173913043\n",
      "EPOCH=1283\n",
      "train: loss=0.08713483343555892 acc=0.9739130434782609\n",
      "test: loss=0.07878802233337387 acc=0.9797101449275363\n",
      "EPOCH=1284\n",
      "train: loss=0.09718844715737551 acc=0.9768115942028985\n",
      "test: loss=0.09006106333138406 acc=0.9695652173913043\n",
      "EPOCH=1285\n",
      "train: loss=0.0653415925409709 acc=0.9855072463768116\n",
      "test: loss=0.07520246305659749 acc=0.9797101449275363\n",
      "EPOCH=1286\n",
      "train: loss=0.04922596342439387 acc=0.9855072463768116\n",
      "test: loss=0.08243114496380989 acc=0.9753623188405797\n",
      "EPOCH=1287\n",
      "train: loss=0.06593234754695453 acc=0.9753623188405797\n",
      "test: loss=0.08894831659138631 acc=0.9739130434782609\n",
      "EPOCH=1288\n",
      "train: loss=0.059714616493250064 acc=0.9710144927536232\n",
      "test: loss=0.07754519000377262 acc=0.9768115942028985\n",
      "EPOCH=1289\n",
      "train: loss=0.07833010550428308 acc=0.9695652173913043\n",
      "test: loss=0.09003491759729683 acc=0.9695652173913043\n",
      "EPOCH=1290\n",
      "train: loss=0.11026127403899308 acc=0.9710144927536232\n",
      "test: loss=0.07706798844119901 acc=0.9753623188405797\n",
      "EPOCH=1291\n",
      "train: loss=0.12729728657114073 acc=0.9652173913043478\n",
      "test: loss=0.08058727807021594 acc=0.9753623188405797\n",
      "EPOCH=1292\n",
      "train: loss=0.0694438666700516 acc=0.9681159420289855\n",
      "test: loss=0.07528360932988966 acc=0.9768115942028985\n",
      "EPOCH=1293\n",
      "train: loss=0.07746368278552493 acc=0.9782608695652174\n",
      "test: loss=0.0737629383811242 acc=0.9768115942028985\n",
      "EPOCH=1294\n",
      "train: loss=0.08210256374951083 acc=0.972463768115942\n",
      "test: loss=0.07575425777200584 acc=0.9753623188405797\n",
      "EPOCH=1295\n",
      "train: loss=0.08949570816002131 acc=0.9666666666666667\n",
      "test: loss=0.07939878070046857 acc=0.9797101449275363\n",
      "EPOCH=1296\n",
      "train: loss=0.08032256630697227 acc=0.9782608695652174\n",
      "test: loss=0.08623480618934598 acc=0.9623188405797102\n",
      "EPOCH=1297\n",
      "train: loss=0.06939781936939489 acc=0.9710144927536232\n",
      "test: loss=0.07630643935818023 acc=0.9768115942028985\n",
      "EPOCH=1298\n",
      "train: loss=0.09029015728703148 acc=0.9666666666666667\n",
      "test: loss=0.08041035177989465 acc=0.9739130434782609\n",
      "EPOCH=1299\n",
      "train: loss=0.11523517943921048 acc=0.9579710144927536\n",
      "test: loss=0.07668084223350817 acc=0.9768115942028985\n",
      "EPOCH=1300\n",
      "train: loss=0.0771048505193353 acc=0.9753623188405797\n",
      "test: loss=0.08466296099232966 acc=0.9695652173913043\n",
      "EPOCH=1301\n",
      "train: loss=0.04336121576480617 acc=0.9855072463768116\n",
      "test: loss=0.07555373881405385 acc=0.9739130434782609\n",
      "EPOCH=1302\n",
      "train: loss=0.08016068659881605 acc=0.9695652173913043\n",
      "test: loss=0.07417101722924578 acc=0.981159420289855\n",
      "EPOCH=1303\n",
      "train: loss=0.07382273625720306 acc=0.9637681159420289\n",
      "test: loss=0.0753010158752055 acc=0.9826086956521739\n",
      "EPOCH=1304\n",
      "train: loss=0.06652011531067199 acc=0.981159420289855\n",
      "test: loss=0.08564586747189587 acc=0.972463768115942\n",
      "EPOCH=1305\n",
      "train: loss=0.10708816087867576 acc=0.9637681159420289\n",
      "test: loss=0.07459878814617482 acc=0.9782608695652174\n",
      "EPOCH=1306\n",
      "train: loss=0.09461321382827219 acc=0.9695652173913043\n",
      "test: loss=0.08421674348033263 acc=0.972463768115942\n",
      "EPOCH=1307\n",
      "train: loss=0.06127639604286427 acc=0.9782608695652174\n",
      "test: loss=0.0718006980822399 acc=0.9768115942028985\n",
      "EPOCH=1308\n",
      "train: loss=0.06771173925006585 acc=0.9753623188405797\n",
      "test: loss=0.0721199355917107 acc=0.9768115942028985\n",
      "EPOCH=1309\n",
      "train: loss=0.09819595337666107 acc=0.9855072463768116\n",
      "test: loss=0.07577488353693246 acc=0.9782608695652174\n",
      "EPOCH=1310\n",
      "train: loss=0.10381687488699903 acc=0.9608695652173913\n",
      "test: loss=0.08690841110845286 acc=0.972463768115942\n",
      "EPOCH=1311\n",
      "train: loss=0.06058211132878812 acc=0.9782608695652174\n",
      "test: loss=0.074536170246094 acc=0.972463768115942\n",
      "EPOCH=1312\n",
      "train: loss=0.10671568073028388 acc=0.9536231884057971\n",
      "test: loss=0.1224382016121349 acc=0.9449275362318841\n",
      "EPOCH=1313\n",
      "train: loss=0.1260045136839548 acc=0.9608695652173913\n",
      "test: loss=0.08252082817213277 acc=0.9695652173913043\n",
      "EPOCH=1314\n",
      "train: loss=0.09229992952115984 acc=0.9536231884057971\n",
      "test: loss=0.07378806853204944 acc=0.9768115942028985\n",
      "EPOCH=1315\n",
      "train: loss=0.11462137358676958 acc=0.9637681159420289\n",
      "test: loss=0.09277361250127668 acc=0.9666666666666667\n",
      "EPOCH=1316\n",
      "train: loss=0.17211442299240434 acc=0.9260869565217391\n",
      "test: loss=0.16183260709304922 acc=0.9289855072463769\n",
      "EPOCH=1317\n",
      "train: loss=0.07551234877182297 acc=0.9695652173913043\n",
      "test: loss=0.09532425024931551 acc=0.972463768115942\n",
      "EPOCH=1318\n",
      "train: loss=0.08710149028133514 acc=0.9797101449275363\n",
      "test: loss=0.09056780938096945 acc=0.9739130434782609\n",
      "EPOCH=1319\n",
      "train: loss=0.09922263570851221 acc=0.9739130434782609\n",
      "test: loss=0.07606273907489224 acc=0.9710144927536232\n",
      "EPOCH=1320\n",
      "train: loss=0.08875234836861837 acc=0.9695652173913043\n",
      "test: loss=0.07365943007951012 acc=0.9797101449275363\n",
      "EPOCH=1321\n",
      "train: loss=0.11977397509490245 acc=0.9666666666666667\n",
      "test: loss=0.07692156572917241 acc=0.9768115942028985\n",
      "EPOCH=1322\n",
      "train: loss=0.1147979155038892 acc=0.9521739130434783\n",
      "test: loss=0.1185471026743565 acc=0.9565217391304348\n",
      "EPOCH=1323\n",
      "train: loss=0.1195883402868612 acc=0.9565217391304348\n",
      "test: loss=0.0996486165209752 acc=0.9666666666666667\n",
      "EPOCH=1324\n",
      "train: loss=0.09552428404504341 acc=0.9637681159420289\n",
      "test: loss=0.07186964885064558 acc=0.9782608695652174\n",
      "EPOCH=1325\n",
      "train: loss=0.10527720906384093 acc=0.9536231884057971\n",
      "test: loss=0.09556123257818241 acc=0.9608695652173913\n",
      "EPOCH=1326\n",
      "train: loss=0.070828165792385 acc=0.9797101449275363\n",
      "test: loss=0.07723949283749589 acc=0.9768115942028985\n",
      "EPOCH=1327\n",
      "train: loss=0.07973317934349887 acc=0.9753623188405797\n",
      "test: loss=0.07697281734462268 acc=0.9797101449275363\n",
      "EPOCH=1328\n",
      "train: loss=0.07455405598881293 acc=0.9695652173913043\n",
      "test: loss=0.07778895712190602 acc=0.9782608695652174\n",
      "EPOCH=1329\n",
      "train: loss=0.06958343261102287 acc=0.9739130434782609\n",
      "test: loss=0.07982739769544725 acc=0.9782608695652174\n",
      "EPOCH=1330\n",
      "train: loss=0.0933076912865768 acc=0.9652173913043478\n",
      "test: loss=0.0758518041227527 acc=0.9797101449275363\n",
      "EPOCH=1331\n",
      "train: loss=0.08960566355075486 acc=0.972463768115942\n",
      "test: loss=0.07498504030906214 acc=0.9753623188405797\n",
      "EPOCH=1332\n",
      "train: loss=0.07833818829792699 acc=0.9666666666666667\n",
      "test: loss=0.09504100653870969 acc=0.9695652173913043\n",
      "EPOCH=1333\n",
      "train: loss=0.12688376402574686 acc=0.9637681159420289\n",
      "test: loss=0.075620061086498 acc=0.9826086956521739\n",
      "EPOCH=1334\n",
      "train: loss=0.09877362670784612 acc=0.9594202898550724\n",
      "test: loss=0.08741000733503777 acc=0.9681159420289855\n",
      "EPOCH=1335\n",
      "train: loss=0.04106398904573749 acc=0.9884057971014493\n",
      "test: loss=0.07669232334131466 acc=0.9797101449275363\n",
      "EPOCH=1336\n",
      "train: loss=0.08809672763597545 acc=0.9666666666666667\n",
      "test: loss=0.0700975962421914 acc=0.981159420289855\n",
      "EPOCH=1337\n",
      "train: loss=0.08642633313054371 acc=0.9666666666666667\n",
      "test: loss=0.08083509291632147 acc=0.9753623188405797\n",
      "EPOCH=1338\n",
      "train: loss=0.082032540124565 acc=0.9710144927536232\n",
      "test: loss=0.07588312103736802 acc=0.9753623188405797\n",
      "EPOCH=1339\n",
      "train: loss=0.11139437233494894 acc=0.9623188405797102\n",
      "test: loss=0.08146273849233962 acc=0.972463768115942\n",
      "EPOCH=1340\n",
      "train: loss=0.09315399990294473 acc=0.9579710144927536\n",
      "test: loss=0.08195795001090343 acc=0.9681159420289855\n",
      "EPOCH=1341\n",
      "train: loss=0.08528413734904419 acc=0.9710144927536232\n",
      "test: loss=0.07780109299157263 acc=0.972463768115942\n",
      "EPOCH=1342\n",
      "train: loss=0.07107701842344642 acc=0.9681159420289855\n",
      "test: loss=0.06972885938320363 acc=0.9753623188405797\n",
      "EPOCH=1343\n",
      "train: loss=0.11607847940269932 acc=0.9492753623188406\n",
      "test: loss=0.07540853349968767 acc=0.9739130434782609\n",
      "EPOCH=1344\n",
      "train: loss=0.07818457268273588 acc=0.9695652173913043\n",
      "test: loss=0.1088367657536178 acc=0.9536231884057971\n",
      "EPOCH=1345\n",
      "train: loss=0.11193665958429089 acc=0.9579710144927536\n",
      "test: loss=0.09409954096957844 acc=0.9739130434782609\n",
      "EPOCH=1346\n",
      "train: loss=0.10922078889097468 acc=0.9681159420289855\n",
      "test: loss=0.07096489663192558 acc=0.9797101449275363\n",
      "EPOCH=1347\n",
      "train: loss=0.10372311512692801 acc=0.9608695652173913\n",
      "test: loss=0.087311499436475 acc=0.9710144927536232\n",
      "EPOCH=1348\n",
      "train: loss=0.10266794470511818 acc=0.9710144927536232\n",
      "test: loss=0.07134933241860619 acc=0.9797101449275363\n",
      "EPOCH=1349\n",
      "train: loss=0.06840852878059368 acc=0.9840579710144928\n",
      "test: loss=0.07308558049936127 acc=0.9797101449275363\n",
      "EPOCH=1350\n",
      "train: loss=0.06452607393457827 acc=0.9753623188405797\n",
      "test: loss=0.07329023920561725 acc=0.9768115942028985\n",
      "EPOCH=1351\n",
      "train: loss=0.08957549649161071 acc=0.972463768115942\n",
      "test: loss=0.07243450629151692 acc=0.9840579710144928\n",
      "EPOCH=1352\n",
      "train: loss=0.0737167592978394 acc=0.9666666666666667\n",
      "test: loss=0.0728584102194884 acc=0.9782608695652174\n",
      "EPOCH=1353\n",
      "train: loss=0.0531311244598968 acc=0.9782608695652174\n",
      "test: loss=0.08282539700787461 acc=0.9739130434782609\n",
      "EPOCH=1354\n",
      "train: loss=0.04897812347630008 acc=0.9898550724637681\n",
      "test: loss=0.0744644191257917 acc=0.9768115942028985\n",
      "EPOCH=1355\n",
      "train: loss=0.07005751049211914 acc=0.9768115942028985\n",
      "test: loss=0.07367901353495827 acc=0.9753623188405797\n",
      "EPOCH=1356\n",
      "train: loss=0.1160578403961983 acc=0.9666666666666667\n",
      "test: loss=0.07444069608940863 acc=0.9840579710144928\n",
      "EPOCH=1357\n",
      "train: loss=0.09376777125451462 acc=0.9710144927536232\n",
      "test: loss=0.07357648549158327 acc=0.9768115942028985\n",
      "EPOCH=1358\n",
      "train: loss=0.10014192378333549 acc=0.972463768115942\n",
      "test: loss=0.07136363593918227 acc=0.981159420289855\n",
      "EPOCH=1359\n",
      "train: loss=0.06627846024478712 acc=0.9710144927536232\n",
      "test: loss=0.08140593962811404 acc=0.9768115942028985\n",
      "EPOCH=1360\n",
      "train: loss=0.11004199691108842 acc=0.9666666666666667\n",
      "test: loss=0.08790130142611775 acc=0.9666666666666667\n",
      "EPOCH=1361\n",
      "train: loss=0.08722545166754737 acc=0.9666666666666667\n",
      "test: loss=0.07269916879246369 acc=0.9797101449275363\n",
      "EPOCH=1362\n",
      "train: loss=0.07280610619718839 acc=0.981159420289855\n",
      "test: loss=0.09644491688459159 acc=0.9681159420289855\n",
      "EPOCH=1363\n",
      "train: loss=0.05381771866115613 acc=0.981159420289855\n",
      "test: loss=0.08036706182684412 acc=0.9768115942028985\n",
      "EPOCH=1364\n",
      "train: loss=0.08857187606034785 acc=0.9637681159420289\n",
      "test: loss=0.07649522639005281 acc=0.9797101449275363\n",
      "EPOCH=1365\n",
      "train: loss=0.08045199250603145 acc=0.9753623188405797\n",
      "test: loss=0.0811316915200373 acc=0.9753623188405797\n",
      "EPOCH=1366\n",
      "train: loss=0.06884446527760185 acc=0.9695652173913043\n",
      "test: loss=0.0771777247872021 acc=0.9797101449275363\n",
      "EPOCH=1367\n",
      "train: loss=0.059341373696652655 acc=0.9782608695652174\n",
      "test: loss=0.08224657327461027 acc=0.9782608695652174\n",
      "EPOCH=1368\n",
      "train: loss=0.0879408701087738 acc=0.9652173913043478\n",
      "test: loss=0.07612347471991229 acc=0.9797101449275363\n",
      "EPOCH=1369\n",
      "train: loss=0.07731239257642665 acc=0.9840579710144928\n",
      "test: loss=0.07430586924801016 acc=0.9753623188405797\n",
      "EPOCH=1370\n",
      "train: loss=0.1665192852984202 acc=0.9405797101449276\n",
      "test: loss=0.1387282371413662 acc=0.9507246376811594\n",
      "EPOCH=1371\n",
      "train: loss=0.10781754434134222 acc=0.9579710144927536\n",
      "test: loss=0.12449604479180026 acc=0.9507246376811594\n",
      "EPOCH=1372\n",
      "train: loss=0.16092763234379012 acc=0.9449275362318841\n",
      "test: loss=0.12847460512245534 acc=0.9478260869565217\n",
      "EPOCH=1373\n",
      "train: loss=0.16870168637036134 acc=0.9405797101449276\n",
      "test: loss=0.158757259298091 acc=0.9347826086956522\n",
      "EPOCH=1374\n",
      "train: loss=0.09973947371830479 acc=0.9579710144927536\n",
      "test: loss=0.1107983041199907 acc=0.9623188405797102\n",
      "EPOCH=1375\n",
      "train: loss=0.11753074861035702 acc=0.9550724637681159\n",
      "test: loss=0.10713893858328301 acc=0.9637681159420289\n",
      "EPOCH=1376\n",
      "train: loss=0.08133174233374159 acc=0.9666666666666667\n",
      "test: loss=0.08636276291542687 acc=0.9695652173913043\n",
      "EPOCH=1377\n",
      "train: loss=0.08847595435382762 acc=0.9739130434782609\n",
      "test: loss=0.07089460248529857 acc=0.9768115942028985\n",
      "EPOCH=1378\n",
      "train: loss=0.058438797216980996 acc=0.9768115942028985\n",
      "test: loss=0.07882014518809007 acc=0.9695652173913043\n",
      "EPOCH=1379\n",
      "train: loss=0.07888871688157392 acc=0.972463768115942\n",
      "test: loss=0.08138395673014513 acc=0.9739130434782609\n",
      "EPOCH=1380\n",
      "train: loss=0.07744487902419446 acc=0.9637681159420289\n",
      "test: loss=0.08039891241974635 acc=0.9652173913043478\n",
      "EPOCH=1381\n",
      "train: loss=0.070265136895871 acc=0.9797101449275363\n",
      "test: loss=0.07522840820493157 acc=0.9710144927536232\n",
      "EPOCH=1382\n",
      "train: loss=0.07934700404358909 acc=0.9695652173913043\n",
      "test: loss=0.07844955645699275 acc=0.9710144927536232\n",
      "EPOCH=1383\n",
      "train: loss=0.07409153484255716 acc=0.9782608695652174\n",
      "test: loss=0.08513763007327617 acc=0.9681159420289855\n",
      "EPOCH=1384\n",
      "train: loss=0.06921941040832462 acc=0.9797101449275363\n",
      "test: loss=0.07050733628867756 acc=0.9753623188405797\n",
      "EPOCH=1385\n",
      "train: loss=0.06189336564418958 acc=0.9768115942028985\n",
      "test: loss=0.07051247745107145 acc=0.9739130434782609\n",
      "EPOCH=1386\n",
      "train: loss=0.0695099382450764 acc=0.9681159420289855\n",
      "test: loss=0.0720536369744643 acc=0.9797101449275363\n",
      "EPOCH=1387\n",
      "train: loss=0.11833927676798613 acc=0.9739130434782609\n",
      "test: loss=0.09333717514525712 acc=0.9710144927536232\n",
      "EPOCH=1388\n",
      "train: loss=0.08044257222778636 acc=0.9695652173913043\n",
      "test: loss=0.07720516726372685 acc=0.9782608695652174\n",
      "EPOCH=1389\n",
      "train: loss=0.06573335743037384 acc=0.9797101449275363\n",
      "test: loss=0.07888071549969143 acc=0.972463768115942\n",
      "EPOCH=1390\n",
      "train: loss=0.07318974655995988 acc=0.9739130434782609\n",
      "test: loss=0.07621403724135428 acc=0.9739130434782609\n",
      "EPOCH=1391\n",
      "train: loss=0.08998746836610547 acc=0.9695652173913043\n",
      "test: loss=0.09480295813106644 acc=0.9681159420289855\n",
      "EPOCH=1392\n",
      "train: loss=0.10413963529239274 acc=0.9710144927536232\n",
      "test: loss=0.078682083488937 acc=0.9797101449275363\n",
      "EPOCH=1393\n",
      "train: loss=0.09313798992153224 acc=0.9695652173913043\n",
      "test: loss=0.07488905949727462 acc=0.9739130434782609\n",
      "EPOCH=1394\n",
      "train: loss=0.07926790413866007 acc=0.9710144927536232\n",
      "test: loss=0.09333619097032005 acc=0.9550724637681159\n",
      "EPOCH=1395\n",
      "train: loss=0.07289109298671498 acc=0.9782608695652174\n",
      "test: loss=0.07809408129906396 acc=0.9782608695652174\n",
      "EPOCH=1396\n",
      "train: loss=0.1012667839464313 acc=0.9710144927536232\n",
      "test: loss=0.07227475839871976 acc=0.981159420289855\n",
      "EPOCH=1397\n",
      "train: loss=0.0892568838558834 acc=0.9652173913043478\n",
      "test: loss=0.07766223367866448 acc=0.9782608695652174\n",
      "EPOCH=1398\n",
      "train: loss=0.07447709948767764 acc=0.9666666666666667\n",
      "test: loss=0.07583311989740679 acc=0.9782608695652174\n",
      "EPOCH=1399\n",
      "train: loss=0.060574611203825825 acc=0.9782608695652174\n",
      "test: loss=0.078963195790879 acc=0.9753623188405797\n",
      "EPOCH=1400\n",
      "train: loss=0.05687539586973492 acc=0.9782608695652174\n",
      "test: loss=0.08010314786643886 acc=0.981159420289855\n",
      "EPOCH=1401\n",
      "train: loss=0.07305639042425796 acc=0.9782608695652174\n",
      "test: loss=0.0787199573748209 acc=0.972463768115942\n",
      "EPOCH=1402\n",
      "train: loss=0.0844787940061866 acc=0.9797101449275363\n",
      "test: loss=0.08361710720979897 acc=0.9768115942028985\n",
      "EPOCH=1403\n",
      "train: loss=0.06698417935788387 acc=0.9797101449275363\n",
      "test: loss=0.07324594244300209 acc=0.9753623188405797\n",
      "EPOCH=1404\n",
      "train: loss=0.10219660436403614 acc=0.9637681159420289\n",
      "test: loss=0.07586998599639089 acc=0.9797101449275363\n",
      "EPOCH=1405\n",
      "train: loss=0.08388028769117453 acc=0.9652173913043478\n",
      "test: loss=0.07214235530431269 acc=0.9782608695652174\n",
      "EPOCH=1406\n",
      "train: loss=0.07313698215852081 acc=0.9797101449275363\n",
      "test: loss=0.0759589162488195 acc=0.9782608695652174\n",
      "EPOCH=1407\n",
      "train: loss=0.06058823075586662 acc=0.9768115942028985\n",
      "test: loss=0.07178998317275478 acc=0.9739130434782609\n",
      "EPOCH=1408\n",
      "train: loss=0.06858721314415638 acc=0.9695652173913043\n",
      "test: loss=0.08670665760086882 acc=0.9681159420289855\n",
      "EPOCH=1409\n",
      "train: loss=0.23447618486157562 acc=0.9188405797101449\n",
      "test: loss=0.17970150840525437 acc=0.9159420289855073\n",
      "EPOCH=1410\n",
      "train: loss=0.49166019191026034 acc=0.863768115942029\n",
      "test: loss=0.4830221760579024 acc=0.8507246376811595\n",
      "EPOCH=1411\n",
      "train: loss=0.36733506882768296 acc=0.8913043478260869\n",
      "test: loss=0.36563156635203253 acc=0.8797101449275362\n",
      "EPOCH=1412\n",
      "train: loss=0.11945127903193961 acc=0.9492753623188406\n",
      "test: loss=0.11476711420332314 acc=0.9565217391304348\n",
      "EPOCH=1413\n",
      "train: loss=0.12793120461378682 acc=0.9521739130434783\n",
      "test: loss=0.12629303031283798 acc=0.9521739130434783\n",
      "EPOCH=1414\n",
      "train: loss=0.10444194515409262 acc=0.9681159420289855\n",
      "test: loss=0.09900436582240003 acc=0.9666666666666667\n",
      "EPOCH=1415\n",
      "train: loss=0.05526507306235701 acc=0.9826086956521739\n",
      "test: loss=0.08660008587553275 acc=0.9710144927536232\n",
      "EPOCH=1416\n",
      "train: loss=0.1479227993854504 acc=0.9478260869565217\n",
      "test: loss=0.0971344105585307 acc=0.9608695652173913\n",
      "EPOCH=1417\n",
      "train: loss=0.054721830058263025 acc=0.981159420289855\n",
      "test: loss=0.07722685017740956 acc=0.9768115942028985\n",
      "EPOCH=1418\n",
      "train: loss=0.07078160692932293 acc=0.9826086956521739\n",
      "test: loss=0.07791339953787615 acc=0.981159420289855\n",
      "EPOCH=1419\n",
      "train: loss=0.06017708580400926 acc=0.9782608695652174\n",
      "test: loss=0.08939471531168186 acc=0.9710144927536232\n",
      "EPOCH=1420\n",
      "train: loss=0.06409022836592519 acc=0.9797101449275363\n",
      "test: loss=0.0868184709743175 acc=0.9710144927536232\n",
      "EPOCH=1421\n",
      "train: loss=0.09638232380280465 acc=0.9579710144927536\n",
      "test: loss=0.08483564285159463 acc=0.9768115942028985\n",
      "EPOCH=1422\n",
      "train: loss=0.08462422343722416 acc=0.9550724637681159\n",
      "test: loss=0.09550611126054977 acc=0.9739130434782609\n",
      "EPOCH=1423\n",
      "train: loss=0.07985905489798177 acc=0.972463768115942\n",
      "test: loss=0.08527841646783771 acc=0.9739130434782609\n",
      "EPOCH=1424\n",
      "train: loss=0.08003210874564184 acc=0.9637681159420289\n",
      "test: loss=0.07625866826253441 acc=0.9782608695652174\n",
      "EPOCH=1425\n",
      "train: loss=0.06480341967165579 acc=0.9797101449275363\n",
      "test: loss=0.08624034161373627 acc=0.9710144927536232\n",
      "EPOCH=1426\n",
      "train: loss=0.07717459257023104 acc=0.9681159420289855\n",
      "test: loss=0.07490442715536427 acc=0.9797101449275363\n",
      "EPOCH=1427\n",
      "train: loss=0.053083600456488134 acc=0.981159420289855\n",
      "test: loss=0.07284476948298943 acc=0.9782608695652174\n",
      "EPOCH=1428\n",
      "train: loss=0.0679772925346584 acc=0.972463768115942\n",
      "test: loss=0.07584183886366475 acc=0.972463768115942\n",
      "EPOCH=1429\n",
      "train: loss=0.0929758744532346 acc=0.9681159420289855\n",
      "test: loss=0.08289389032543897 acc=0.9739130434782609\n",
      "EPOCH=1430\n",
      "train: loss=0.11312809303129409 acc=0.9681159420289855\n",
      "test: loss=0.07069476101968063 acc=0.972463768115942\n",
      "EPOCH=1431\n",
      "train: loss=0.10098412289743888 acc=0.9492753623188406\n",
      "test: loss=0.07127599128708713 acc=0.9768115942028985\n",
      "EPOCH=1432\n",
      "train: loss=0.050499195217746026 acc=0.9826086956521739\n",
      "test: loss=0.07134989061165951 acc=0.972463768115942\n",
      "EPOCH=1433\n",
      "train: loss=0.05891581671428612 acc=0.9797101449275363\n",
      "test: loss=0.07363799338689538 acc=0.9768115942028985\n",
      "EPOCH=1434\n",
      "train: loss=0.06047654407763975 acc=0.9840579710144928\n",
      "test: loss=0.07364098969714128 acc=0.9753623188405797\n",
      "EPOCH=1435\n",
      "train: loss=0.11219178702110688 acc=0.9623188405797102\n",
      "test: loss=0.0717888378201941 acc=0.9782608695652174\n",
      "EPOCH=1436\n",
      "train: loss=0.0847947443974577 acc=0.9681159420289855\n",
      "test: loss=0.07251197825155474 acc=0.9739130434782609\n",
      "EPOCH=1437\n",
      "train: loss=0.08506713685336718 acc=0.9739130434782609\n",
      "test: loss=0.0728260598059714 acc=0.9753623188405797\n",
      "EPOCH=1438\n",
      "train: loss=0.09129385501798805 acc=0.9681159420289855\n",
      "test: loss=0.06870260084906049 acc=0.9753623188405797\n",
      "EPOCH=1439\n",
      "train: loss=0.0611480915628164 acc=0.9782608695652174\n",
      "test: loss=0.0704377360779709 acc=0.9753623188405797\n",
      "EPOCH=1440\n",
      "train: loss=0.09803563955197825 acc=0.9579710144927536\n",
      "test: loss=0.07165694789376613 acc=0.9739130434782609\n",
      "EPOCH=1441\n",
      "train: loss=0.08125653023329348 acc=0.972463768115942\n",
      "test: loss=0.07748069159570084 acc=0.9710144927536232\n",
      "EPOCH=1442\n",
      "train: loss=0.07485686819296414 acc=0.9753623188405797\n",
      "test: loss=0.07606530482667621 acc=0.9768115942028985\n",
      "EPOCH=1443\n",
      "train: loss=0.09349946100663545 acc=0.9695652173913043\n",
      "test: loss=0.06986886083733271 acc=0.9753623188405797\n",
      "EPOCH=1444\n",
      "train: loss=0.05924828936666029 acc=0.9797101449275363\n",
      "test: loss=0.07213289562384291 acc=0.9826086956521739\n",
      "EPOCH=1445\n",
      "train: loss=0.0646057188770517 acc=0.9826086956521739\n",
      "test: loss=0.07414725733081091 acc=0.9710144927536232\n",
      "EPOCH=1446\n",
      "train: loss=0.07963098394241507 acc=0.972463768115942\n",
      "test: loss=0.08233764788929494 acc=0.9768115942028985\n",
      "EPOCH=1447\n",
      "train: loss=0.08120312139639646 acc=0.9652173913043478\n",
      "test: loss=0.084786946832169 acc=0.9623188405797102\n",
      "EPOCH=1448\n",
      "train: loss=0.11521914385021383 acc=0.972463768115942\n",
      "test: loss=0.0776994075191422 acc=0.9768115942028985\n",
      "EPOCH=1449\n",
      "train: loss=0.10353484227487228 acc=0.9666666666666667\n",
      "test: loss=0.0849601472924719 acc=0.9652173913043478\n",
      "EPOCH=1450\n",
      "train: loss=0.11235238279426107 acc=0.9652173913043478\n",
      "test: loss=0.0716430433531953 acc=0.9826086956521739\n",
      "EPOCH=1451\n",
      "train: loss=0.06318928047896095 acc=0.9797101449275363\n",
      "test: loss=0.08786358230471049 acc=0.9739130434782609\n",
      "EPOCH=1452\n",
      "train: loss=0.09133162598004485 acc=0.9623188405797102\n",
      "test: loss=0.0919664022203119 acc=0.972463768115942\n",
      "EPOCH=1453\n",
      "train: loss=0.07847460755871238 acc=0.9826086956521739\n",
      "test: loss=0.0778130109200697 acc=0.9710144927536232\n",
      "EPOCH=1454\n",
      "train: loss=0.05385563523339755 acc=0.9797101449275363\n",
      "test: loss=0.07245273917973567 acc=0.9753623188405797\n",
      "EPOCH=1455\n",
      "train: loss=0.07650664971536995 acc=0.9637681159420289\n",
      "test: loss=0.07271984960747233 acc=0.9826086956521739\n",
      "EPOCH=1456\n",
      "train: loss=0.06306115385938768 acc=0.972463768115942\n",
      "test: loss=0.07392696618656404 acc=0.9797101449275363\n",
      "EPOCH=1457\n",
      "train: loss=0.06889856942426051 acc=0.9782608695652174\n",
      "test: loss=0.07083829990782581 acc=0.9797101449275363\n",
      "EPOCH=1458\n",
      "train: loss=0.09869270282100014 acc=0.9666666666666667\n",
      "test: loss=0.07328579011551957 acc=0.9797101449275363\n",
      "EPOCH=1459\n",
      "train: loss=0.13070212882487237 acc=0.9521739130434783\n",
      "test: loss=0.11542072860004558 acc=0.9507246376811594\n",
      "EPOCH=1460\n",
      "train: loss=0.13831323074360433 acc=0.9565217391304348\n",
      "test: loss=0.09423321600007917 acc=0.9550724637681159\n",
      "EPOCH=1461\n",
      "train: loss=0.08022170415623753 acc=0.9797101449275363\n",
      "test: loss=0.08938244527910133 acc=0.9695652173913043\n",
      "EPOCH=1462\n",
      "train: loss=0.07720248708318671 acc=0.9826086956521739\n",
      "test: loss=0.0930509469193854 acc=0.9637681159420289\n",
      "EPOCH=1463\n",
      "train: loss=0.13853588326240238 acc=0.9666666666666667\n",
      "test: loss=0.09503423519128879 acc=0.9681159420289855\n",
      "EPOCH=1464\n",
      "train: loss=0.0828930130195907 acc=0.9753623188405797\n",
      "test: loss=0.08995054102944058 acc=0.9710144927536232\n",
      "EPOCH=1465\n",
      "train: loss=0.23377256227135568 acc=0.9637681159420289\n",
      "test: loss=0.08229422505295277 acc=0.9695652173913043\n",
      "EPOCH=1466\n",
      "train: loss=0.08603325207392978 acc=0.9623188405797102\n",
      "test: loss=0.08259592738400436 acc=0.9797101449275363\n",
      "EPOCH=1467\n",
      "train: loss=0.08093322911166835 acc=0.9681159420289855\n",
      "test: loss=0.07976790100242268 acc=0.9753623188405797\n",
      "EPOCH=1468\n",
      "train: loss=0.072856912928941 acc=0.9710144927536232\n",
      "test: loss=0.07329494157503352 acc=0.9753623188405797\n",
      "EPOCH=1469\n",
      "train: loss=0.13198167246661738 acc=0.9492753623188406\n",
      "test: loss=0.09678065860067195 acc=0.9608695652173913\n",
      "EPOCH=1470\n",
      "train: loss=0.07921737817219475 acc=0.972463768115942\n",
      "test: loss=0.08290909451993261 acc=0.9753623188405797\n",
      "EPOCH=1471\n",
      "train: loss=0.11707587389934329 acc=0.9507246376811594\n",
      "test: loss=0.13999518577028613 acc=0.9449275362318841\n",
      "EPOCH=1472\n",
      "train: loss=0.06792246862239076 acc=0.9797101449275363\n",
      "test: loss=0.07835342703676865 acc=0.9753623188405797\n",
      "EPOCH=1473\n",
      "train: loss=0.09875786641576902 acc=0.9768115942028985\n",
      "test: loss=0.09920503001879069 acc=0.9652173913043478\n",
      "EPOCH=1474\n",
      "train: loss=0.1695378316588264 acc=0.9478260869565217\n",
      "test: loss=0.11960046731405947 acc=0.9521739130434783\n",
      "EPOCH=1475\n",
      "train: loss=0.12275112444914817 acc=0.9579710144927536\n",
      "test: loss=0.0888165960808145 acc=0.9652173913043478\n",
      "EPOCH=1476\n",
      "train: loss=0.07764801975273915 acc=0.9681159420289855\n",
      "test: loss=0.09324040502798285 acc=0.9594202898550724\n",
      "EPOCH=1477\n",
      "train: loss=0.20048697429572596 acc=0.9304347826086956\n",
      "test: loss=0.23199177862676898 acc=0.9057971014492754\n",
      "EPOCH=1478\n",
      "train: loss=0.10445067251749057 acc=0.9565217391304348\n",
      "test: loss=0.10338681352073772 acc=0.9652173913043478\n",
      "EPOCH=1479\n",
      "train: loss=0.18871868681291631 acc=0.9260869565217391\n",
      "test: loss=0.15361409702634266 acc=0.9420289855072463\n",
      "EPOCH=1480\n",
      "train: loss=0.07373612689820357 acc=0.9710144927536232\n",
      "test: loss=0.08091289181795125 acc=0.9739130434782609\n",
      "EPOCH=1481\n",
      "train: loss=0.06454078218977143 acc=0.9753623188405797\n",
      "test: loss=0.08463778121725124 acc=0.9695652173913043\n",
      "EPOCH=1482\n",
      "train: loss=0.11013761204375713 acc=0.9594202898550724\n",
      "test: loss=0.08001065966004907 acc=0.9739130434782609\n",
      "EPOCH=1483\n",
      "train: loss=0.09070563328026285 acc=0.9652173913043478\n",
      "test: loss=0.0723249470178458 acc=0.9768115942028985\n",
      "EPOCH=1484\n",
      "train: loss=0.07548835386868277 acc=0.9753623188405797\n",
      "test: loss=0.07548098644765523 acc=0.9753623188405797\n",
      "EPOCH=1485\n",
      "train: loss=0.09053230560229773 acc=0.972463768115942\n",
      "test: loss=0.06887202238490245 acc=0.9768115942028985\n",
      "EPOCH=1486\n",
      "train: loss=0.1285632542798934 acc=0.9478260869565217\n",
      "test: loss=0.1522017167316614 acc=0.9347826086956522\n",
      "EPOCH=1487\n",
      "train: loss=0.20293816132203538 acc=0.9318840579710145\n",
      "test: loss=0.16259763542004063 acc=0.9318840579710145\n",
      "EPOCH=1488\n",
      "train: loss=0.07946466525557057 acc=0.9608695652173913\n",
      "test: loss=0.07290161476288805 acc=0.972463768115942\n",
      "EPOCH=1489\n",
      "train: loss=0.11827612784366834 acc=0.9608695652173913\n",
      "test: loss=0.10478086016487165 acc=0.9637681159420289\n",
      "EPOCH=1490\n",
      "train: loss=0.08710768008212137 acc=0.9666666666666667\n",
      "test: loss=0.07929050633639476 acc=0.9710144927536232\n",
      "EPOCH=1491\n",
      "train: loss=0.059434331465900915 acc=0.9739130434782609\n",
      "test: loss=0.08196186152655216 acc=0.9710144927536232\n",
      "EPOCH=1492\n",
      "train: loss=0.06687306510528394 acc=0.9855072463768116\n",
      "test: loss=0.08693956327779995 acc=0.9753623188405797\n",
      "EPOCH=1493\n",
      "train: loss=0.07392220653874601 acc=0.9681159420289855\n",
      "test: loss=0.0783126994626893 acc=0.9768115942028985\n",
      "EPOCH=1494\n",
      "train: loss=0.048442870842748245 acc=0.9826086956521739\n",
      "test: loss=0.07686704714190881 acc=0.9753623188405797\n",
      "EPOCH=1495\n",
      "train: loss=0.11328374472267205 acc=0.9710144927536232\n",
      "test: loss=0.07491395690603463 acc=0.9782608695652174\n",
      "EPOCH=1496\n",
      "train: loss=0.07999988188616472 acc=0.9666666666666667\n",
      "test: loss=0.0721780643630874 acc=0.9753623188405797\n",
      "EPOCH=1497\n",
      "train: loss=0.08096784170147762 acc=0.9666666666666667\n",
      "test: loss=0.07617414777380505 acc=0.9710144927536232\n",
      "EPOCH=1498\n",
      "train: loss=0.08307158807701366 acc=0.9652173913043478\n",
      "test: loss=0.08135985415708537 acc=0.9695652173913043\n",
      "EPOCH=1499\n",
      "train: loss=0.07654360755238249 acc=0.9666666666666667\n",
      "test: loss=0.07080404857093925 acc=0.9782608695652174\n",
      "EPOCH=1500\n",
      "train: loss=0.05617419941061365 acc=0.981159420289855\n",
      "test: loss=0.0776941645819051 acc=0.9753623188405797\n",
      "EPOCH=1501\n",
      "train: loss=0.09068006939599908 acc=0.9710144927536232\n",
      "test: loss=0.06686760783611961 acc=0.9782608695652174\n",
      "EPOCH=1502\n",
      "train: loss=0.06354977099968764 acc=0.9782608695652174\n",
      "test: loss=0.07198425194550277 acc=0.9753623188405797\n",
      "EPOCH=1503\n",
      "train: loss=0.09088503180222868 acc=0.9768115942028985\n",
      "test: loss=0.07397865248779521 acc=0.9782608695652174\n",
      "EPOCH=1504\n",
      "train: loss=0.08080924463519058 acc=0.972463768115942\n",
      "test: loss=0.07111693969012027 acc=0.9710144927536232\n",
      "EPOCH=1505\n",
      "train: loss=0.06728305807090129 acc=0.972463768115942\n",
      "test: loss=0.0738238947014672 acc=0.9739130434782609\n",
      "EPOCH=1506\n",
      "train: loss=0.04341898873375959 acc=0.9869565217391304\n",
      "test: loss=0.07476610309251329 acc=0.9782608695652174\n",
      "EPOCH=1507\n",
      "train: loss=0.06624723396490939 acc=0.9695652173913043\n",
      "test: loss=0.0830917350490254 acc=0.972463768115942\n",
      "EPOCH=1508\n",
      "train: loss=0.10140131304178193 acc=0.972463768115942\n",
      "test: loss=0.08080586661845571 acc=0.9710144927536232\n",
      "EPOCH=1509\n",
      "train: loss=0.09268937611095067 acc=0.9666666666666667\n",
      "test: loss=0.07684507575475444 acc=0.9739130434782609\n",
      "EPOCH=1510\n",
      "train: loss=0.09555097623578239 acc=0.9666666666666667\n",
      "test: loss=0.07201555313438313 acc=0.9797101449275363\n",
      "EPOCH=1511\n",
      "train: loss=0.07508715675186677 acc=0.9782608695652174\n",
      "test: loss=0.06662700476430267 acc=0.981159420289855\n",
      "EPOCH=1512\n",
      "train: loss=0.07265722084600815 acc=0.981159420289855\n",
      "test: loss=0.07009926799037981 acc=0.9753623188405797\n",
      "EPOCH=1513\n",
      "train: loss=0.0845699955072914 acc=0.9681159420289855\n",
      "test: loss=0.06948890588813349 acc=0.9739130434782609\n",
      "EPOCH=1514\n",
      "train: loss=0.06403767130659294 acc=0.972463768115942\n",
      "test: loss=0.0744276188169495 acc=0.9768115942028985\n",
      "EPOCH=1515\n",
      "train: loss=0.051412367580432425 acc=0.9884057971014493\n",
      "test: loss=0.07118567077394854 acc=0.9753623188405797\n",
      "EPOCH=1516\n",
      "train: loss=0.13052258282327694 acc=0.9608695652173913\n",
      "test: loss=0.06442793311762836 acc=0.9782608695652174\n",
      "EPOCH=1517\n",
      "train: loss=0.06650801747938863 acc=0.9753623188405797\n",
      "test: loss=0.06981372857797596 acc=0.9782608695652174\n",
      "EPOCH=1518\n",
      "train: loss=0.11353932760048964 acc=0.981159420289855\n",
      "test: loss=0.06892737144769885 acc=0.9768115942028985\n",
      "EPOCH=1519\n",
      "train: loss=0.06750320703032026 acc=0.981159420289855\n",
      "test: loss=0.08293695044374239 acc=0.9710144927536232\n",
      "EPOCH=1520\n",
      "train: loss=0.08034188713197152 acc=0.9710144927536232\n",
      "test: loss=0.08492148637558554 acc=0.9695652173913043\n",
      "EPOCH=1521\n",
      "train: loss=0.08553996299273825 acc=0.972463768115942\n",
      "test: loss=0.07792134051825043 acc=0.981159420289855\n",
      "EPOCH=1522\n",
      "train: loss=0.06809222409264887 acc=0.972463768115942\n",
      "test: loss=0.07819483293836324 acc=0.9768115942028985\n",
      "EPOCH=1523\n",
      "train: loss=0.08709858219633548 acc=0.9652173913043478\n",
      "test: loss=0.07614081030505754 acc=0.9782608695652174\n",
      "EPOCH=1524\n",
      "train: loss=0.06303618283283752 acc=0.9753623188405797\n",
      "test: loss=0.0710733126205932 acc=0.9782608695652174\n",
      "EPOCH=1525\n",
      "train: loss=0.07102615770908377 acc=0.972463768115942\n",
      "test: loss=0.07074101848946457 acc=0.9782608695652174\n",
      "EPOCH=1526\n",
      "train: loss=0.07720379747627845 acc=0.9768115942028985\n",
      "test: loss=0.08466750591565583 acc=0.9739130434782609\n",
      "EPOCH=1527\n",
      "train: loss=0.061440177541285175 acc=0.981159420289855\n",
      "test: loss=0.07400124438222418 acc=0.9753623188405797\n",
      "EPOCH=1528\n",
      "train: loss=0.05272337659664154 acc=0.981159420289855\n",
      "test: loss=0.07554011629203938 acc=0.972463768115942\n",
      "EPOCH=1529\n",
      "train: loss=0.06566075939328246 acc=0.9753623188405797\n",
      "test: loss=0.0797006413693691 acc=0.9768115942028985\n",
      "EPOCH=1530\n",
      "train: loss=0.05901809058574096 acc=0.9797101449275363\n",
      "test: loss=0.06804663192907698 acc=0.981159420289855\n",
      "EPOCH=1531\n",
      "train: loss=0.0849050304250857 acc=0.9623188405797102\n",
      "test: loss=0.07517468148810202 acc=0.9710144927536232\n",
      "EPOCH=1532\n",
      "train: loss=0.08460931076125744 acc=0.9652173913043478\n",
      "test: loss=0.07343784313653103 acc=0.9753623188405797\n",
      "EPOCH=1533\n",
      "train: loss=0.09755494926452542 acc=0.9753623188405797\n",
      "test: loss=0.07059102739374266 acc=0.981159420289855\n",
      "EPOCH=1534\n",
      "train: loss=0.07088904461282569 acc=0.9739130434782609\n",
      "test: loss=0.07030895251898779 acc=0.9797101449275363\n",
      "EPOCH=1535\n",
      "train: loss=0.022650463254807428 acc=0.9942028985507246\n",
      "test: loss=0.07173528420069819 acc=0.9797101449275363\n",
      "EPOCH=1536\n",
      "train: loss=0.08370955195144776 acc=0.9753623188405797\n",
      "test: loss=0.0684656935646803 acc=0.9826086956521739\n",
      "EPOCH=1537\n",
      "train: loss=0.07092415226061245 acc=0.9826086956521739\n",
      "test: loss=0.07049413294229137 acc=0.9826086956521739\n",
      "EPOCH=1538\n",
      "train: loss=0.10608347257047432 acc=0.981159420289855\n",
      "test: loss=0.06993939347977512 acc=0.9739130434782609\n",
      "EPOCH=1539\n",
      "train: loss=0.0693535833637946 acc=0.9797101449275363\n",
      "test: loss=0.07301177410821973 acc=0.9768115942028985\n",
      "EPOCH=1540\n",
      "train: loss=0.1256816258217725 acc=0.9623188405797102\n",
      "test: loss=0.12121435368709006 acc=0.9594202898550724\n",
      "EPOCH=1541\n",
      "train: loss=0.1282926089847086 acc=0.9449275362318841\n",
      "test: loss=0.11561367722491876 acc=0.9521739130434783\n",
      "EPOCH=1542\n",
      "train: loss=0.06878932185651247 acc=0.9739130434782609\n",
      "test: loss=0.09233181808795375 acc=0.9753623188405797\n",
      "EPOCH=1543\n",
      "train: loss=0.13261018487612006 acc=0.9637681159420289\n",
      "test: loss=0.07795221159623222 acc=0.972463768115942\n",
      "EPOCH=1544\n",
      "train: loss=0.11222797244356758 acc=0.9623188405797102\n",
      "test: loss=0.07485447724465158 acc=0.9768115942028985\n",
      "EPOCH=1545\n",
      "train: loss=0.06735926886944493 acc=0.9782608695652174\n",
      "test: loss=0.07136780707210287 acc=0.9768115942028985\n",
      "EPOCH=1546\n",
      "train: loss=0.07930134883741786 acc=0.9695652173913043\n",
      "test: loss=0.0885083949984317 acc=0.9666666666666667\n",
      "EPOCH=1547\n",
      "train: loss=0.05770145849763768 acc=0.9768115942028985\n",
      "test: loss=0.07153905689014285 acc=0.981159420289855\n",
      "EPOCH=1548\n",
      "train: loss=0.08163987974458141 acc=0.9782608695652174\n",
      "test: loss=0.07329600505745607 acc=0.9768115942028985\n",
      "EPOCH=1549\n",
      "train: loss=0.06677112988176584 acc=0.9739130434782609\n",
      "test: loss=0.06702968091935989 acc=0.9768115942028985\n",
      "EPOCH=1550\n",
      "train: loss=0.16186096936284428 acc=0.9521739130434783\n",
      "test: loss=0.12222319114203502 acc=0.9492753623188406\n",
      "EPOCH=1551\n",
      "train: loss=0.05559394746186708 acc=0.9768115942028985\n",
      "test: loss=0.07565036968217305 acc=0.9797101449275363\n",
      "EPOCH=1552\n",
      "train: loss=0.0960271562713368 acc=0.9695652173913043\n",
      "test: loss=0.07191565984904924 acc=0.9768115942028985\n",
      "EPOCH=1553\n",
      "train: loss=0.0340278691613367 acc=0.9884057971014493\n",
      "test: loss=0.0800894858962403 acc=0.972463768115942\n",
      "EPOCH=1554\n",
      "train: loss=0.08539656944985521 acc=0.9623188405797102\n",
      "test: loss=0.0894440128561095 acc=0.9666666666666667\n",
      "EPOCH=1555\n",
      "train: loss=0.0979493871006225 acc=0.9637681159420289\n",
      "test: loss=0.10853702367184602 acc=0.9536231884057971\n",
      "EPOCH=1556\n",
      "train: loss=0.21451964706462104 acc=0.9246376811594202\n",
      "test: loss=0.2062270383658303 acc=0.9289855072463769\n",
      "EPOCH=1557\n",
      "train: loss=0.12936098610957414 acc=0.9434782608695652\n",
      "test: loss=0.11158891144556701 acc=0.9681159420289855\n",
      "EPOCH=1558\n",
      "train: loss=0.18648849942404097 acc=0.9376811594202898\n",
      "test: loss=0.1921379675043527 acc=0.9391304347826087\n",
      "EPOCH=1559\n",
      "train: loss=0.12994929358520932 acc=0.9695652173913043\n",
      "test: loss=0.11967797042542686 acc=0.9492753623188406\n",
      "EPOCH=1560\n",
      "train: loss=0.1322230134327242 acc=0.9565217391304348\n",
      "test: loss=0.10623900814046838 acc=0.9579710144927536\n",
      "EPOCH=1561\n",
      "train: loss=0.11136867832889068 acc=0.9521739130434783\n",
      "test: loss=0.09769993729313058 acc=0.9594202898550724\n",
      "EPOCH=1562\n",
      "train: loss=0.08241792848009782 acc=0.9652173913043478\n",
      "test: loss=0.08049792084107812 acc=0.9695652173913043\n",
      "EPOCH=1563\n",
      "train: loss=0.09126582837165724 acc=0.9739130434782609\n",
      "test: loss=0.07566423519588214 acc=0.972463768115942\n",
      "EPOCH=1564\n",
      "train: loss=0.11765690090819544 acc=0.9507246376811594\n",
      "test: loss=0.13381335650339277 acc=0.9420289855072463\n",
      "EPOCH=1565\n",
      "train: loss=0.08917646910488149 acc=0.9594202898550724\n",
      "test: loss=0.09064539968379402 acc=0.9710144927536232\n",
      "EPOCH=1566\n",
      "train: loss=0.11174257312537385 acc=0.9594202898550724\n",
      "test: loss=0.08253275311686374 acc=0.9666666666666667\n",
      "EPOCH=1567\n",
      "train: loss=0.09402009621048156 acc=0.9623188405797102\n",
      "test: loss=0.08740473510977448 acc=0.9681159420289855\n",
      "EPOCH=1568\n",
      "train: loss=0.06293542467462329 acc=0.9753623188405797\n",
      "test: loss=0.08257886007458319 acc=0.972463768115942\n",
      "EPOCH=1569\n",
      "train: loss=0.059582386129848654 acc=0.9782608695652174\n",
      "test: loss=0.0893800076216006 acc=0.9710144927536232\n",
      "EPOCH=1570\n",
      "train: loss=0.12530587265180124 acc=0.9681159420289855\n",
      "test: loss=0.07315913688861919 acc=0.9768115942028985\n",
      "EPOCH=1571\n",
      "train: loss=0.08637066993917421 acc=0.9753623188405797\n",
      "test: loss=0.08257194324992158 acc=0.9695652173913043\n",
      "EPOCH=1572\n",
      "train: loss=0.08751654264207465 acc=0.9710144927536232\n",
      "test: loss=0.07764505077694218 acc=0.972463768115942\n",
      "EPOCH=1573\n",
      "train: loss=0.0950375143980898 acc=0.9681159420289855\n",
      "test: loss=0.07731103931440496 acc=0.9753623188405797\n",
      "EPOCH=1574\n",
      "train: loss=0.07214267400818564 acc=0.9710144927536232\n",
      "test: loss=0.07409101505324706 acc=0.9739130434782609\n",
      "EPOCH=1575\n",
      "train: loss=0.08740419470809935 acc=0.9768115942028985\n",
      "test: loss=0.07776422180270748 acc=0.9710144927536232\n",
      "EPOCH=1576\n",
      "train: loss=0.11951439356000727 acc=0.9652173913043478\n",
      "test: loss=0.10328966882426213 acc=0.9594202898550724\n",
      "EPOCH=1577\n",
      "train: loss=0.06691685134032452 acc=0.9666666666666667\n",
      "test: loss=0.07349065859422799 acc=0.981159420289855\n",
      "EPOCH=1578\n",
      "train: loss=0.09855845997016129 acc=0.9594202898550724\n",
      "test: loss=0.09218846758488 acc=0.9739130434782609\n",
      "EPOCH=1579\n",
      "train: loss=0.11146049203209434 acc=0.9681159420289855\n",
      "test: loss=0.11485821702576114 acc=0.9666666666666667\n",
      "EPOCH=1580\n",
      "train: loss=0.08554180821047187 acc=0.9652173913043478\n",
      "test: loss=0.09782810005702719 acc=0.9681159420289855\n",
      "EPOCH=1581\n",
      "train: loss=0.061550541762457264 acc=0.9753623188405797\n",
      "test: loss=0.07448242598959305 acc=0.9855072463768116\n",
      "EPOCH=1582\n",
      "train: loss=0.07360660174469795 acc=0.9666666666666667\n",
      "test: loss=0.07451011587888508 acc=0.9782608695652174\n",
      "EPOCH=1583\n",
      "train: loss=0.07991588785534989 acc=0.9753623188405797\n",
      "test: loss=0.07207648433324475 acc=0.981159420289855\n",
      "EPOCH=1584\n",
      "train: loss=0.08038586736105045 acc=0.9623188405797102\n",
      "test: loss=0.07354639940692359 acc=0.9782608695652174\n",
      "EPOCH=1585\n",
      "train: loss=0.039379274369233906 acc=0.9884057971014493\n",
      "test: loss=0.07070635664103304 acc=0.9797101449275363\n",
      "EPOCH=1586\n",
      "train: loss=0.1088575599935937 acc=0.9652173913043478\n",
      "test: loss=0.06740565703392316 acc=0.9797101449275363\n",
      "EPOCH=1587\n",
      "train: loss=0.071736476939877 acc=0.9753623188405797\n",
      "test: loss=0.07746304534907755 acc=0.9710144927536232\n",
      "EPOCH=1588\n",
      "train: loss=0.09979580485619717 acc=0.9710144927536232\n",
      "test: loss=0.08979905094816203 acc=0.9695652173913043\n",
      "EPOCH=1589\n",
      "train: loss=0.10302969976407689 acc=0.9739130434782609\n",
      "test: loss=0.08339043285930564 acc=0.9753623188405797\n",
      "EPOCH=1590\n",
      "train: loss=0.08508748815485703 acc=0.9681159420289855\n",
      "test: loss=0.08862919122087011 acc=0.9768115942028985\n",
      "EPOCH=1591\n",
      "train: loss=0.15748477577473746 acc=0.9666666666666667\n",
      "test: loss=0.0800048375987951 acc=0.9695652173913043\n",
      "EPOCH=1592\n",
      "train: loss=0.07527847741805074 acc=0.9768115942028985\n",
      "test: loss=0.07924617966435521 acc=0.9753623188405797\n",
      "EPOCH=1593\n",
      "train: loss=0.1281301915084089 acc=0.9753623188405797\n",
      "test: loss=0.07073070334838942 acc=0.9826086956521739\n",
      "EPOCH=1594\n",
      "train: loss=0.08296083331659884 acc=0.9695652173913043\n",
      "test: loss=0.07555727383060011 acc=0.9768115942028985\n",
      "EPOCH=1595\n",
      "train: loss=0.09887684118853958 acc=0.9797101449275363\n",
      "test: loss=0.07651761595289233 acc=0.972463768115942\n",
      "EPOCH=1596\n",
      "train: loss=0.08761879828299256 acc=0.9637681159420289\n",
      "test: loss=0.07466990533184814 acc=0.9782608695652174\n",
      "EPOCH=1597\n",
      "train: loss=0.07880184162096188 acc=0.9797101449275363\n",
      "test: loss=0.07939028141202781 acc=0.9797101449275363\n",
      "EPOCH=1598\n",
      "train: loss=0.10409684099993557 acc=0.981159420289855\n",
      "test: loss=0.08298630643989093 acc=0.9753623188405797\n",
      "EPOCH=1599\n",
      "train: loss=0.07057900397040909 acc=0.9855072463768116\n",
      "test: loss=0.07817750283925073 acc=0.9782608695652174\n",
      "EPOCH=1600\n",
      "train: loss=0.05193495425062629 acc=0.9869565217391304\n",
      "test: loss=0.07059516242614153 acc=0.981159420289855\n",
      "EPOCH=1601\n",
      "train: loss=0.062329648375362055 acc=0.9782608695652174\n",
      "test: loss=0.06862089499116053 acc=0.981159420289855\n",
      "EPOCH=1602\n",
      "train: loss=0.054205725540913484 acc=0.9782608695652174\n",
      "test: loss=0.06736140587472905 acc=0.9768115942028985\n",
      "EPOCH=1603\n",
      "train: loss=0.06500560063258859 acc=0.981159420289855\n",
      "test: loss=0.07064460625593559 acc=0.9739130434782609\n",
      "EPOCH=1604\n",
      "train: loss=0.067955634331287 acc=0.9753623188405797\n",
      "test: loss=0.07711617881711509 acc=0.9768115942028985\n",
      "EPOCH=1605\n",
      "train: loss=0.12001393298203838 acc=0.9710144927536232\n",
      "test: loss=0.0915326910656637 acc=0.9637681159420289\n",
      "EPOCH=1606\n",
      "train: loss=0.08975691171783781 acc=0.9768115942028985\n",
      "test: loss=0.0946027625748285 acc=0.9768115942028985\n",
      "EPOCH=1607\n",
      "train: loss=0.056205237602602605 acc=0.9797101449275363\n",
      "test: loss=0.0895593103415215 acc=0.9681159420289855\n",
      "EPOCH=1608\n",
      "train: loss=0.051114623682076865 acc=0.9782608695652174\n",
      "test: loss=0.07330039273218238 acc=0.981159420289855\n",
      "EPOCH=1609\n",
      "train: loss=0.09069247925046647 acc=0.9681159420289855\n",
      "test: loss=0.10290952163559362 acc=0.9536231884057971\n",
      "EPOCH=1610\n",
      "train: loss=0.059922300954122605 acc=0.9855072463768116\n",
      "test: loss=0.09118063444837987 acc=0.9768115942028985\n",
      "EPOCH=1611\n",
      "train: loss=0.07635494339312379 acc=0.972463768115942\n",
      "test: loss=0.08742823162866416 acc=0.9753623188405797\n",
      "EPOCH=1612\n",
      "train: loss=0.05083569811104966 acc=0.9826086956521739\n",
      "test: loss=0.07925698611292206 acc=0.9753623188405797\n",
      "EPOCH=1613\n",
      "train: loss=0.06205978157112594 acc=0.9768115942028985\n",
      "test: loss=0.07056960910345184 acc=0.9840579710144928\n",
      "EPOCH=1614\n",
      "train: loss=0.07183019117335804 acc=0.9695652173913043\n",
      "test: loss=0.0765224013483888 acc=0.981159420289855\n",
      "EPOCH=1615\n",
      "train: loss=0.10860661700527954 acc=0.9739130434782609\n",
      "test: loss=0.070136339215543 acc=0.9782608695652174\n",
      "EPOCH=1616\n",
      "train: loss=0.08701852596522412 acc=0.9739130434782609\n",
      "test: loss=0.07989783255343913 acc=0.9710144927536232\n",
      "EPOCH=1617\n",
      "train: loss=0.07017072758711751 acc=0.972463768115942\n",
      "test: loss=0.06775649897598199 acc=0.9826086956521739\n",
      "EPOCH=1618\n",
      "train: loss=0.07616775834999374 acc=0.972463768115942\n",
      "test: loss=0.07265293689785776 acc=0.981159420289855\n",
      "EPOCH=1619\n",
      "train: loss=0.12253418340531708 acc=0.9681159420289855\n",
      "test: loss=0.06958198918320921 acc=0.9826086956521739\n",
      "EPOCH=1620\n",
      "train: loss=0.055194711357166325 acc=0.9898550724637681\n",
      "test: loss=0.0664535477748479 acc=0.9797101449275363\n",
      "EPOCH=1621\n",
      "train: loss=0.06661902567697256 acc=0.9652173913043478\n",
      "test: loss=0.07052960974254963 acc=0.9753623188405797\n",
      "EPOCH=1622\n",
      "train: loss=0.06789313675201092 acc=0.9840579710144928\n",
      "test: loss=0.07730460686370902 acc=0.9768115942028985\n",
      "EPOCH=1623\n",
      "train: loss=0.06688640985797265 acc=0.9884057971014493\n",
      "test: loss=0.06855222322878551 acc=0.9782608695652174\n",
      "EPOCH=1624\n",
      "train: loss=0.050536279603816514 acc=0.9869565217391304\n",
      "test: loss=0.07682265464472492 acc=0.9782608695652174\n",
      "EPOCH=1625\n",
      "train: loss=0.07074022312108474 acc=0.9797101449275363\n",
      "test: loss=0.07440320171869069 acc=0.9768115942028985\n",
      "EPOCH=1626\n",
      "train: loss=0.10150085652084 acc=0.9666666666666667\n",
      "test: loss=0.07284871723807274 acc=0.9840579710144928\n",
      "EPOCH=1627\n",
      "train: loss=0.08109418638506186 acc=0.9739130434782609\n",
      "test: loss=0.06775458989224321 acc=0.9797101449275363\n",
      "EPOCH=1628\n",
      "train: loss=0.14506531820307042 acc=0.9594202898550724\n",
      "test: loss=0.10768446429360383 acc=0.9594202898550724\n",
      "EPOCH=1629\n",
      "train: loss=0.12174752377453371 acc=0.9565217391304348\n",
      "test: loss=0.09519149266609714 acc=0.9579710144927536\n",
      "EPOCH=1630\n",
      "train: loss=0.08432407534847818 acc=0.9710144927536232\n",
      "test: loss=0.07596801085271102 acc=0.9782608695652174\n",
      "EPOCH=1631\n",
      "train: loss=0.14133420017049003 acc=0.9579710144927536\n",
      "test: loss=0.09198661506438567 acc=0.9695652173913043\n",
      "EPOCH=1632\n",
      "train: loss=0.08312253749871447 acc=0.9710144927536232\n",
      "test: loss=0.0809760884232143 acc=0.9710144927536232\n",
      "EPOCH=1633\n",
      "train: loss=0.09059166499999977 acc=0.9739130434782609\n",
      "test: loss=0.0856134076706099 acc=0.9739130434782609\n",
      "EPOCH=1634\n",
      "train: loss=0.09153526274772814 acc=0.9710144927536232\n",
      "test: loss=0.06992690606615268 acc=0.9768115942028985\n",
      "EPOCH=1635\n",
      "train: loss=0.06352873155940583 acc=0.9797101449275363\n",
      "test: loss=0.0699096396510111 acc=0.9782608695652174\n",
      "EPOCH=1636\n",
      "train: loss=0.060521339853070746 acc=0.9739130434782609\n",
      "test: loss=0.06665234043034722 acc=0.9826086956521739\n",
      "EPOCH=1637\n",
      "train: loss=0.051021335859221986 acc=0.981159420289855\n",
      "test: loss=0.06625441858939807 acc=0.981159420289855\n",
      "EPOCH=1638\n",
      "train: loss=0.0705588117450508 acc=0.9753623188405797\n",
      "test: loss=0.06565599656671665 acc=0.9826086956521739\n",
      "EPOCH=1639\n",
      "train: loss=0.07288965416756134 acc=0.9768115942028985\n",
      "test: loss=0.06806974235440214 acc=0.9782608695652174\n",
      "EPOCH=1640\n",
      "train: loss=0.0500867685618883 acc=0.9782608695652174\n",
      "test: loss=0.0655490529609817 acc=0.981159420289855\n",
      "EPOCH=1641\n",
      "train: loss=0.1154745835908188 acc=0.9536231884057971\n",
      "test: loss=0.10539812455526146 acc=0.9536231884057971\n",
      "EPOCH=1642\n",
      "train: loss=0.061809651469159614 acc=0.9695652173913043\n",
      "test: loss=0.06668346779195071 acc=0.9826086956521739\n",
      "EPOCH=1643\n",
      "train: loss=0.08119326308942086 acc=0.9826086956521739\n",
      "test: loss=0.07552836666682494 acc=0.9782608695652174\n",
      "EPOCH=1644\n",
      "train: loss=0.05947800634942877 acc=0.9782608695652174\n",
      "test: loss=0.067507960546382 acc=0.9869565217391304\n",
      "EPOCH=1645\n",
      "train: loss=0.07226558005547157 acc=0.9652173913043478\n",
      "test: loss=0.06670189617111186 acc=0.9855072463768116\n",
      "EPOCH=1646\n",
      "train: loss=0.07356907618942324 acc=0.972463768115942\n",
      "test: loss=0.06540547516300954 acc=0.9840579710144928\n",
      "EPOCH=1647\n",
      "train: loss=0.04496052025515505 acc=0.9855072463768116\n",
      "test: loss=0.06966049521528257 acc=0.9768115942028985\n",
      "EPOCH=1648\n",
      "train: loss=0.07641967226477549 acc=0.9666666666666667\n",
      "test: loss=0.06937801582491866 acc=0.9826086956521739\n",
      "EPOCH=1649\n",
      "train: loss=0.07198774537926989 acc=0.972463768115942\n",
      "test: loss=0.06775118648490101 acc=0.981159420289855\n",
      "EPOCH=1650\n",
      "train: loss=0.06271132028050182 acc=0.9739130434782609\n",
      "test: loss=0.06285069462321152 acc=0.9826086956521739\n",
      "EPOCH=1651\n",
      "train: loss=0.04626952615719383 acc=0.9869565217391304\n",
      "test: loss=0.06798641733981174 acc=0.9797101449275363\n",
      "EPOCH=1652\n",
      "train: loss=0.08363674858741842 acc=0.9710144927536232\n",
      "test: loss=0.06616635247851468 acc=0.9826086956521739\n",
      "EPOCH=1653\n",
      "train: loss=0.09721674584069384 acc=0.9565217391304348\n",
      "test: loss=0.06509282816910196 acc=0.9782608695652174\n",
      "EPOCH=1654\n",
      "train: loss=0.0749560023497009 acc=0.9739130434782609\n",
      "test: loss=0.06730873689327871 acc=0.981159420289855\n",
      "EPOCH=1655\n",
      "train: loss=0.07644297288489486 acc=0.9768115942028985\n",
      "test: loss=0.07801277851394844 acc=0.972463768115942\n",
      "EPOCH=1656\n",
      "train: loss=0.06230221153862031 acc=0.9710144927536232\n",
      "test: loss=0.0655401045454641 acc=0.9840579710144928\n",
      "EPOCH=1657\n",
      "train: loss=0.058158598309589075 acc=0.9782608695652174\n",
      "test: loss=0.0641595854750646 acc=0.9797101449275363\n",
      "EPOCH=1658\n",
      "train: loss=0.07666286260220526 acc=0.9797101449275363\n",
      "test: loss=0.065761592240705 acc=0.981159420289855\n",
      "EPOCH=1659\n",
      "train: loss=0.06661159975443226 acc=0.9884057971014493\n",
      "test: loss=0.06860148573032063 acc=0.9797101449275363\n",
      "EPOCH=1660\n",
      "train: loss=0.07819793382111165 acc=0.9710144927536232\n",
      "test: loss=0.06407524475228028 acc=0.9840579710144928\n",
      "EPOCH=1661\n",
      "train: loss=0.06260536195449914 acc=0.9739130434782609\n",
      "test: loss=0.06544807869249 acc=0.9797101449275363\n",
      "EPOCH=1662\n",
      "train: loss=0.056396245066001986 acc=0.972463768115942\n",
      "test: loss=0.06493436076312688 acc=0.9768115942028985\n",
      "EPOCH=1663\n",
      "train: loss=0.0804899092506902 acc=0.972463768115942\n",
      "test: loss=0.06640671804076055 acc=0.981159420289855\n",
      "EPOCH=1664\n",
      "train: loss=0.050892135600224264 acc=0.9855072463768116\n",
      "test: loss=0.06669556693859381 acc=0.9768115942028985\n",
      "EPOCH=1665\n",
      "train: loss=0.05918046274483684 acc=0.9898550724637681\n",
      "test: loss=0.06421720446100103 acc=0.9840579710144928\n",
      "EPOCH=1666\n",
      "train: loss=0.060456340374112336 acc=0.9782608695652174\n",
      "test: loss=0.06430428764606236 acc=0.981159420289855\n",
      "EPOCH=1667\n",
      "train: loss=0.0889383699438723 acc=0.9768115942028985\n",
      "test: loss=0.06665433772996895 acc=0.9797101449275363\n",
      "EPOCH=1668\n",
      "train: loss=0.0758035726539038 acc=0.9695652173913043\n",
      "test: loss=0.06492890904867935 acc=0.981159420289855\n",
      "EPOCH=1669\n",
      "train: loss=0.12581422784332338 acc=0.9710144927536232\n",
      "test: loss=0.06441276520684915 acc=0.9840579710144928\n",
      "EPOCH=1670\n",
      "train: loss=0.04473539671663037 acc=0.9826086956521739\n",
      "test: loss=0.07337349503024246 acc=0.9753623188405797\n",
      "EPOCH=1671\n",
      "train: loss=0.04427317464797419 acc=0.981159420289855\n",
      "test: loss=0.06643519163924588 acc=0.9826086956521739\n",
      "EPOCH=1672\n",
      "train: loss=0.07577943405137295 acc=0.9782608695652174\n",
      "test: loss=0.0699307739259345 acc=0.9768115942028985\n",
      "EPOCH=1673\n",
      "train: loss=0.05503034634931955 acc=0.981159420289855\n",
      "test: loss=0.06846505333511695 acc=0.9768115942028985\n",
      "EPOCH=1674\n",
      "train: loss=0.0716101653213754 acc=0.9739130434782609\n",
      "test: loss=0.06300476245799791 acc=0.9840579710144928\n",
      "EPOCH=1675\n",
      "train: loss=0.07226461579412671 acc=0.9710144927536232\n",
      "test: loss=0.06345406558948606 acc=0.981159420289855\n",
      "EPOCH=1676\n",
      "train: loss=0.0640981805338166 acc=0.981159420289855\n",
      "test: loss=0.06333604085821673 acc=0.9826086956521739\n",
      "EPOCH=1677\n",
      "train: loss=0.0699019355883753 acc=0.9710144927536232\n",
      "test: loss=0.0630230142073181 acc=0.9797101449275363\n",
      "EPOCH=1678\n",
      "train: loss=0.0637869114798502 acc=0.972463768115942\n",
      "test: loss=0.0650949070170688 acc=0.9826086956521739\n",
      "EPOCH=1679\n",
      "train: loss=0.08966619059600293 acc=0.9681159420289855\n",
      "test: loss=0.06803156948676703 acc=0.9782608695652174\n",
      "EPOCH=1680\n",
      "train: loss=0.062307119850612 acc=0.9739130434782609\n",
      "test: loss=0.06410813476071323 acc=0.9884057971014493\n",
      "EPOCH=1681\n",
      "train: loss=0.0610930410257811 acc=0.9768115942028985\n",
      "test: loss=0.06365370055321148 acc=0.9884057971014493\n",
      "EPOCH=1682\n",
      "train: loss=0.07821675127831665 acc=0.9739130434782609\n",
      "test: loss=0.061758158490315795 acc=0.9855072463768116\n",
      "EPOCH=1683\n",
      "train: loss=0.08285785761196325 acc=0.9637681159420289\n",
      "test: loss=0.0645211635669455 acc=0.9782608695652174\n",
      "EPOCH=1684\n",
      "train: loss=0.05366380151291492 acc=0.9797101449275363\n",
      "test: loss=0.06488828255009743 acc=0.9826086956521739\n",
      "EPOCH=1685\n",
      "train: loss=0.05314596428944106 acc=0.981159420289855\n",
      "test: loss=0.06363594549078837 acc=0.9768115942028985\n",
      "EPOCH=1686\n",
      "train: loss=0.07709630382491031 acc=0.972463768115942\n",
      "test: loss=0.06874004094239357 acc=0.9739130434782609\n",
      "EPOCH=1687\n",
      "train: loss=0.048586326789791356 acc=0.9826086956521739\n",
      "test: loss=0.06508738037500887 acc=0.9869565217391304\n",
      "EPOCH=1688\n",
      "train: loss=0.08255663727183625 acc=0.9695652173913043\n",
      "test: loss=0.06940279844916707 acc=0.9768115942028985\n",
      "EPOCH=1689\n",
      "train: loss=0.04697493260657711 acc=0.991304347826087\n",
      "test: loss=0.06458277497373494 acc=0.9768115942028985\n",
      "EPOCH=1690\n",
      "train: loss=0.06157219516558977 acc=0.9797101449275363\n",
      "test: loss=0.05946279625511604 acc=0.981159420289855\n",
      "EPOCH=1691\n",
      "train: loss=0.06511626381814531 acc=0.9666666666666667\n",
      "test: loss=0.06649288971705228 acc=0.9869565217391304\n",
      "EPOCH=1692\n",
      "train: loss=0.05802162859109817 acc=0.9840579710144928\n",
      "test: loss=0.0676239901933311 acc=0.9739130434782609\n",
      "EPOCH=1693\n",
      "train: loss=0.1019396993201845 acc=0.9739130434782609\n",
      "test: loss=0.06602877444290368 acc=0.9826086956521739\n",
      "EPOCH=1694\n",
      "train: loss=0.0981382400868181 acc=0.972463768115942\n",
      "test: loss=0.06258484253662494 acc=0.9855072463768116\n",
      "EPOCH=1695\n",
      "train: loss=0.06607152047258555 acc=0.972463768115942\n",
      "test: loss=0.06232012382012691 acc=0.9840579710144928\n",
      "EPOCH=1696\n",
      "train: loss=0.1009907271684627 acc=0.9652173913043478\n",
      "test: loss=0.06115939035663608 acc=0.9782608695652174\n",
      "EPOCH=1697\n",
      "train: loss=0.10257253097392759 acc=0.9681159420289855\n",
      "test: loss=0.06383894262266947 acc=0.9797101449275363\n",
      "EPOCH=1698\n",
      "train: loss=0.07272978596782599 acc=0.9695652173913043\n",
      "test: loss=0.062392940372887604 acc=0.981159420289855\n",
      "EPOCH=1699\n",
      "train: loss=0.0520700457239915 acc=0.9797101449275363\n",
      "test: loss=0.06297560998970173 acc=0.9797101449275363\n",
      "EPOCH=1700\n",
      "train: loss=0.06478292048158305 acc=0.9840579710144928\n",
      "test: loss=0.06580755809334159 acc=0.981159420289855\n",
      "EPOCH=1701\n",
      "train: loss=0.03711711974839908 acc=0.9898550724637681\n",
      "test: loss=0.06271660697368298 acc=0.9782608695652174\n",
      "EPOCH=1702\n",
      "train: loss=0.05413476890843095 acc=0.9840579710144928\n",
      "test: loss=0.06280870224485693 acc=0.9797101449275363\n",
      "EPOCH=1703\n",
      "train: loss=0.07310044231714906 acc=0.9681159420289855\n",
      "test: loss=0.062410721245984714 acc=0.9797101449275363\n",
      "EPOCH=1704\n",
      "train: loss=0.05613757910072346 acc=0.9739130434782609\n",
      "test: loss=0.0625420767994314 acc=0.9782608695652174\n",
      "EPOCH=1705\n",
      "train: loss=0.06980987589349669 acc=0.972463768115942\n",
      "test: loss=0.0610032498557599 acc=0.9826086956521739\n",
      "EPOCH=1706\n",
      "train: loss=0.0834715505343122 acc=0.972463768115942\n",
      "test: loss=0.06616202631008201 acc=0.9739130434782609\n",
      "EPOCH=1707\n",
      "train: loss=0.06326501479260481 acc=0.9840579710144928\n",
      "test: loss=0.06219122550635133 acc=0.9826086956521739\n",
      "EPOCH=1708\n",
      "train: loss=0.04968517632829187 acc=0.9826086956521739\n",
      "test: loss=0.06059851299810836 acc=0.9768115942028985\n",
      "EPOCH=1709\n",
      "train: loss=0.07726367692448972 acc=0.9681159420289855\n",
      "test: loss=0.06580248518367325 acc=0.9782608695652174\n",
      "EPOCH=1710\n",
      "train: loss=0.07313421575639743 acc=0.9695652173913043\n",
      "test: loss=0.062023888767718485 acc=0.9869565217391304\n",
      "EPOCH=1711\n",
      "train: loss=0.06316672138290114 acc=0.9768115942028985\n",
      "test: loss=0.06364790004282976 acc=0.9826086956521739\n",
      "EPOCH=1712\n",
      "train: loss=0.07157031323645521 acc=0.9739130434782609\n",
      "test: loss=0.06431577300613055 acc=0.9826086956521739\n",
      "EPOCH=1713\n",
      "train: loss=0.061538848636201626 acc=0.972463768115942\n",
      "test: loss=0.0642094707556812 acc=0.981159420289855\n",
      "EPOCH=1714\n",
      "train: loss=0.060504807413655694 acc=0.9739130434782609\n",
      "test: loss=0.0625983948545641 acc=0.981159420289855\n",
      "EPOCH=1715\n",
      "train: loss=0.07582158472296419 acc=0.9681159420289855\n",
      "test: loss=0.06651753165569427 acc=0.9739130434782609\n",
      "EPOCH=1716\n",
      "train: loss=0.06731872402828197 acc=0.9695652173913043\n",
      "test: loss=0.06012724899839819 acc=0.9826086956521739\n",
      "EPOCH=1717\n",
      "train: loss=0.030452301866021485 acc=0.9927536231884058\n",
      "test: loss=0.06523103999201048 acc=0.9840579710144928\n",
      "EPOCH=1718\n",
      "train: loss=0.09251552222245507 acc=0.9695652173913043\n",
      "test: loss=0.06332075391340905 acc=0.981159420289855\n",
      "EPOCH=1719\n",
      "train: loss=0.08005299756213766 acc=0.9739130434782609\n",
      "test: loss=0.05946085937237582 acc=0.9840579710144928\n",
      "EPOCH=1720\n",
      "train: loss=0.08553245345933093 acc=0.9739130434782609\n",
      "test: loss=0.06187847411846224 acc=0.9855072463768116\n",
      "EPOCH=1721\n",
      "train: loss=0.0734934561721848 acc=0.9797101449275363\n",
      "test: loss=0.06156648719610002 acc=0.9797101449275363\n",
      "EPOCH=1722\n",
      "train: loss=0.073666510131475 acc=0.9695652173913043\n",
      "test: loss=0.06336124519283365 acc=0.9855072463768116\n",
      "EPOCH=1723\n",
      "train: loss=0.06715010179765547 acc=0.9739130434782609\n",
      "test: loss=0.06293123287114129 acc=0.9797101449275363\n",
      "EPOCH=1724\n",
      "train: loss=0.05617470701863108 acc=0.9753623188405797\n",
      "test: loss=0.06399639364592122 acc=0.981159420289855\n",
      "EPOCH=1725\n",
      "train: loss=0.07866603331764499 acc=0.9710144927536232\n",
      "test: loss=0.06132761750641055 acc=0.9840579710144928\n",
      "EPOCH=1726\n",
      "train: loss=0.04554826132892849 acc=0.9753623188405797\n",
      "test: loss=0.06859772757109282 acc=0.981159420289855\n",
      "EPOCH=1727\n",
      "train: loss=0.03646807159928262 acc=0.991304347826087\n",
      "test: loss=0.06453337787622829 acc=0.9782608695652174\n",
      "EPOCH=1728\n",
      "train: loss=0.09038952630515135 acc=0.9695652173913043\n",
      "test: loss=0.06051139322854942 acc=0.9826086956521739\n",
      "EPOCH=1729\n",
      "train: loss=0.07506989082581982 acc=0.9739130434782609\n",
      "test: loss=0.05892353329509256 acc=0.9855072463768116\n",
      "EPOCH=1730\n",
      "train: loss=0.05291640558091755 acc=0.9768115942028985\n",
      "test: loss=0.059148277218129056 acc=0.9826086956521739\n",
      "EPOCH=1731\n",
      "train: loss=0.10394051411850802 acc=0.9695652173913043\n",
      "test: loss=0.060184305499939084 acc=0.9826086956521739\n",
      "EPOCH=1732\n",
      "train: loss=0.04409171545315022 acc=0.9884057971014493\n",
      "test: loss=0.06321865020081317 acc=0.981159420289855\n",
      "EPOCH=1733\n",
      "train: loss=0.06054447627901194 acc=0.9753623188405797\n",
      "test: loss=0.06826549116235045 acc=0.981159420289855\n",
      "EPOCH=1734\n",
      "train: loss=0.06341450635824862 acc=0.9753623188405797\n",
      "test: loss=0.060461678279809784 acc=0.981159420289855\n",
      "EPOCH=1735\n",
      "train: loss=0.04716571775761714 acc=0.9826086956521739\n",
      "test: loss=0.061225165841832914 acc=0.981159420289855\n",
      "EPOCH=1736\n",
      "train: loss=0.062117930179971415 acc=0.9884057971014493\n",
      "test: loss=0.06008549008392976 acc=0.9797101449275363\n",
      "EPOCH=1737\n",
      "train: loss=0.05874876221722981 acc=0.9782608695652174\n",
      "test: loss=0.05935775254184071 acc=0.9840579710144928\n",
      "EPOCH=1738\n",
      "train: loss=0.0864119305481052 acc=0.9652173913043478\n",
      "test: loss=0.05863256776383899 acc=0.9840579710144928\n",
      "EPOCH=1739\n",
      "train: loss=0.020355717154381947 acc=0.9971014492753624\n",
      "test: loss=0.06098585190694327 acc=0.981159420289855\n",
      "EPOCH=1740\n",
      "train: loss=0.0613844411567298 acc=0.9753623188405797\n",
      "test: loss=0.06262397686498623 acc=0.9826086956521739\n",
      "EPOCH=1741\n",
      "train: loss=0.06754324589449306 acc=0.9768115942028985\n",
      "test: loss=0.05813143065583476 acc=0.9840579710144928\n",
      "EPOCH=1742\n",
      "train: loss=0.0785774178457469 acc=0.972463768115942\n",
      "test: loss=0.06052169730968249 acc=0.9840579710144928\n",
      "EPOCH=1743\n",
      "train: loss=0.0683670687311322 acc=0.9695652173913043\n",
      "test: loss=0.060250559234868134 acc=0.9797101449275363\n",
      "EPOCH=1744\n",
      "train: loss=0.04655797179503834 acc=0.9855072463768116\n",
      "test: loss=0.05844284273490776 acc=0.9797101449275363\n",
      "EPOCH=1745\n",
      "train: loss=0.0653494647662438 acc=0.9739130434782609\n",
      "test: loss=0.06063393749440512 acc=0.9855072463768116\n",
      "EPOCH=1746\n",
      "train: loss=0.05402006123742813 acc=0.9782608695652174\n",
      "test: loss=0.06247643478818777 acc=0.9826086956521739\n",
      "EPOCH=1747\n",
      "train: loss=0.04005926643216252 acc=0.9869565217391304\n",
      "test: loss=0.061352903390263225 acc=0.9855072463768116\n",
      "EPOCH=1748\n",
      "train: loss=0.05791761561259906 acc=0.9753623188405797\n",
      "test: loss=0.06209966231525175 acc=0.9797101449275363\n",
      "EPOCH=1749\n",
      "train: loss=0.06674762769831594 acc=0.972463768115942\n",
      "test: loss=0.05937489077439857 acc=0.9840579710144928\n",
      "EPOCH=1750\n",
      "train: loss=0.049639757163663686 acc=0.9797101449275363\n",
      "test: loss=0.06106953743376307 acc=0.9826086956521739\n",
      "EPOCH=1751\n",
      "train: loss=0.07366771561095138 acc=0.972463768115942\n",
      "test: loss=0.06016299639404225 acc=0.9826086956521739\n",
      "EPOCH=1752\n",
      "train: loss=0.07255316129709856 acc=0.9681159420289855\n",
      "test: loss=0.060448695539914526 acc=0.9797101449275363\n",
      "EPOCH=1753\n",
      "train: loss=0.06746034230407708 acc=0.9710144927536232\n",
      "test: loss=0.06016499404350741 acc=0.9826086956521739\n",
      "EPOCH=1754\n",
      "train: loss=0.08365282802730392 acc=0.9797101449275363\n",
      "test: loss=0.06414373369688994 acc=0.981159420289855\n",
      "EPOCH=1755\n",
      "train: loss=0.0685829651517244 acc=0.9782608695652174\n",
      "test: loss=0.06284163764929293 acc=0.9840579710144928\n",
      "EPOCH=1756\n",
      "train: loss=0.0315169195463884 acc=0.9884057971014493\n",
      "test: loss=0.05946618930174808 acc=0.9826086956521739\n",
      "EPOCH=1757\n",
      "train: loss=0.06329997587939194 acc=0.9782608695652174\n",
      "test: loss=0.05948655712346013 acc=0.9826086956521739\n",
      "EPOCH=1758\n",
      "train: loss=0.05400029328736237 acc=0.9840579710144928\n",
      "test: loss=0.05913831472298287 acc=0.981159420289855\n",
      "EPOCH=1759\n",
      "train: loss=0.03821423319306175 acc=0.9927536231884058\n",
      "test: loss=0.05980146615419019 acc=0.9840579710144928\n",
      "EPOCH=1760\n",
      "train: loss=0.06482939113741383 acc=0.9753623188405797\n",
      "test: loss=0.05974566591312899 acc=0.9840579710144928\n",
      "EPOCH=1761\n",
      "train: loss=0.10399510642446384 acc=0.9710144927536232\n",
      "test: loss=0.06756523743116197 acc=0.972463768115942\n",
      "EPOCH=1762\n",
      "train: loss=0.06068798586564468 acc=0.981159420289855\n",
      "test: loss=0.06374149422369307 acc=0.9840579710144928\n",
      "EPOCH=1763\n",
      "train: loss=0.03314234111038705 acc=0.9898550724637681\n",
      "test: loss=0.05863238993055568 acc=0.9855072463768116\n",
      "EPOCH=1764\n",
      "train: loss=0.09141513344878137 acc=0.9666666666666667\n",
      "test: loss=0.06765485903159234 acc=0.9840579710144928\n",
      "EPOCH=1765\n",
      "train: loss=0.059834875691640727 acc=0.972463768115942\n",
      "test: loss=0.05807139549072908 acc=0.9840579710144928\n",
      "EPOCH=1766\n",
      "train: loss=0.0669080242324453 acc=0.9768115942028985\n",
      "test: loss=0.059108413495802474 acc=0.9855072463768116\n",
      "EPOCH=1767\n",
      "train: loss=0.07958582574944596 acc=0.9710144927536232\n",
      "test: loss=0.05736732353979016 acc=0.9826086956521739\n",
      "EPOCH=1768\n",
      "train: loss=0.04278355175143531 acc=0.9826086956521739\n",
      "test: loss=0.06016840320537542 acc=0.981159420289855\n",
      "EPOCH=1769\n",
      "train: loss=0.03291890902941114 acc=0.9898550724637681\n",
      "test: loss=0.05954292145346004 acc=0.9782608695652174\n",
      "EPOCH=1770\n",
      "train: loss=0.05637853082164163 acc=0.9797101449275363\n",
      "test: loss=0.06662649980996417 acc=0.981159420289855\n",
      "EPOCH=1771\n",
      "train: loss=0.059522133564384225 acc=0.9739130434782609\n",
      "test: loss=0.06365765689089169 acc=0.9840579710144928\n",
      "EPOCH=1772\n",
      "train: loss=0.049871545838446586 acc=0.9840579710144928\n",
      "test: loss=0.057609560986893016 acc=0.9797101449275363\n",
      "EPOCH=1773\n",
      "train: loss=0.05504907130780738 acc=0.972463768115942\n",
      "test: loss=0.0597859043941114 acc=0.981159420289855\n",
      "EPOCH=1774\n",
      "train: loss=0.05923800532558036 acc=0.9797101449275363\n",
      "test: loss=0.05959218822111023 acc=0.9797101449275363\n",
      "EPOCH=1775\n",
      "train: loss=0.06934253349252703 acc=0.9710144927536232\n",
      "test: loss=0.05858889301070059 acc=0.9753623188405797\n",
      "EPOCH=1776\n",
      "train: loss=0.06082309515064389 acc=0.9681159420289855\n",
      "test: loss=0.0593003970855191 acc=0.9840579710144928\n",
      "EPOCH=1777\n",
      "train: loss=0.04034769803996158 acc=0.9840579710144928\n",
      "test: loss=0.057600618252211966 acc=0.9855072463768116\n",
      "EPOCH=1778\n",
      "train: loss=0.04563796671381817 acc=0.9869565217391304\n",
      "test: loss=0.05947961695000715 acc=0.9869565217391304\n",
      "EPOCH=1779\n",
      "train: loss=0.044048050829279456 acc=0.9898550724637681\n",
      "test: loss=0.06244817123669362 acc=0.9826086956521739\n",
      "EPOCH=1780\n",
      "train: loss=0.04267079563510623 acc=0.9826086956521739\n",
      "test: loss=0.05925663939485444 acc=0.9797101449275363\n",
      "EPOCH=1781\n",
      "train: loss=0.07628349071567188 acc=0.9753623188405797\n",
      "test: loss=0.06045395961221043 acc=0.9826086956521739\n",
      "EPOCH=1782\n",
      "train: loss=0.05601411898496497 acc=0.9768115942028985\n",
      "test: loss=0.05814550893353445 acc=0.9797101449275363\n",
      "EPOCH=1783\n",
      "train: loss=0.04360892841795065 acc=0.9768115942028985\n",
      "test: loss=0.061274986379370254 acc=0.9840579710144928\n",
      "EPOCH=1784\n",
      "train: loss=0.08366709071666344 acc=0.972463768115942\n",
      "test: loss=0.05766468663625461 acc=0.9782608695652174\n",
      "EPOCH=1785\n",
      "train: loss=0.08200905497447337 acc=0.9739130434782609\n",
      "test: loss=0.059590088984448744 acc=0.9797101449275363\n",
      "EPOCH=1786\n",
      "train: loss=0.07310420648616454 acc=0.9739130434782609\n",
      "test: loss=0.06049696934276725 acc=0.9840579710144928\n",
      "EPOCH=1787\n",
      "train: loss=0.10271285902514052 acc=0.9681159420289855\n",
      "test: loss=0.05836590407205772 acc=0.9753623188405797\n",
      "EPOCH=1788\n",
      "train: loss=0.09788727038530612 acc=0.9637681159420289\n",
      "test: loss=0.061005033890327735 acc=0.9869565217391304\n",
      "EPOCH=1789\n",
      "train: loss=0.08845348245146685 acc=0.9710144927536232\n",
      "test: loss=0.05736835551328752 acc=0.9840579710144928\n",
      "EPOCH=1790\n",
      "train: loss=0.05345802405936705 acc=0.981159420289855\n",
      "test: loss=0.05689951668486351 acc=0.9855072463768116\n",
      "EPOCH=1791\n",
      "train: loss=0.03645254691069132 acc=0.9840579710144928\n",
      "test: loss=0.05867836686980469 acc=0.9782608695652174\n",
      "EPOCH=1792\n",
      "train: loss=0.07136667599903376 acc=0.9695652173913043\n",
      "test: loss=0.058440371769596704 acc=0.9826086956521739\n",
      "EPOCH=1793\n",
      "train: loss=0.046061882226101816 acc=0.981159420289855\n",
      "test: loss=0.05717607846120898 acc=0.9840579710144928\n",
      "EPOCH=1794\n",
      "train: loss=0.05655009434245783 acc=0.981159420289855\n",
      "test: loss=0.05749137557268098 acc=0.9840579710144928\n",
      "EPOCH=1795\n",
      "train: loss=0.060397322588900196 acc=0.981159420289855\n",
      "test: loss=0.056830136846500665 acc=0.9797101449275363\n",
      "EPOCH=1796\n",
      "train: loss=0.07988222680614007 acc=0.9666666666666667\n",
      "test: loss=0.05677368678624242 acc=0.981159420289855\n",
      "EPOCH=1797\n",
      "train: loss=0.043630490043705034 acc=0.9826086956521739\n",
      "test: loss=0.058271538879545864 acc=0.981159420289855\n",
      "EPOCH=1798\n",
      "train: loss=0.06071669395264043 acc=0.9782608695652174\n",
      "test: loss=0.05981712268464653 acc=0.981159420289855\n",
      "EPOCH=1799\n",
      "train: loss=0.04344589019472322 acc=0.981159420289855\n",
      "test: loss=0.05794621156321492 acc=0.981159420289855\n",
      "EPOCH=1800\n",
      "train: loss=0.062484368487005275 acc=0.981159420289855\n",
      "test: loss=0.056634372217154484 acc=0.9840579710144928\n",
      "EPOCH=1801\n",
      "train: loss=0.05961475224380668 acc=0.9855072463768116\n",
      "test: loss=0.05655029150526348 acc=0.981159420289855\n",
      "EPOCH=1802\n",
      "train: loss=0.07018635192944231 acc=0.9637681159420289\n",
      "test: loss=0.06871982344852924 acc=0.9753623188405797\n",
      "EPOCH=1803\n",
      "train: loss=0.05394264506980591 acc=0.9884057971014493\n",
      "test: loss=0.05805680157601652 acc=0.9840579710144928\n",
      "EPOCH=1804\n",
      "train: loss=0.05760802576116488 acc=0.9739130434782609\n",
      "test: loss=0.05812802630264656 acc=0.9840579710144928\n",
      "EPOCH=1805\n",
      "train: loss=0.05304581713842889 acc=0.9797101449275363\n",
      "test: loss=0.058170498476309465 acc=0.9869565217391304\n",
      "EPOCH=1806\n",
      "train: loss=0.05043384323674165 acc=0.9782608695652174\n",
      "test: loss=0.05824515518164698 acc=0.9797101449275363\n",
      "EPOCH=1807\n",
      "train: loss=0.04718008401936775 acc=0.9753623188405797\n",
      "test: loss=0.057252359312344785 acc=0.9855072463768116\n",
      "EPOCH=1808\n",
      "train: loss=0.0619439707060065 acc=0.9710144927536232\n",
      "test: loss=0.06387890964106017 acc=0.9797101449275363\n",
      "EPOCH=1809\n",
      "train: loss=0.07426301567558395 acc=0.9666666666666667\n",
      "test: loss=0.06445974045418018 acc=0.9782608695652174\n",
      "EPOCH=1810\n",
      "train: loss=0.07355454415114504 acc=0.9710144927536232\n",
      "test: loss=0.07108348981168823 acc=0.9768115942028985\n",
      "EPOCH=1811\n",
      "train: loss=0.055694629417437275 acc=0.9869565217391304\n",
      "test: loss=0.06434659602672196 acc=0.9695652173913043\n",
      "EPOCH=1812\n",
      "train: loss=0.07524268313474718 acc=0.9739130434782609\n",
      "test: loss=0.058405998557031776 acc=0.9826086956521739\n",
      "EPOCH=1813\n",
      "train: loss=0.036474807901816945 acc=0.9898550724637681\n",
      "test: loss=0.06750356603759215 acc=0.9695652173913043\n",
      "EPOCH=1814\n",
      "train: loss=0.06561640530367563 acc=0.981159420289855\n",
      "test: loss=0.05788775751373378 acc=0.9840579710144928\n",
      "EPOCH=1815\n",
      "train: loss=0.09701067538069255 acc=0.9623188405797102\n",
      "test: loss=0.09190864483880132 acc=0.9666666666666667\n",
      "EPOCH=1816\n",
      "train: loss=0.2811555704524223 acc=0.9304347826086956\n",
      "test: loss=0.2579403153555585 acc=0.9304347826086956\n",
      "EPOCH=1817\n",
      "train: loss=0.46752858910188516 acc=0.8579710144927536\n",
      "test: loss=0.5248557606221147 acc=0.8724637681159421\n",
      "EPOCH=1818\n",
      "train: loss=0.1627634387908108 acc=0.9434782608695652\n",
      "test: loss=0.1511544006183693 acc=0.9507246376811594\n",
      "EPOCH=1819\n",
      "train: loss=0.08884108626680132 acc=0.972463768115942\n",
      "test: loss=0.0875212441083899 acc=0.9608695652173913\n",
      "EPOCH=1820\n",
      "train: loss=0.07077677470607727 acc=0.972463768115942\n",
      "test: loss=0.07586675369083845 acc=0.9681159420289855\n",
      "EPOCH=1821\n",
      "train: loss=0.12379771142240616 acc=0.9681159420289855\n",
      "test: loss=0.08269296706991983 acc=0.972463768115942\n",
      "EPOCH=1822\n",
      "train: loss=0.10797887257219621 acc=0.9565217391304348\n",
      "test: loss=0.07070121355322347 acc=0.9768115942028985\n",
      "EPOCH=1823\n",
      "train: loss=0.10814071682223125 acc=0.9710144927536232\n",
      "test: loss=0.07354051395343952 acc=0.9753623188405797\n",
      "EPOCH=1824\n",
      "train: loss=0.047921965160105724 acc=0.9797101449275363\n",
      "test: loss=0.07088383782888512 acc=0.9739130434782609\n",
      "EPOCH=1825\n",
      "train: loss=0.06586776316225577 acc=0.9753623188405797\n",
      "test: loss=0.08727383435528693 acc=0.9666666666666667\n",
      "EPOCH=1826\n",
      "train: loss=0.07417496559644329 acc=0.9666666666666667\n",
      "test: loss=0.0805433490424367 acc=0.9739130434782609\n",
      "EPOCH=1827\n",
      "train: loss=0.08580241107866274 acc=0.9695652173913043\n",
      "test: loss=0.07748435454574047 acc=0.972463768115942\n",
      "EPOCH=1828\n",
      "train: loss=0.07599454357598404 acc=0.9782608695652174\n",
      "test: loss=0.07898195604717399 acc=0.9753623188405797\n",
      "EPOCH=1829\n",
      "train: loss=0.05542827074810659 acc=0.9797101449275363\n",
      "test: loss=0.07416358787063125 acc=0.9768115942028985\n",
      "EPOCH=1830\n",
      "train: loss=0.05801875563606496 acc=0.9753623188405797\n",
      "test: loss=0.07260795455553294 acc=0.9768115942028985\n",
      "EPOCH=1831\n",
      "train: loss=0.06044043791347563 acc=0.972463768115942\n",
      "test: loss=0.0736113008632598 acc=0.9782608695652174\n",
      "EPOCH=1832\n",
      "train: loss=0.07382853338594968 acc=0.9739130434782609\n",
      "test: loss=0.0778674273251639 acc=0.9739130434782609\n",
      "EPOCH=1833\n",
      "train: loss=0.11020936088285013 acc=0.9608695652173913\n",
      "test: loss=0.08113257445808732 acc=0.9710144927536232\n",
      "EPOCH=1834\n",
      "train: loss=0.12183475873826552 acc=0.946376811594203\n",
      "test: loss=0.0893876134941298 acc=0.9637681159420289\n",
      "EPOCH=1835\n",
      "train: loss=0.07936815924536159 acc=0.972463768115942\n",
      "test: loss=0.08883725895185109 acc=0.9637681159420289\n",
      "EPOCH=1836\n",
      "train: loss=0.07750621445225321 acc=0.9768115942028985\n",
      "test: loss=0.07622723673748005 acc=0.9739130434782609\n",
      "EPOCH=1837\n",
      "train: loss=0.04482937029438753 acc=0.9840579710144928\n",
      "test: loss=0.07179706380839171 acc=0.9782608695652174\n",
      "EPOCH=1838\n",
      "train: loss=0.0849497154055838 acc=0.9681159420289855\n",
      "test: loss=0.07305682019121956 acc=0.9768115942028985\n",
      "EPOCH=1839\n",
      "train: loss=0.07088142387327227 acc=0.9695652173913043\n",
      "test: loss=0.06709601164808032 acc=0.9768115942028985\n",
      "EPOCH=1840\n",
      "train: loss=0.09153582892102773 acc=0.9768115942028985\n",
      "test: loss=0.07287373657585068 acc=0.9753623188405797\n",
      "EPOCH=1841\n",
      "train: loss=0.05961100426071025 acc=0.9782608695652174\n",
      "test: loss=0.06826886361408704 acc=0.9753623188405797\n",
      "EPOCH=1842\n",
      "train: loss=0.07627795495636525 acc=0.972463768115942\n",
      "test: loss=0.07408417275783431 acc=0.9753623188405797\n",
      "EPOCH=1843\n",
      "train: loss=0.08560281203481918 acc=0.9710144927536232\n",
      "test: loss=0.07001269859012584 acc=0.981159420289855\n",
      "EPOCH=1844\n",
      "train: loss=0.06941916000571625 acc=0.972463768115942\n",
      "test: loss=0.06906697860515797 acc=0.9782608695652174\n",
      "EPOCH=1845\n",
      "train: loss=0.0735425664923572 acc=0.9753623188405797\n",
      "test: loss=0.06546708809094398 acc=0.9753623188405797\n",
      "EPOCH=1846\n",
      "train: loss=0.0851716137532389 acc=0.9666666666666667\n",
      "test: loss=0.06442278672943279 acc=0.9797101449275363\n",
      "EPOCH=1847\n",
      "train: loss=0.0734071304242362 acc=0.972463768115942\n",
      "test: loss=0.07109238288745336 acc=0.972463768115942\n",
      "EPOCH=1848\n",
      "train: loss=0.08946920208397922 acc=0.9695652173913043\n",
      "test: loss=0.06662708067871781 acc=0.9710144927536232\n",
      "EPOCH=1849\n",
      "train: loss=0.09050070069988991 acc=0.9695652173913043\n",
      "test: loss=0.07166823066626656 acc=0.9695652173913043\n",
      "EPOCH=1850\n",
      "train: loss=0.07728689083809351 acc=0.9753623188405797\n",
      "test: loss=0.06569932571254533 acc=0.9782608695652174\n",
      "EPOCH=1851\n",
      "train: loss=0.06565702283746364 acc=0.9768115942028985\n",
      "test: loss=0.06832783792291426 acc=0.9768115942028985\n",
      "EPOCH=1852\n",
      "train: loss=0.036452211044960686 acc=0.991304347826087\n",
      "test: loss=0.06920620757712716 acc=0.9739130434782609\n",
      "EPOCH=1853\n",
      "train: loss=0.06826822719260948 acc=0.9739130434782609\n",
      "test: loss=0.06581478207219733 acc=0.9753623188405797\n",
      "EPOCH=1854\n",
      "train: loss=0.04182982001428475 acc=0.9840579710144928\n",
      "test: loss=0.07069381590985256 acc=0.9753623188405797\n",
      "EPOCH=1855\n",
      "train: loss=0.05922497878136192 acc=0.9739130434782609\n",
      "test: loss=0.07336553486343567 acc=0.9739130434782609\n",
      "EPOCH=1856\n",
      "train: loss=0.08529084236177721 acc=0.9753623188405797\n",
      "test: loss=0.07049607544185114 acc=0.9739130434782609\n",
      "EPOCH=1857\n",
      "train: loss=0.04224693358713972 acc=0.9840579710144928\n",
      "test: loss=0.06687751396874198 acc=0.972463768115942\n",
      "EPOCH=1858\n",
      "train: loss=0.05934420591778508 acc=0.9768115942028985\n",
      "test: loss=0.06304214156316523 acc=0.9753623188405797\n",
      "EPOCH=1859\n",
      "train: loss=0.07134933307209226 acc=0.9739130434782609\n",
      "test: loss=0.06603723985551847 acc=0.9782608695652174\n",
      "EPOCH=1860\n",
      "train: loss=0.06608787729241565 acc=0.972463768115942\n",
      "test: loss=0.06507294667794895 acc=0.9797101449275363\n",
      "EPOCH=1861\n",
      "train: loss=0.07228988707857714 acc=0.9768115942028985\n",
      "test: loss=0.07495910707665195 acc=0.9768115942028985\n",
      "EPOCH=1862\n",
      "train: loss=0.040254305496762816 acc=0.9840579710144928\n",
      "test: loss=0.06851554402547846 acc=0.9753623188405797\n",
      "EPOCH=1863\n",
      "train: loss=0.0680628015057319 acc=0.9768115942028985\n",
      "test: loss=0.07206644347412691 acc=0.9782608695652174\n",
      "EPOCH=1864\n",
      "train: loss=0.08425015720713488 acc=0.9710144927536232\n",
      "test: loss=0.06795551194415678 acc=0.9695652173913043\n",
      "EPOCH=1865\n",
      "train: loss=0.06670259140982193 acc=0.9782608695652174\n",
      "test: loss=0.0654226515186454 acc=0.9739130434782609\n",
      "EPOCH=1866\n",
      "train: loss=0.06732592829857159 acc=0.9739130434782609\n",
      "test: loss=0.06561135286079814 acc=0.9797101449275363\n",
      "EPOCH=1867\n",
      "train: loss=0.08304359723756737 acc=0.9594202898550724\n",
      "test: loss=0.06907700273922703 acc=0.9782608695652174\n",
      "EPOCH=1868\n",
      "train: loss=0.053713562992741784 acc=0.9797101449275363\n",
      "test: loss=0.06086025305299922 acc=0.9768115942028985\n",
      "EPOCH=1869\n",
      "train: loss=0.0859743139436111 acc=0.9681159420289855\n",
      "test: loss=0.06469871670056906 acc=0.9797101449275363\n",
      "EPOCH=1870\n",
      "train: loss=0.08079587219451768 acc=0.9637681159420289\n",
      "test: loss=0.06671493400236432 acc=0.9782608695652174\n",
      "EPOCH=1871\n",
      "train: loss=0.0566826148402505 acc=0.981159420289855\n",
      "test: loss=0.06851777919932081 acc=0.972463768115942\n",
      "EPOCH=1872\n",
      "train: loss=0.08581094833605254 acc=0.9768115942028985\n",
      "test: loss=0.06427722475473802 acc=0.9797101449275363\n",
      "EPOCH=1873\n",
      "train: loss=0.06667767775023895 acc=0.9666666666666667\n",
      "test: loss=0.0636334679162283 acc=0.9753623188405797\n",
      "EPOCH=1874\n",
      "train: loss=0.05408548482399171 acc=0.9855072463768116\n",
      "test: loss=0.06276258797619275 acc=0.9782608695652174\n",
      "EPOCH=1875\n",
      "train: loss=0.073780581118783 acc=0.9753623188405797\n",
      "test: loss=0.06805705512789391 acc=0.9753623188405797\n",
      "EPOCH=1876\n",
      "train: loss=0.057297050302607906 acc=0.9768115942028985\n",
      "test: loss=0.06559793436990477 acc=0.9797101449275363\n",
      "EPOCH=1877\n",
      "train: loss=0.06302161544034464 acc=0.9710144927536232\n",
      "test: loss=0.06642040174717062 acc=0.9768115942028985\n",
      "EPOCH=1878\n",
      "train: loss=0.05190446161909285 acc=0.9797101449275363\n",
      "test: loss=0.06357927635780677 acc=0.9826086956521739\n",
      "EPOCH=1879\n",
      "train: loss=0.08757000264482374 acc=0.9652173913043478\n",
      "test: loss=0.0650588238720191 acc=0.9768115942028985\n",
      "EPOCH=1880\n",
      "train: loss=0.0674139955532802 acc=0.9739130434782609\n",
      "test: loss=0.06301043215759795 acc=0.9782608695652174\n",
      "EPOCH=1881\n",
      "train: loss=0.048630368390351 acc=0.9797101449275363\n",
      "test: loss=0.06329203709246733 acc=0.9753623188405797\n",
      "EPOCH=1882\n",
      "train: loss=0.07121163701722508 acc=0.981159420289855\n",
      "test: loss=0.06617588811684912 acc=0.9768115942028985\n",
      "EPOCH=1883\n",
      "train: loss=0.06895695174085438 acc=0.9753623188405797\n",
      "test: loss=0.06531374414302661 acc=0.9797101449275363\n",
      "EPOCH=1884\n",
      "train: loss=0.05590648003812942 acc=0.9797101449275363\n",
      "test: loss=0.06431445873355555 acc=0.981159420289855\n",
      "EPOCH=1885\n",
      "train: loss=0.05876960850397588 acc=0.9782608695652174\n",
      "test: loss=0.06824865262058312 acc=0.9768115942028985\n",
      "EPOCH=1886\n",
      "train: loss=0.07451164721065393 acc=0.9782608695652174\n",
      "test: loss=0.06378206700126872 acc=0.9753623188405797\n",
      "EPOCH=1887\n",
      "train: loss=0.05580059394680544 acc=0.9797101449275363\n",
      "test: loss=0.06678522469107784 acc=0.9797101449275363\n",
      "EPOCH=1888\n",
      "train: loss=0.03734871862303678 acc=0.9884057971014493\n",
      "test: loss=0.06321670472963058 acc=0.981159420289855\n",
      "EPOCH=1889\n",
      "train: loss=0.048898616153087285 acc=0.9826086956521739\n",
      "test: loss=0.06304184688313097 acc=0.9826086956521739\n",
      "EPOCH=1890\n",
      "train: loss=0.06283330195010668 acc=0.9681159420289855\n",
      "test: loss=0.06639090059194432 acc=0.9739130434782609\n",
      "EPOCH=1891\n",
      "train: loss=0.04207441828593191 acc=0.9855072463768116\n",
      "test: loss=0.0647288334055035 acc=0.9768115942028985\n",
      "EPOCH=1892\n",
      "train: loss=0.039752313176004285 acc=0.9855072463768116\n",
      "test: loss=0.06226751252539423 acc=0.9826086956521739\n",
      "EPOCH=1893\n",
      "train: loss=0.07491977923632899 acc=0.9782608695652174\n",
      "test: loss=0.06397679194919834 acc=0.9797101449275363\n",
      "EPOCH=1894\n",
      "train: loss=0.06440190126593769 acc=0.9652173913043478\n",
      "test: loss=0.06327473860810139 acc=0.9782608695652174\n",
      "EPOCH=1895\n",
      "train: loss=0.07391199910013 acc=0.9797101449275363\n",
      "test: loss=0.0617613090401332 acc=0.9753623188405797\n",
      "EPOCH=1896\n",
      "train: loss=0.0654560924816463 acc=0.981159420289855\n",
      "test: loss=0.06589052457525031 acc=0.9797101449275363\n",
      "EPOCH=1897\n",
      "train: loss=0.05808515316872797 acc=0.9753623188405797\n",
      "test: loss=0.06390266998147189 acc=0.9768115942028985\n",
      "EPOCH=1898\n",
      "train: loss=0.09910348079346266 acc=0.9652173913043478\n",
      "test: loss=0.06470178361626948 acc=0.9710144927536232\n",
      "EPOCH=1899\n",
      "train: loss=0.06641114042338353 acc=0.9710144927536232\n",
      "test: loss=0.06658690948625133 acc=0.9753623188405797\n",
      "EPOCH=1900\n",
      "train: loss=0.07427926844678745 acc=0.9753623188405797\n",
      "test: loss=0.06345149219741272 acc=0.9768115942028985\n",
      "EPOCH=1901\n",
      "train: loss=0.06006050332444897 acc=0.9753623188405797\n",
      "test: loss=0.06825934091563587 acc=0.9753623188405797\n",
      "EPOCH=1902\n",
      "train: loss=0.055953364816560026 acc=0.9797101449275363\n",
      "test: loss=0.06203415273994993 acc=0.9797101449275363\n",
      "EPOCH=1903\n",
      "train: loss=0.04097220596557228 acc=0.9869565217391304\n",
      "test: loss=0.06264836982738527 acc=0.9768115942028985\n",
      "EPOCH=1904\n",
      "train: loss=0.06991607463803708 acc=0.9695652173913043\n",
      "test: loss=0.06280693874457856 acc=0.9797101449275363\n",
      "EPOCH=1905\n",
      "train: loss=0.069742822478707 acc=0.9652173913043478\n",
      "test: loss=0.06340460140676941 acc=0.9782608695652174\n",
      "EPOCH=1906\n",
      "train: loss=0.054348685996170196 acc=0.981159420289855\n",
      "test: loss=0.06341351751838609 acc=0.9782608695652174\n",
      "EPOCH=1907\n",
      "train: loss=0.04967117897613793 acc=0.9840579710144928\n",
      "test: loss=0.06641177587632278 acc=0.972463768115942\n",
      "EPOCH=1908\n",
      "train: loss=0.07347726617277278 acc=0.9695652173913043\n",
      "test: loss=0.06311930337062197 acc=0.9768115942028985\n",
      "EPOCH=1909\n",
      "train: loss=0.06196842995908048 acc=0.9681159420289855\n",
      "test: loss=0.06355842536147532 acc=0.9797101449275363\n",
      "EPOCH=1910\n",
      "train: loss=0.06583583053779815 acc=0.9782608695652174\n",
      "test: loss=0.06313495404883783 acc=0.981159420289855\n",
      "EPOCH=1911\n",
      "train: loss=0.0482801803680831 acc=0.9869565217391304\n",
      "test: loss=0.06313116619352864 acc=0.981159420289855\n",
      "EPOCH=1912\n",
      "train: loss=0.04893154746592125 acc=0.9826086956521739\n",
      "test: loss=0.059945192589258796 acc=0.9739130434782609\n",
      "EPOCH=1913\n",
      "train: loss=0.06938955624951122 acc=0.9782608695652174\n",
      "test: loss=0.05807426277323662 acc=0.9782608695652174\n",
      "EPOCH=1914\n",
      "train: loss=0.05646957895166343 acc=0.9739130434782609\n",
      "test: loss=0.06461484931828923 acc=0.981159420289855\n",
      "EPOCH=1915\n",
      "train: loss=0.06722859706809532 acc=0.9768115942028985\n",
      "test: loss=0.061381644376149305 acc=0.9782608695652174\n",
      "EPOCH=1916\n",
      "train: loss=0.07920877434003752 acc=0.9739130434782609\n",
      "test: loss=0.060203031859365964 acc=0.981159420289855\n",
      "EPOCH=1917\n",
      "train: loss=0.0580015608826594 acc=0.9768115942028985\n",
      "test: loss=0.06129733200545916 acc=0.9739130434782609\n",
      "EPOCH=1918\n",
      "train: loss=0.04608851491629702 acc=0.9840579710144928\n",
      "test: loss=0.06301374525659226 acc=0.9739130434782609\n",
      "EPOCH=1919\n",
      "train: loss=0.0762465949503248 acc=0.9666666666666667\n",
      "test: loss=0.05902245472824741 acc=0.9797101449275363\n",
      "EPOCH=1920\n",
      "train: loss=0.07288605225291246 acc=0.9782608695652174\n",
      "test: loss=0.05912515981808759 acc=0.9753623188405797\n",
      "EPOCH=1921\n",
      "train: loss=0.04890169344028796 acc=0.9855072463768116\n",
      "test: loss=0.06104398986421114 acc=0.9797101449275363\n",
      "EPOCH=1922\n",
      "train: loss=0.06650792941948486 acc=0.9681159420289855\n",
      "test: loss=0.06002672250258448 acc=0.9782608695652174\n",
      "EPOCH=1923\n",
      "train: loss=0.04782892282144685 acc=0.981159420289855\n",
      "test: loss=0.063091327435746 acc=0.9797101449275363\n",
      "EPOCH=1924\n",
      "train: loss=0.0693658166602073 acc=0.9768115942028985\n",
      "test: loss=0.05867405164728067 acc=0.9782608695652174\n",
      "EPOCH=1925\n",
      "train: loss=0.07451751686594084 acc=0.9666666666666667\n",
      "test: loss=0.06122892113685279 acc=0.9797101449275363\n",
      "EPOCH=1926\n",
      "train: loss=0.04680080865840908 acc=0.9826086956521739\n",
      "test: loss=0.06112459149805996 acc=0.9768115942028985\n",
      "EPOCH=1927\n",
      "train: loss=0.06341190316998142 acc=0.9768115942028985\n",
      "test: loss=0.06451162674225923 acc=0.9797101449275363\n",
      "EPOCH=1928\n",
      "train: loss=0.06555017945624732 acc=0.9782608695652174\n",
      "test: loss=0.06044272321404958 acc=0.9768115942028985\n",
      "EPOCH=1929\n",
      "train: loss=0.05126901676700726 acc=0.9797101449275363\n",
      "test: loss=0.06030539150073905 acc=0.981159420289855\n",
      "EPOCH=1930\n",
      "train: loss=0.06788582889335744 acc=0.9681159420289855\n",
      "test: loss=0.06030599882937154 acc=0.981159420289855\n",
      "EPOCH=1931\n",
      "train: loss=0.0779718895647106 acc=0.9681159420289855\n",
      "test: loss=0.061184230652850426 acc=0.9782608695652174\n",
      "EPOCH=1932\n",
      "train: loss=0.06113716174373145 acc=0.9753623188405797\n",
      "test: loss=0.05840160939908139 acc=0.9782608695652174\n",
      "EPOCH=1933\n",
      "train: loss=0.04699545716327878 acc=0.981159420289855\n",
      "test: loss=0.05973707309366829 acc=0.981159420289855\n",
      "EPOCH=1934\n",
      "train: loss=0.07400780234808166 acc=0.972463768115942\n",
      "test: loss=0.06167539047570898 acc=0.9782608695652174\n",
      "EPOCH=1935\n",
      "train: loss=0.07733869954610467 acc=0.9782608695652174\n",
      "test: loss=0.06092974778409947 acc=0.9826086956521739\n",
      "EPOCH=1936\n",
      "train: loss=0.0880514083984068 acc=0.9739130434782609\n",
      "test: loss=0.06435491907997547 acc=0.9753623188405797\n",
      "EPOCH=1937\n",
      "train: loss=0.08778795852770312 acc=0.9623188405797102\n",
      "test: loss=0.06195149069449492 acc=0.9826086956521739\n",
      "EPOCH=1938\n",
      "train: loss=0.06052980760412156 acc=0.9753623188405797\n",
      "test: loss=0.06288083265747643 acc=0.9768115942028985\n",
      "EPOCH=1939\n",
      "train: loss=0.047607475238528915 acc=0.981159420289855\n",
      "test: loss=0.06270920334323027 acc=0.9884057971014493\n",
      "EPOCH=1940\n",
      "train: loss=0.04445803108664484 acc=0.9855072463768116\n",
      "test: loss=0.06142960813199149 acc=0.9797101449275363\n",
      "EPOCH=1941\n",
      "train: loss=0.07150191352805485 acc=0.972463768115942\n",
      "test: loss=0.059456215071732266 acc=0.9782608695652174\n",
      "EPOCH=1942\n",
      "train: loss=0.07737923488183403 acc=0.9637681159420289\n",
      "test: loss=0.061519127718146306 acc=0.9797101449275363\n",
      "EPOCH=1943\n",
      "train: loss=0.06548431524104116 acc=0.9695652173913043\n",
      "test: loss=0.06054852547153188 acc=0.9826086956521739\n",
      "EPOCH=1944\n",
      "train: loss=0.07689628340482384 acc=0.9739130434782609\n",
      "test: loss=0.0606494279486844 acc=0.9768115942028985\n",
      "EPOCH=1945\n",
      "train: loss=0.06170318734593792 acc=0.9695652173913043\n",
      "test: loss=0.06274732136949678 acc=0.9797101449275363\n",
      "EPOCH=1946\n",
      "train: loss=0.07486066459774887 acc=0.9782608695652174\n",
      "test: loss=0.06039594042824469 acc=0.9782608695652174\n",
      "EPOCH=1947\n",
      "train: loss=0.06703449085629516 acc=0.9695652173913043\n",
      "test: loss=0.05883556241977687 acc=0.9797101449275363\n",
      "EPOCH=1948\n",
      "train: loss=0.04963724608387448 acc=0.9826086956521739\n",
      "test: loss=0.060223660672966035 acc=0.9782608695652174\n",
      "EPOCH=1949\n",
      "train: loss=0.05764935835773626 acc=0.9826086956521739\n",
      "test: loss=0.06078735930305869 acc=0.981159420289855\n",
      "EPOCH=1950\n",
      "train: loss=0.04224211700926783 acc=0.9826086956521739\n",
      "test: loss=0.05960176229030708 acc=0.981159420289855\n",
      "EPOCH=1951\n",
      "train: loss=0.058939027140734575 acc=0.9753623188405797\n",
      "test: loss=0.0627839810114388 acc=0.981159420289855\n",
      "EPOCH=1952\n",
      "train: loss=0.0879025114398733 acc=0.9681159420289855\n",
      "test: loss=0.06193189807784788 acc=0.9826086956521739\n",
      "EPOCH=1953\n",
      "train: loss=0.03481448602782209 acc=0.9826086956521739\n",
      "test: loss=0.057415815622613844 acc=0.9826086956521739\n",
      "EPOCH=1954\n",
      "train: loss=0.0540627898377312 acc=0.9753623188405797\n",
      "test: loss=0.059729355720477294 acc=0.9797101449275363\n",
      "EPOCH=1955\n",
      "train: loss=0.0519873305370307 acc=0.9768115942028985\n",
      "test: loss=0.05726318228072926 acc=0.9782608695652174\n",
      "EPOCH=1956\n",
      "train: loss=0.060136956410988374 acc=0.9782608695652174\n",
      "test: loss=0.06065286264941341 acc=0.9826086956521739\n",
      "EPOCH=1957\n",
      "train: loss=0.06024646512397641 acc=0.9710144927536232\n",
      "test: loss=0.05949911795812806 acc=0.981159420289855\n",
      "EPOCH=1958\n",
      "train: loss=0.06616155879289584 acc=0.9652173913043478\n",
      "test: loss=0.059021612076087644 acc=0.9826086956521739\n",
      "EPOCH=1959\n",
      "train: loss=0.027171041678859045 acc=0.9855072463768116\n",
      "test: loss=0.06197152578259819 acc=0.981159420289855\n",
      "EPOCH=1960\n",
      "train: loss=0.06076477480507754 acc=0.9782608695652174\n",
      "test: loss=0.06347416670549717 acc=0.9768115942028985\n",
      "EPOCH=1961\n",
      "train: loss=0.06495177981793232 acc=0.9753623188405797\n",
      "test: loss=0.061125314520745676 acc=0.981159420289855\n",
      "EPOCH=1962\n",
      "train: loss=0.08694123158346381 acc=0.9666666666666667\n",
      "test: loss=0.057630304631671535 acc=0.9826086956521739\n",
      "EPOCH=1963\n",
      "train: loss=0.053820273484154184 acc=0.9797101449275363\n",
      "test: loss=0.05947122705109548 acc=0.9797101449275363\n",
      "EPOCH=1964\n",
      "train: loss=0.0585195919420973 acc=0.9782608695652174\n",
      "test: loss=0.05872991828134112 acc=0.9797101449275363\n",
      "EPOCH=1965\n",
      "train: loss=0.059865021363817265 acc=0.9826086956521739\n",
      "test: loss=0.06448263311471117 acc=0.9753623188405797\n",
      "EPOCH=1966\n",
      "train: loss=0.06408232859168382 acc=0.9681159420289855\n",
      "test: loss=0.05993762531289848 acc=0.9782608695652174\n",
      "EPOCH=1967\n",
      "train: loss=0.055843973820618675 acc=0.9768115942028985\n",
      "test: loss=0.05924747803457356 acc=0.9782608695652174\n",
      "EPOCH=1968\n",
      "train: loss=0.06055763817092516 acc=0.9768115942028985\n",
      "test: loss=0.06643932770900855 acc=0.9753623188405797\n",
      "EPOCH=1969\n",
      "train: loss=0.04629971945703866 acc=0.9869565217391304\n",
      "test: loss=0.062348192153435116 acc=0.9753623188405797\n",
      "EPOCH=1970\n",
      "train: loss=0.059675955052298564 acc=0.9768115942028985\n",
      "test: loss=0.06156424864910074 acc=0.9782608695652174\n",
      "EPOCH=1971\n",
      "train: loss=0.042032335059528504 acc=0.9855072463768116\n",
      "test: loss=0.06125127992078444 acc=0.9782608695652174\n",
      "EPOCH=1972\n",
      "train: loss=0.06777682187447807 acc=0.972463768115942\n",
      "test: loss=0.061236022711941555 acc=0.9840579710144928\n",
      "EPOCH=1973\n",
      "train: loss=0.06206871771752897 acc=0.972463768115942\n",
      "test: loss=0.059544682341770086 acc=0.9753623188405797\n",
      "EPOCH=1974\n",
      "train: loss=0.06414061709940268 acc=0.9695652173913043\n",
      "test: loss=0.06092668647195117 acc=0.981159420289855\n",
      "EPOCH=1975\n",
      "train: loss=0.09818381441753571 acc=0.9666666666666667\n",
      "test: loss=0.060116465857122096 acc=0.9855072463768116\n",
      "EPOCH=1976\n",
      "train: loss=0.07582820346138697 acc=0.972463768115942\n",
      "test: loss=0.06127254710881264 acc=0.9797101449275363\n",
      "EPOCH=1977\n",
      "train: loss=0.07578117330893959 acc=0.9681159420289855\n",
      "test: loss=0.062238378686998795 acc=0.9768115942028985\n",
      "EPOCH=1978\n",
      "train: loss=0.06192857836851615 acc=0.972463768115942\n",
      "test: loss=0.05846610647949685 acc=0.981159420289855\n",
      "EPOCH=1979\n",
      "train: loss=0.06330571763133633 acc=0.9782608695652174\n",
      "test: loss=0.05817961807166587 acc=0.9826086956521739\n",
      "EPOCH=1980\n",
      "train: loss=0.0780692588901271 acc=0.9695652173913043\n",
      "test: loss=0.061937154224010665 acc=0.9768115942028985\n",
      "EPOCH=1981\n",
      "train: loss=0.04184312985057807 acc=0.9782608695652174\n",
      "test: loss=0.06001358214317397 acc=0.9797101449275363\n",
      "EPOCH=1982\n",
      "train: loss=0.06280800621640699 acc=0.9695652173913043\n",
      "test: loss=0.05937831320683921 acc=0.9797101449275363\n",
      "EPOCH=1983\n",
      "train: loss=0.07791834285901045 acc=0.9681159420289855\n",
      "test: loss=0.05805456296647612 acc=0.9826086956521739\n",
      "EPOCH=1984\n",
      "train: loss=0.06702972850139655 acc=0.9681159420289855\n",
      "test: loss=0.05932514373376875 acc=0.972463768115942\n",
      "EPOCH=1985\n",
      "train: loss=0.09036469724568232 acc=0.9681159420289855\n",
      "test: loss=0.059526091725155154 acc=0.9797101449275363\n",
      "EPOCH=1986\n",
      "train: loss=0.058117841513145765 acc=0.972463768115942\n",
      "test: loss=0.057886163463165934 acc=0.981159420289855\n",
      "EPOCH=1987\n",
      "train: loss=0.04419864692194291 acc=0.9797101449275363\n",
      "test: loss=0.05696179685364821 acc=0.9826086956521739\n",
      "EPOCH=1988\n",
      "train: loss=0.05908351413800206 acc=0.981159420289855\n",
      "test: loss=0.05931915073603073 acc=0.981159420289855\n",
      "EPOCH=1989\n",
      "train: loss=0.07793101419559621 acc=0.9739130434782609\n",
      "test: loss=0.05898967185247188 acc=0.9797101449275363\n",
      "EPOCH=1990\n",
      "train: loss=0.04664076914146628 acc=0.9840579710144928\n",
      "test: loss=0.05764493797056887 acc=0.9826086956521739\n",
      "EPOCH=1991\n",
      "train: loss=0.04988635733242592 acc=0.9797101449275363\n",
      "test: loss=0.06041236548827019 acc=0.9797101449275363\n",
      "EPOCH=1992\n",
      "train: loss=0.047559362819930326 acc=0.9884057971014493\n",
      "test: loss=0.0600364409638564 acc=0.9797101449275363\n",
      "EPOCH=1993\n",
      "train: loss=0.0632617130049958 acc=0.9782608695652174\n",
      "test: loss=0.06014747492310226 acc=0.9826086956521739\n",
      "EPOCH=1994\n",
      "train: loss=0.11879101317256914 acc=0.9681159420289855\n",
      "test: loss=0.05599218740307554 acc=0.9782608695652174\n",
      "EPOCH=1995\n",
      "train: loss=0.04678361772702467 acc=0.9797101449275363\n",
      "test: loss=0.05852684724364855 acc=0.9797101449275363\n",
      "EPOCH=1996\n",
      "train: loss=0.058474916656179446 acc=0.9840579710144928\n",
      "test: loss=0.05617900030548609 acc=0.9826086956521739\n",
      "EPOCH=1997\n",
      "train: loss=0.0639749156626136 acc=0.9695652173913043\n",
      "test: loss=0.05634743444631423 acc=0.981159420289855\n",
      "EPOCH=1998\n",
      "train: loss=0.041436150020561266 acc=0.9826086956521739\n",
      "test: loss=0.059540679264404234 acc=0.9782608695652174\n",
      "EPOCH=1999\n",
      "train: loss=0.08181639538753158 acc=0.9753623188405797\n",
      "test: loss=0.05851589433939122 acc=0.981159420289855\n",
      "EPOCH=2000\n",
      "train: loss=0.050811385563838844 acc=0.9753623188405797\n",
      "test: loss=0.05611937210408298 acc=0.981159420289855\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "metrics_names = ['loss_train','loss_test','acc_train','acc_test']\n",
    "losses = []\n",
    "\n",
    "net.to(DEVICE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'EPOCH={epoch + 1}')\n",
    "    for X, y in train_dl:\n",
    "        X = X.float().to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = net(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "    train_ll, train_acc = evaluate_model(net, train_dl)\n",
    "    test_ll, test_acc = evaluate_model(net, test_dl)\n",
    "    \n",
    "    \n",
    "    print(f'train: loss={train_ll} acc={train_acc}')\n",
    "    print(f'test: loss={test_ll} acc={test_acc}')\n",
    "          \n",
    "    metrics.append([train_ll, test_ll, train_acc, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.DataFrame(metrics[0:250], columns=['train-loss','test-loss','train-acc','test-acc']).plot(subplots=False, figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './model/sequence_lstm_classifier.pt')\n",
    "\n",
    "# m_state_dict = torch.load('mymodule.pt')\n",
    "# new_m = MyModule()\n",
    "# new_m.load_state_dict(m_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr-dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
