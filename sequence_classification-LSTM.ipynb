{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "from utils.data_augmentation import rotate_keypoints_sequence\n",
    "from utils.keypoints import read_keypoints, rescale_keypoints, keypoints_sequence_padding\n",
    "\n",
    "from model.datasets import SequenceKeypointsDataset\n",
    "from model.transforms import RotateKeypointsSequence, KeypointsSequencePadding\n",
    "from model.models import SequenceRecognitionNetLSTM\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILES_PATH = [\n",
    "    './data/labels/sequence/sequence_labelsSep-16-2020_0516.json',\n",
    "    './data/labels/sequence/sequence_labelsSep-16-2020_1930.json',\n",
    "    './data/labels/sequence/sequence_labelsSep-18-2020_0447.json',\n",
    "    './data/labels/sequence/sequence_labelsSep-18-2020_2351.json'\n",
    "]\n",
    "JSON_KEYPOINTS_BASE_PATH = './data/keypoints'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "N_EPOCHS = 2000\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.00001\n",
    "\n",
    "LABEL_OCCURENCES_MIN = 10\n",
    "SEQUENCE_LENGTH_MAX = 50\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "label_map = {\n",
    "    'Meet': 0,\n",
    "    'Name': 1,\n",
    "    'Good day': 2,\n",
    "    'See you around': 3,\n",
    "    'Thank you': 4,\n",
    "    'Hello': 5,\n",
    "    'Bye bye': 6,\n",
    "    'Tom': 7,\n",
    "    'Nice': 8,\n",
    "    'You': 9,\n",
    "    'My': 10\n",
    "} \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>keypoints_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/rendered_video/internal_resource_fps10/...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>[/home/tom/Desktop/projects/sign-language/data...</td>\n",
       "      <td>Hello</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_path  \\\n",
       "0  ./data/rendered_video/internal_resource_fps10/...   \n",
       "1  ./data/rendered_video/internal_resource_fps10/...   \n",
       "2  ./data/rendered_video/internal_resource_fps10/...   \n",
       "3  ./data/rendered_video/internal_resource_fps10/...   \n",
       "4  ./data/rendered_video/internal_resource_fps10/...   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "1  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "2  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "3  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "4  [/home/tom/Desktop/projects/sign-language/data...   \n",
       "\n",
       "                                      keypoints_path  label  label_id  \n",
       "0  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "1  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "2  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "3  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  \n",
       "4  [/home/tom/Desktop/projects/sign-language/data...  Hello         5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack label files\n",
    "labels_json = []\n",
    "\n",
    "for label_file in JSON_FILES_PATH:\n",
    "    with open(label_file, 'r') as f:\n",
    "        for row in f.readlines():\n",
    "            labels_json.append(json.loads(row))\n",
    "\n",
    "            \n",
    "# Fix labels\n",
    "            \n",
    "# Convert into DataFrame\n",
    "df = pd.concat(\n",
    "    [pd.DataFrame(item) for item in labels_json],\n",
    "    axis=0\n",
    ").drop(columns=['id'])\n",
    "\n",
    "# Drop ignore labels\n",
    "df = df[~df.label.isin(['ignore', '<random movements>', 'My name'])]\n",
    "\n",
    "# Drop low freq classes\n",
    "labels_freq = df.label.value_counts()\n",
    "low_freq_labels = labels_freq[(labels_freq < LABEL_OCCURENCES_MIN)].index.tolist()\n",
    "df = df[~(df.label.isin(low_freq_labels))]\n",
    "\n",
    "df = df[df.label.isin(list(label_map.keys()))]\n",
    "\n",
    "# Get label ID\n",
    "df['label_id'] = df.label.map(label_map)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(863, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You               109\n",
      "My                102\n",
      "Name               88\n",
      "Meet               87\n",
      "Nice               79\n",
      "Tom                78\n",
      "Hello              74\n",
      "Good day           72\n",
      "Thank you          65\n",
      "See you around     65\n",
      "Bye bye            44\n",
      "Name: label, dtype: int64\n",
      "9     109\n",
      "10    102\n",
      "1      88\n",
      "0      87\n",
      "8      79\n",
      "7      78\n",
      "5      74\n",
      "2      72\n",
      "4      65\n",
      "3      65\n",
      "6      44\n",
      "Name: label_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.label.value_counts())\n",
    "print(df.label_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(df_train):\n",
    "    class_sample_count = np.array([len(np.where(df_train.label_id==t)[0]) for t in np.unique(df_train.label_id)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in df_train.label_id])\n",
    "\n",
    "    return torch.from_numpy(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Create Transformer\n",
    "keypoints_sequence_transform_train = transforms.Compose([\n",
    "    RotateKeypointsSequence(-30, 30),\n",
    "    KeypointsSequencePadding(SEQUENCE_LENGTH_MAX)\n",
    "])\n",
    "keypoints_sequence_transform_test = transforms.Compose([\n",
    "    KeypointsSequencePadding(SEQUENCE_LENGTH_MAX)\n",
    "])\n",
    "\n",
    "# Initialize Datasets\n",
    "ds_train = SequenceKeypointsDataset(df_train, keypoints_sequence_transform_train)\n",
    "ds_test = SequenceKeypointsDataset(df_test, keypoints_sequence_transform_test)\n",
    "\n",
    "# Initialize Sampler\n",
    "sampler = torch.utils.data.WeightedRandomSampler(get_weights(df_train), df_train.shape[0])\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_dl = torch.utils.data.DataLoader(ds_train, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "test_dl = torch.utils.data.DataLoader(ds_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Model params\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = SequenceRecognitionNetLSTM(\n",
    "    out_class_num=NUM_CLASSES, \n",
    "    input_size=5700,\n",
    "    hidden_size=1024,\n",
    "    num_layers=8, \n",
    "    dropout=0\n",
    ").to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "You               87\n",
       "My                83\n",
       "Name              70\n",
       "Meet              68\n",
       "Tom               66\n",
       "Nice              65\n",
       "Hello             59\n",
       "See you around    56\n",
       "Thank you         54\n",
       "Good day          51\n",
       "Bye bye           31\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "You               22\n",
       "Good day          21\n",
       "My                19\n",
       "Meet              19\n",
       "Name              18\n",
       "Hello             15\n",
       "Nice              14\n",
       "Bye bye           13\n",
       "Tom               12\n",
       "Thank you         11\n",
       "See you around     9\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, data_loader, print_info=False):\n",
    "    model.eval()\n",
    "    collect_results = []\n",
    "    collect_targets = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        X, y = batch\n",
    "        X = X.float().to(DEVICE)\n",
    "        y = y.float().to(DEVICE).detach().cpu().numpy()\n",
    "\n",
    "        pred = model(X)\n",
    "        pred = F.softmax(pred, dim=0)\n",
    "        collect_results.append(pred.detach().cpu().numpy())\n",
    "        collect_targets.append(y)\n",
    "\n",
    "    preds_proba = np.concatenate(collect_results)\n",
    "    preds = preds_proba.argmax(axis=1)\n",
    "    targets = np.concatenate(collect_targets)\n",
    "\n",
    "    ll = log_loss(targets, preds_proba)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "\n",
    "    \n",
    "    if print_info:\n",
    "        print(\"test log-loss: {}\".format(ll))\n",
    "        print(\"overall accuracy:  {}\".format(acc))\n",
    "        \n",
    "    model.train()\n",
    "        \n",
    "    return ll, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH=1\n",
      "train: loss=2.397895336151123 acc=0.08985507246376812\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=2\n",
      "train: loss=2.397895336151123 acc=0.09565217391304348\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=3\n",
      "train: loss=2.397895336151123 acc=0.0782608695652174\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=4\n",
      "train: loss=2.397895336151123 acc=0.09130434782608696\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=5\n",
      "train: loss=2.397895336151123 acc=0.09565217391304348\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=6\n",
      "train: loss=2.397895336151123 acc=0.07681159420289856\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=7\n",
      "train: loss=2.397895336151123 acc=0.08405797101449275\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=8\n",
      "train: loss=2.397895336151123 acc=0.08405797101449275\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=9\n",
      "train: loss=2.397895336151123 acc=0.08840579710144927\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=10\n",
      "train: loss=2.397895336151123 acc=0.11739130434782609\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=11\n",
      "train: loss=2.397895336151123 acc=0.09420289855072464\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=12\n",
      "train: loss=2.397895336151123 acc=0.07681159420289856\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=13\n",
      "train: loss=2.397895336151123 acc=0.09130434782608696\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=14\n",
      "train: loss=2.397895336151123 acc=0.09855072463768116\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=15\n",
      "train: loss=2.397895335805589 acc=0.08405797101449275\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=16\n",
      "train: loss=2.397895336151123 acc=0.10434782608695652\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=17\n",
      "train: loss=2.397895336151123 acc=0.08405797101449275\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=18\n",
      "train: loss=2.397895336151123 acc=0.07971014492753623\n",
      "test: loss=2.397895336151123 acc=0.09855072463768116\n",
      "EPOCH=19\n",
      "train: loss=2.397895336151123 acc=0.08405797101449275\n",
      "test: loss=2.397895334423452 acc=0.10579710144927536\n",
      "EPOCH=20\n",
      "train: loss=2.3978953351145207 acc=0.10289855072463767\n",
      "test: loss=2.3978953354600545 acc=0.09855072463768116\n",
      "EPOCH=21\n",
      "train: loss=2.397895334077918 acc=0.09130434782608696\n",
      "test: loss=2.3978953313136446 acc=0.1144927536231884\n",
      "EPOCH=22\n",
      "train: loss=2.397895332350247 acc=0.11594202898550725\n",
      "test: loss=2.3978953330413155 acc=0.12028985507246377\n",
      "EPOCH=23\n",
      "train: loss=2.3978953257850977 acc=0.1246376811594203\n",
      "test: loss=2.3978953285493714 acc=0.1391304347826087\n",
      "EPOCH=24\n",
      "train: loss=2.3978953250940296 acc=0.11594202898550725\n",
      "test: loss=2.397895321984222 acc=0.15217391304347827\n",
      "EPOCH=25\n",
      "train: loss=2.3978953268217005 acc=0.11884057971014493\n",
      "test: loss=2.397895325439564 acc=0.14492753623188406\n",
      "EPOCH=26\n",
      "train: loss=2.3978953257850977 acc=0.12028985507246377\n",
      "test: loss=2.3978953123092652 acc=0.18115942028985507\n",
      "EPOCH=27\n",
      "train: loss=2.3978953171467436 acc=0.14347826086956522\n",
      "test: loss=2.3978953154190727 acc=0.16666666666666666\n",
      "EPOCH=28\n",
      "train: loss=2.3978953015977056 acc=0.17246376811594202\n",
      "test: loss=2.397895299870035 acc=0.19420289855072465\n",
      "EPOCH=29\n",
      "train: loss=2.3978953029798427 acc=0.17101449275362318\n",
      "test: loss=2.397895302288774 acc=0.19420289855072465\n",
      "EPOCH=30\n",
      "train: loss=2.3978952850120656 acc=0.1927536231884058\n",
      "test: loss=2.3978952912316807 acc=0.19855072463768117\n",
      "EPOCH=31\n",
      "train: loss=2.397895288121873 acc=0.2028985507246377\n",
      "test: loss=2.3978952870852703 acc=0.22028985507246376\n",
      "EPOCH=32\n",
      "train: loss=2.3978952791379844 acc=0.2463768115942029\n",
      "test: loss=2.397895284320997 acc=0.2289855072463768\n",
      "EPOCH=33\n",
      "train: loss=2.3978952580604 acc=0.2246376811594203\n",
      "test: loss=2.3978952511497167 acc=0.2391304347826087\n",
      "EPOCH=34\n",
      "train: loss=2.397895236291747 acc=0.3115942028985507\n",
      "test: loss=2.397895227307859 acc=0.2753623188405797\n",
      "EPOCH=35\n",
      "train: loss=2.3978952017383297 acc=0.33043478260869563\n",
      "test: loss=2.397895215559697 acc=0.2811594202898551\n",
      "EPOCH=36\n",
      "train: loss=2.397895217632902 acc=0.30289855072463767\n",
      "test: loss=2.3978952172873678 acc=0.2971014492753623\n",
      "EPOCH=37\n",
      "train: loss=2.397895187916963 acc=0.30144927536231886\n",
      "test: loss=2.397895182042882 acc=0.26521739130434785\n",
      "EPOCH=38\n",
      "train: loss=2.397895173750062 acc=0.30289855072463767\n",
      "test: loss=2.397895160274229 acc=0.2971014492753623\n",
      "EPOCH=39\n",
      "train: loss=2.3978951616563657 acc=0.25217391304347825\n",
      "test: loss=2.3978951492171356 acc=0.2971014492753623\n",
      "EPOCH=40\n",
      "train: loss=2.3978951464528624 acc=0.3347826086956522\n",
      "test: loss=2.3978951388511103 acc=0.3318840579710145\n",
      "EPOCH=41\n",
      "train: loss=2.3978951329770295 acc=0.30869565217391304\n",
      "test: loss=2.397895128139551 acc=0.3072463768115942\n",
      "EPOCH=42\n",
      "train: loss=2.3978951226110046 acc=0.30434782608695654\n",
      "test: loss=2.3978951132815816 acc=0.3115942028985507\n",
      "EPOCH=43\n",
      "train: loss=2.3978951105173083 acc=0.2942028985507246\n",
      "test: loss=2.3978950963504073 acc=0.32318840579710145\n",
      "EPOCH=44\n",
      "train: loss=2.3978950846022453 acc=0.3391304347826087\n",
      "test: loss=2.397895079073699 acc=0.32463768115942027\n",
      "EPOCH=45\n",
      "train: loss=2.397895071817481 acc=0.2608695652173913\n",
      "test: loss=2.3978950742362204 acc=0.3\n",
      "EPOCH=46\n",
      "train: loss=2.3978950742362204 acc=0.29130434782608694\n",
      "test: loss=2.397895060069319 acc=0.2971014492753623\n",
      "EPOCH=47\n",
      "train: loss=2.3978950365729954 acc=0.23043478260869565\n",
      "test: loss=2.397895043829213 acc=0.21159420289855072\n",
      "EPOCH=48\n",
      "train: loss=2.3978950203328893 acc=0.19710144927536233\n",
      "test: loss=2.3978950089302615 acc=0.2072463768115942\n",
      "EPOCH=49\n",
      "train: loss=2.397894994072292 acc=0.1927536231884058\n",
      "test: loss=2.3978949667750924 acc=0.19130434782608696\n",
      "EPOCH=50\n",
      "train: loss=2.397894962974217 acc=0.2318840579710145\n",
      "test: loss=2.397894939477893 acc=0.18985507246376812\n",
      "EPOCH=51\n",
      "train: loss=2.39789487797281 acc=0.16521739130434782\n",
      "test: loss=2.3978948758996053 acc=0.1855072463768116\n",
      "EPOCH=52\n",
      "train: loss=2.3978947801866393 acc=0.1927536231884058\n",
      "test: loss=2.3978947642920674 acc=0.1855072463768116\n",
      "EPOCH=53\n",
      "train: loss=2.397894643700641 acc=0.16521739130434782\n",
      "test: loss=2.3978946170945097 acc=0.1782608695652174\n",
      "EPOCH=54\n",
      "train: loss=2.397894383167875 acc=0.1782608695652174\n",
      "test: loss=2.3978943600170854 acc=0.1782608695652174\n",
      "EPOCH=55\n",
      "train: loss=2.3978941869044648 acc=0.1753623188405797\n",
      "test: loss=2.397894151659979 acc=0.17971014492753623\n",
      "EPOCH=56\n",
      "train: loss=2.397893676550492 acc=0.16956521739130434\n",
      "test: loss=2.3978935290074004 acc=0.17101449275362318\n",
      "EPOCH=57\n",
      "train: loss=2.3978926126507747 acc=0.17391304347826086\n",
      "test: loss=2.397892568076866 acc=0.17391304347826086\n",
      "EPOCH=58\n",
      "train: loss=2.3978905004003774 acc=0.17971014492753623\n",
      "test: loss=2.397890597841014 acc=0.1753623188405797\n",
      "EPOCH=59\n",
      "train: loss=2.397887120385101 acc=0.18115942028985507\n",
      "test: loss=2.3978863650473996 acc=0.1782608695652174\n",
      "EPOCH=60\n",
      "train: loss=2.3978759561759837 acc=0.16376811594202897\n",
      "test: loss=2.3978735612786335 acc=0.17681159420289855\n",
      "EPOCH=61\n",
      "train: loss=2.397825705141261 acc=0.18985507246376812\n",
      "test: loss=2.397830022590748 acc=0.1782608695652174\n",
      "EPOCH=62\n",
      "train: loss=2.3974804242451984 acc=0.18840579710144928\n",
      "test: loss=2.3974830990252287 acc=0.1782608695652174\n",
      "EPOCH=63\n",
      "train: loss=2.3933559628500456 acc=0.19855072463768117\n",
      "test: loss=2.3933802811995797 acc=0.17971014492753623\n",
      "EPOCH=64\n",
      "train: loss=2.370127405291018 acc=0.1608695652173913\n",
      "test: loss=2.3653390742730402 acc=0.17246376811594202\n",
      "EPOCH=65\n",
      "train: loss=2.253428309723951 acc=0.1565217391304348\n",
      "test: loss=2.2315971747688623 acc=0.12608695652173912\n",
      "EPOCH=66\n",
      "train: loss=2.1376987386440884 acc=0.1608695652173913\n",
      "test: loss=2.1178230548250503 acc=0.11739130434782609\n",
      "EPOCH=67\n",
      "train: loss=2.102587658253269 acc=0.18695652173913044\n",
      "test: loss=2.0744241923525713 acc=0.21594202898550724\n",
      "EPOCH=68\n",
      "train: loss=2.067224327032117 acc=0.17391304347826086\n",
      "test: loss=2.0341891674027925 acc=0.21884057971014492\n",
      "EPOCH=69\n",
      "train: loss=2.0883990386258002 acc=0.15942028985507245\n",
      "test: loss=2.0517402035602625 acc=0.21014492753623187\n",
      "EPOCH=70\n",
      "train: loss=2.0057302544082423 acc=0.2\n",
      "test: loss=1.9821313315543576 acc=0.21739130434782608\n",
      "EPOCH=71\n",
      "train: loss=1.9969745511594026 acc=0.1753623188405797\n",
      "test: loss=1.978251594736956 acc=0.21739130434782608\n",
      "EPOCH=72\n",
      "train: loss=1.9876556816308395 acc=0.17971014492753623\n",
      "test: loss=1.957294295836186 acc=0.21739130434782608\n",
      "EPOCH=73\n",
      "train: loss=1.9673962193986645 acc=0.16376811594202897\n",
      "test: loss=1.929726781534112 acc=0.19710144927536233\n",
      "EPOCH=74\n",
      "train: loss=1.947932019268257 acc=0.18840579710144928\n",
      "test: loss=1.9073585655378258 acc=0.21739130434782608\n",
      "EPOCH=75\n",
      "train: loss=1.915596486520076 acc=0.2246376811594203\n",
      "test: loss=1.9106267263923866 acc=0.2246376811594203\n",
      "EPOCH=76\n",
      "train: loss=1.873807502483976 acc=0.17971014492753623\n",
      "test: loss=1.859656500125277 acc=0.22753623188405797\n",
      "EPOCH=77\n",
      "train: loss=1.8591999048772065 acc=0.22753623188405797\n",
      "test: loss=1.8326138646706291 acc=0.23043478260869565\n",
      "EPOCH=78\n",
      "train: loss=1.829241815338964 acc=0.18840579710144928\n",
      "test: loss=1.8247609376907348 acc=0.20579710144927535\n",
      "EPOCH=79\n",
      "train: loss=1.8160429379214411 acc=0.21739130434782608\n",
      "test: loss=1.811866397443025 acc=0.2072463768115942\n",
      "EPOCH=80\n",
      "train: loss=1.79899552801381 acc=0.2391304347826087\n",
      "test: loss=1.7862923356070035 acc=0.24347826086956523\n",
      "EPOCH=81\n",
      "train: loss=1.7706055394117384 acc=0.23043478260869565\n",
      "test: loss=1.7784493066262508 acc=0.24492753623188407\n",
      "EPOCH=82\n",
      "train: loss=1.7771168812461522 acc=0.2768115942028985\n",
      "test: loss=1.7691913673843163 acc=0.2318840579710145\n",
      "EPOCH=83\n",
      "train: loss=1.7566040413967077 acc=0.263768115942029\n",
      "test: loss=1.7641705322956693 acc=0.24202898550724639\n",
      "EPOCH=84\n",
      "train: loss=1.742758942514226 acc=0.2826086956521739\n",
      "test: loss=1.7552509416704591 acc=0.24057971014492754\n",
      "EPOCH=85\n",
      "train: loss=1.7480522193770478 acc=0.28695652173913044\n",
      "test: loss=1.7480191593584806 acc=0.23478260869565218\n",
      "EPOCH=86\n",
      "train: loss=1.7489730587904004 acc=0.2782608695652174\n",
      "test: loss=1.7391478020211926 acc=0.2391304347826087\n",
      "EPOCH=87\n",
      "train: loss=1.728259371156278 acc=0.2971014492753623\n",
      "test: loss=1.7341854354609614 acc=0.2927536231884058\n",
      "EPOCH=88\n",
      "train: loss=1.7133938241696012 acc=0.30434782608695654\n",
      "test: loss=1.731830350385196 acc=0.3072463768115942\n",
      "EPOCH=89\n",
      "train: loss=1.726689434569815 acc=0.2985507246376812\n",
      "test: loss=1.7251370796258898 acc=0.28695652173913044\n",
      "EPOCH=90\n",
      "train: loss=1.7083452820777894 acc=0.30869565217391304\n",
      "test: loss=1.7197448623353155 acc=0.28405797101449276\n",
      "EPOCH=91\n",
      "train: loss=1.6933353944101195 acc=0.3\n",
      "test: loss=1.7157997993455416 acc=0.2565217391304348\n",
      "EPOCH=92\n",
      "train: loss=1.6865215681601262 acc=0.3130434782608696\n",
      "test: loss=1.7111615341642628 acc=0.2768115942028985\n",
      "EPOCH=93\n",
      "train: loss=1.6864481119142063 acc=0.32753623188405795\n",
      "test: loss=1.702573051314423 acc=0.26811594202898553\n",
      "EPOCH=94\n",
      "train: loss=1.6879169115121815 acc=0.32463768115942027\n",
      "test: loss=1.707520237867383 acc=0.2608695652173913\n",
      "EPOCH=95\n",
      "train: loss=1.6817334187203559 acc=0.33768115942028987\n",
      "test: loss=1.6976221129514169 acc=0.2768115942028985\n",
      "EPOCH=96\n",
      "train: loss=1.665912819945294 acc=0.33043478260869563\n",
      "test: loss=1.6930926599364349 acc=0.26521739130434785\n",
      "EPOCH=97\n",
      "train: loss=1.6607960291530774 acc=0.33768115942028987\n",
      "test: loss=1.6873285003330396 acc=0.2985507246376812\n",
      "EPOCH=98\n",
      "train: loss=1.6550934511682263 acc=0.3521739130434783\n",
      "test: loss=1.68358977843022 acc=0.32318840579710145\n",
      "EPOCH=99\n",
      "train: loss=1.6633327551510022 acc=0.3347826086956522\n",
      "test: loss=1.6766969461371932 acc=0.3130434782608696\n",
      "EPOCH=100\n",
      "train: loss=1.633041577062745 acc=0.3884057971014493\n",
      "test: loss=1.6755644043286642 acc=0.3072463768115942\n",
      "EPOCH=101\n",
      "train: loss=1.6311352176942686 acc=0.3507246376811594\n",
      "test: loss=1.6666654490042423 acc=0.32028985507246377\n",
      "EPOCH=102\n",
      "train: loss=1.6319509349007537 acc=0.3898550724637681\n",
      "test: loss=1.6658525962760482 acc=0.34202898550724636\n",
      "EPOCH=103\n",
      "train: loss=1.6301029098206672 acc=0.36231884057971014\n",
      "test: loss=1.6585225146749745 acc=0.3289855072463768\n",
      "EPOCH=104\n",
      "train: loss=1.6077369644158128 acc=0.34057971014492755\n",
      "test: loss=1.6518960900928663 acc=0.30869565217391304\n",
      "EPOCH=105\n",
      "train: loss=1.6223161515118418 acc=0.36666666666666664\n",
      "test: loss=1.6507547122844752 acc=0.3391304347826087\n",
      "EPOCH=106\n",
      "train: loss=1.6083670906398608 acc=0.3521739130434783\n",
      "test: loss=1.6388327600299448 acc=0.33768115942028987\n",
      "EPOCH=107\n",
      "train: loss=1.59688590194868 acc=0.3492753623188406\n",
      "test: loss=1.6348690794861835 acc=0.3130434782608696\n",
      "EPOCH=108\n",
      "train: loss=1.5895312184008998 acc=0.36086956521739133\n",
      "test: loss=1.6291313633538675 acc=0.30869565217391304\n",
      "EPOCH=109\n",
      "train: loss=1.5933060863743658 acc=0.34057971014492755\n",
      "test: loss=1.626923905680145 acc=0.32028985507246377\n",
      "EPOCH=110\n",
      "train: loss=1.5915305827838788 acc=0.34202898550724636\n",
      "test: loss=1.6153347942276277 acc=0.30869565217391304\n",
      "EPOCH=111\n",
      "train: loss=1.5457019270330234 acc=0.391304347826087\n",
      "test: loss=1.6124295845411827 acc=0.3115942028985507\n",
      "EPOCH=112\n",
      "train: loss=1.574633531725925 acc=0.3463768115942029\n",
      "test: loss=1.6059719041637752 acc=0.3072463768115942\n",
      "EPOCH=113\n",
      "train: loss=1.5421488754991173 acc=0.38405797101449274\n",
      "test: loss=1.5920282669689345 acc=0.3565217391304348\n",
      "EPOCH=114\n",
      "train: loss=1.5357171157132024 acc=0.40144927536231884\n",
      "test: loss=1.5901208244372105 acc=0.3521739130434783\n",
      "EPOCH=115\n",
      "train: loss=1.5264567046925641 acc=0.38115942028985506\n",
      "test: loss=1.5664493674817292 acc=0.35507246376811596\n",
      "EPOCH=116\n",
      "train: loss=1.507457302014033 acc=0.3710144927536232\n",
      "test: loss=1.564597363402878 acc=0.3565217391304348\n",
      "EPOCH=117\n",
      "train: loss=1.5111525677252506 acc=0.3782608695652174\n",
      "test: loss=1.5463348124338232 acc=0.3652173913043478\n",
      "EPOCH=118\n",
      "train: loss=1.4793549754481385 acc=0.4115942028985507\n",
      "test: loss=1.5384504418442215 acc=0.38115942028985506\n",
      "EPOCH=119\n",
      "train: loss=1.475005789252295 acc=0.3753623188405797\n",
      "test: loss=1.524890176023262 acc=0.37971014492753624\n",
      "EPOCH=120\n",
      "train: loss=1.4634876043036364 acc=0.35942028985507246\n",
      "test: loss=1.5073178421759952 acc=0.39565217391304347\n",
      "EPOCH=121\n",
      "train: loss=1.431934384332187 acc=0.39710144927536234\n",
      "test: loss=1.493443223618079 acc=0.3652173913043478\n",
      "EPOCH=122\n",
      "train: loss=1.4024875721205836 acc=0.4057971014492754\n",
      "test: loss=1.4643764083800108 acc=0.39855072463768115\n",
      "EPOCH=123\n",
      "train: loss=1.3857219044713007 acc=0.39420289855072466\n",
      "test: loss=1.454802165860715 acc=0.3884057971014493\n",
      "EPOCH=124\n",
      "train: loss=1.4165883186070816 acc=0.372463768115942\n",
      "test: loss=1.4620404042195583 acc=0.3565217391304348\n",
      "EPOCH=125\n",
      "train: loss=1.3681325744027677 acc=0.4144927536231884\n",
      "test: loss=1.419927078658256 acc=0.3739130434782609\n",
      "EPOCH=126\n",
      "train: loss=1.358315874873728 acc=0.3710144927536232\n",
      "test: loss=1.4041284657906794 acc=0.391304347826087\n",
      "EPOCH=127\n",
      "train: loss=1.3200243331383967 acc=0.4144927536231884\n",
      "test: loss=1.375105338252109 acc=0.4028985507246377\n",
      "EPOCH=128\n",
      "train: loss=1.3248197963272317 acc=0.40144927536231884\n",
      "test: loss=1.3766767226267551 acc=0.38115942028985506\n",
      "EPOCH=129\n",
      "train: loss=1.329994783539703 acc=0.4246376811594203\n",
      "test: loss=1.3795545957226685 acc=0.3753623188405797\n",
      "EPOCH=130\n",
      "train: loss=1.2890235188214676 acc=0.43478260869565216\n",
      "test: loss=1.3155774604583133 acc=0.4449275362318841\n",
      "EPOCH=131\n",
      "train: loss=1.2678554157416027 acc=0.4391304347826087\n",
      "test: loss=1.308235658424488 acc=0.43768115942028984\n",
      "EPOCH=132\n",
      "train: loss=1.2848437907039256 acc=0.4028985507246377\n",
      "test: loss=1.3093197778515193 acc=0.4144927536231884\n",
      "EPOCH=133\n",
      "train: loss=1.2572193634682807 acc=0.4579710144927536\n",
      "test: loss=1.2904185669145722 acc=0.43043478260869567\n",
      "EPOCH=134\n",
      "train: loss=1.2228795124136884 acc=0.4666666666666667\n",
      "test: loss=1.2615731470826743 acc=0.44782608695652176\n",
      "EPOCH=135\n",
      "train: loss=1.2165394093679345 acc=0.5159420289855072\n",
      "test: loss=1.2532244034435438 acc=0.48985507246376814\n",
      "EPOCH=136\n",
      "train: loss=1.1950027571208235 acc=0.4956521739130435\n",
      "test: loss=1.255611119840456 acc=0.47101449275362317\n",
      "EPOCH=137\n",
      "train: loss=1.1861579208270363 acc=0.5072463768115942\n",
      "test: loss=1.2292086367157922 acc=0.47681159420289854\n",
      "EPOCH=138\n",
      "train: loss=1.1793789548286493 acc=0.518840579710145\n",
      "test: loss=1.2151613439338795 acc=0.4811594202898551\n",
      "EPOCH=139\n",
      "train: loss=1.1628377694582595 acc=0.5144927536231884\n",
      "test: loss=1.2041115821271702 acc=0.4666666666666667\n",
      "EPOCH=140\n",
      "train: loss=1.142772316587144 acc=0.5101449275362319\n",
      "test: loss=1.1843682622564011 acc=0.48985507246376814\n",
      "EPOCH=141\n",
      "train: loss=1.1510896809723066 acc=0.48695652173913045\n",
      "test: loss=1.1867554370043933 acc=0.5043478260869565\n",
      "EPOCH=142\n",
      "train: loss=1.1021881330704344 acc=0.5565217391304348\n",
      "test: loss=1.1597270773804707 acc=0.5492753623188406\n",
      "EPOCH=143\n",
      "train: loss=1.1292820029068684 acc=0.5072463768115942\n",
      "test: loss=1.149229779796324 acc=0.5043478260869565\n",
      "EPOCH=144\n",
      "train: loss=1.1221137851908587 acc=0.518840579710145\n",
      "test: loss=1.137639206431914 acc=0.5173913043478261\n",
      "EPOCH=145\n",
      "train: loss=1.051235537684482 acc=0.5681159420289855\n",
      "test: loss=1.1163361166266428 acc=0.5376811594202898\n",
      "EPOCH=146\n",
      "train: loss=1.1016909052280413 acc=0.5768115942028985\n",
      "test: loss=1.1156218292488569 acc=0.5492753623188406\n",
      "EPOCH=147\n",
      "train: loss=1.1041101659985557 acc=0.5173913043478261\n",
      "test: loss=1.108175125113432 acc=0.5391304347826087\n",
      "EPOCH=148\n",
      "train: loss=1.081002790318883 acc=0.5666666666666667\n",
      "test: loss=1.1121810905311418 acc=0.5594202898550724\n",
      "EPOCH=149\n",
      "train: loss=1.0789803594350815 acc=0.49420289855072463\n",
      "test: loss=1.1398904056652732 acc=0.4927536231884058\n",
      "EPOCH=150\n",
      "train: loss=1.0819282464143158 acc=0.5420289855072464\n",
      "test: loss=1.086167596647705 acc=0.5623188405797102\n",
      "EPOCH=151\n",
      "train: loss=1.0333596821280493 acc=0.5811594202898551\n",
      "test: loss=1.0609395114192064 acc=0.5753623188405798\n",
      "EPOCH=152\n",
      "train: loss=1.0280128645832125 acc=0.5956521739130435\n",
      "test: loss=1.050213060517242 acc=0.5739130434782609\n",
      "EPOCH=153\n",
      "train: loss=0.9873922969120136 acc=0.5666666666666667\n",
      "test: loss=1.0639996695993603 acc=0.5521739130434783\n",
      "EPOCH=154\n",
      "train: loss=0.9817286048894344 acc=0.6289855072463768\n",
      "test: loss=1.0426496549792912 acc=0.5971014492753624\n",
      "EPOCH=155\n",
      "train: loss=0.9981271925071875 acc=0.5985507246376811\n",
      "test: loss=1.0292263823682848 acc=0.5811594202898551\n",
      "EPOCH=156\n",
      "train: loss=1.0245658177072587 acc=0.6173913043478261\n",
      "test: loss=1.044844584847274 acc=0.5826086956521739\n",
      "EPOCH=157\n",
      "train: loss=0.9694084226113299 acc=0.5956521739130435\n",
      "test: loss=1.0196254451637683 acc=0.5768115942028985\n",
      "EPOCH=158\n",
      "train: loss=0.9821170753434948 acc=0.5768115942028985\n",
      "test: loss=0.995634809060805 acc=0.6217391304347826\n",
      "EPOCH=159\n",
      "train: loss=0.9769925236593986 acc=0.5869565217391305\n",
      "test: loss=0.9901028825644997 acc=0.5898550724637681\n",
      "EPOCH=160\n",
      "train: loss=0.9811850599188735 acc=0.5927536231884057\n",
      "test: loss=1.0125336214792038 acc=0.5985507246376811\n",
      "EPOCH=161\n",
      "train: loss=0.970312140352916 acc=0.6014492753623188\n",
      "test: loss=0.9907318836105042 acc=0.6101449275362318\n",
      "EPOCH=162\n",
      "train: loss=0.9489410004430059 acc=0.6043478260869565\n",
      "test: loss=0.9806454375386238 acc=0.5956521739130435\n",
      "EPOCH=163\n",
      "train: loss=0.9087158460871897 acc=0.6536231884057971\n",
      "test: loss=0.9379464602124864 acc=0.6376811594202898\n",
      "EPOCH=164\n",
      "train: loss=0.8886440866559312 acc=0.6565217391304348\n",
      "test: loss=0.9238985267454299 acc=0.6391304347826087\n",
      "EPOCH=165\n",
      "train: loss=0.9607538906236489 acc=0.6304347826086957\n",
      "test: loss=0.9965788346269856 acc=0.5869565217391305\n",
      "EPOCH=166\n",
      "train: loss=0.9421855598526157 acc=0.6246376811594203\n",
      "test: loss=0.949872209811988 acc=0.6318840579710145\n",
      "EPOCH=167\n",
      "train: loss=0.9367781424220057 acc=0.6173913043478261\n",
      "test: loss=0.9831235352525677 acc=0.6231884057971014\n",
      "EPOCH=168\n",
      "train: loss=0.8978979120543902 acc=0.6507246376811594\n",
      "test: loss=0.9333080628805834 acc=0.618840579710145\n",
      "EPOCH=169\n",
      "train: loss=0.8651423496882553 acc=0.6608695652173913\n",
      "test: loss=0.8987141079660775 acc=0.6565217391304348\n",
      "EPOCH=170\n",
      "train: loss=0.9883712759322446 acc=0.5869565217391305\n",
      "test: loss=1.0114502607696298 acc=0.5840579710144927\n",
      "EPOCH=171\n",
      "train: loss=0.8528353704058605 acc=0.6811594202898551\n",
      "test: loss=0.8632223064268845 acc=0.6579710144927536\n",
      "EPOCH=172\n",
      "train: loss=0.8673695182454759 acc=0.6478260869565218\n",
      "test: loss=0.9243865384409825 acc=0.6130434782608696\n",
      "EPOCH=173\n",
      "train: loss=0.8651363056423007 acc=0.6333333333333333\n",
      "test: loss=0.873706023492243 acc=0.6536231884057971\n",
      "EPOCH=174\n",
      "train: loss=0.8226116456307363 acc=0.7130434782608696\n",
      "test: loss=0.8431911145809337 acc=0.6782608695652174\n",
      "EPOCH=175\n",
      "train: loss=0.8548632291995961 acc=0.6507246376811594\n",
      "test: loss=0.8904476371364317 acc=0.6318840579710145\n",
      "EPOCH=176\n",
      "train: loss=0.8332265930493241 acc=0.6826086956521739\n",
      "test: loss=0.8536620235799447 acc=0.6985507246376812\n",
      "EPOCH=177\n",
      "train: loss=0.7797649689828572 acc=0.7115942028985507\n",
      "test: loss=0.845145138606861 acc=0.6782608695652174\n",
      "EPOCH=178\n",
      "train: loss=0.7881955944217633 acc=0.7217391304347827\n",
      "test: loss=0.8203150615420031 acc=0.736231884057971\n",
      "EPOCH=179\n",
      "train: loss=0.7433805250630214 acc=0.763768115942029\n",
      "test: loss=0.7894174007887858 acc=0.7434782608695653\n",
      "EPOCH=180\n",
      "train: loss=0.7959925264012122 acc=0.7202898550724638\n",
      "test: loss=0.8491909293376881 acc=0.6869565217391305\n",
      "EPOCH=181\n",
      "train: loss=0.7560802267891341 acc=0.7318840579710145\n",
      "test: loss=0.7752918611294117 acc=0.7608695652173914\n",
      "EPOCH=182\n",
      "train: loss=0.7318724965595681 acc=0.7434782608695653\n",
      "test: loss=0.7881833014496858 acc=0.736231884057971\n",
      "EPOCH=183\n",
      "train: loss=0.751624912849587 acc=0.6927536231884058\n",
      "test: loss=0.8076552786216463 acc=0.6898550724637681\n",
      "EPOCH=184\n",
      "train: loss=0.7330884897568519 acc=0.7376811594202899\n",
      "test: loss=0.7944686538013427 acc=0.7159420289855073\n",
      "EPOCH=185\n",
      "train: loss=0.7525528428023276 acc=0.7028985507246377\n",
      "test: loss=0.8006914242751141 acc=0.6985507246376812\n",
      "EPOCH=186\n",
      "train: loss=0.6612494466812823 acc=0.7985507246376812\n",
      "test: loss=0.7206046781462172 acc=0.7898550724637681\n",
      "EPOCH=187\n",
      "train: loss=0.6718022364637126 acc=0.7666666666666667\n",
      "test: loss=0.763561959348727 acc=0.7130434782608696\n",
      "EPOCH=188\n",
      "train: loss=0.7948549649853637 acc=0.6739130434782609\n",
      "test: loss=0.8070737715635071 acc=0.6739130434782609\n",
      "EPOCH=189\n",
      "train: loss=0.7556573039190708 acc=0.6782608695652174\n",
      "test: loss=0.7849061959909033 acc=0.6768115942028986\n",
      "EPOCH=190\n",
      "train: loss=0.6986063143656845 acc=0.7463768115942029\n",
      "test: loss=0.7197850991782827 acc=0.7594202898550725\n",
      "EPOCH=191\n",
      "train: loss=0.655434403333651 acc=0.782608695652174\n",
      "test: loss=0.6800136108187608 acc=0.8\n",
      "EPOCH=192\n",
      "train: loss=0.7172910717073018 acc=0.7115942028985507\n",
      "test: loss=0.6969118480206183 acc=0.7507246376811594\n",
      "EPOCH=193\n",
      "train: loss=0.5900130742931388 acc=0.836231884057971\n",
      "test: loss=0.6557015633723442 acc=0.8173913043478261\n",
      "EPOCH=194\n",
      "train: loss=0.5892191965429895 acc=0.7971014492753623\n",
      "test: loss=0.6454474062017719 acc=0.7956521739130434\n",
      "EPOCH=195\n",
      "train: loss=0.7047099474534069 acc=0.7304347826086957\n",
      "test: loss=0.7527370040588405 acc=0.7275362318840579\n",
      "EPOCH=196\n",
      "train: loss=0.6091540610971118 acc=0.7695652173913043\n",
      "test: loss=0.639557194268412 acc=0.7927536231884058\n",
      "EPOCH=197\n",
      "train: loss=0.5725816775882697 acc=0.8115942028985508\n",
      "test: loss=0.6224830123501411 acc=0.8072463768115942\n",
      "EPOCH=198\n",
      "train: loss=0.5822973927698921 acc=0.8188405797101449\n",
      "test: loss=0.6068522001137499 acc=0.808695652173913\n",
      "EPOCH=199\n",
      "train: loss=0.536671124562702 acc=0.8318840579710145\n",
      "test: loss=0.5692076053348897 acc=0.8318840579710145\n",
      "EPOCH=200\n",
      "train: loss=0.6021708146351781 acc=0.7681159420289855\n",
      "test: loss=0.6171135773423357 acc=0.7898550724637681\n",
      "EPOCH=201\n",
      "train: loss=0.5872515530163503 acc=0.7768115942028986\n",
      "test: loss=0.618947650866745 acc=0.7971014492753623\n",
      "EPOCH=202\n",
      "train: loss=0.6663907577067721 acc=0.7289855072463768\n",
      "test: loss=0.675238855039377 acc=0.7289855072463768\n",
      "EPOCH=203\n",
      "train: loss=0.6716175823512932 acc=0.7652173913043478\n",
      "test: loss=0.6433175529117116 acc=0.7768115942028986\n",
      "EPOCH=204\n",
      "train: loss=0.5222917849717634 acc=0.8347826086956521\n",
      "test: loss=0.5664720923214664 acc=0.8202898550724638\n",
      "EPOCH=205\n",
      "train: loss=0.6367708826761531 acc=0.744927536231884\n",
      "test: loss=0.6654554137173851 acc=0.7318840579710145\n",
      "EPOCH=206\n",
      "train: loss=0.5153037727938669 acc=0.8289855072463768\n",
      "test: loss=0.5922882536702884 acc=0.7898550724637681\n",
      "EPOCH=207\n",
      "train: loss=0.47555602635695615 acc=0.8289855072463768\n",
      "test: loss=0.5398387403767286 acc=0.836231884057971\n",
      "EPOCH=208\n",
      "train: loss=0.5751529072809533 acc=0.7927536231884058\n",
      "test: loss=0.5561634032656371 acc=0.8014492753623188\n",
      "EPOCH=209\n",
      "train: loss=0.49666604940455567 acc=0.8405797101449275\n",
      "test: loss=0.5410354831431439 acc=0.8304347826086956\n",
      "EPOCH=210\n",
      "train: loss=0.5009874275696559 acc=0.8130434782608695\n",
      "test: loss=0.5177694757080947 acc=0.8376811594202899\n",
      "EPOCH=211\n",
      "train: loss=0.5801057947553235 acc=0.7768115942028986\n",
      "test: loss=0.5788746779504489 acc=0.8043478260869565\n",
      "EPOCH=212\n",
      "train: loss=0.4839081065908101 acc=0.8463768115942029\n",
      "test: loss=0.5268258170163987 acc=0.827536231884058\n",
      "EPOCH=213\n",
      "train: loss=0.4730335026982821 acc=0.8347826086956521\n",
      "test: loss=0.5207628946828966 acc=0.8217391304347826\n",
      "EPOCH=214\n",
      "train: loss=0.43369111025442736 acc=0.8608695652173913\n",
      "test: loss=0.4766096496468653 acc=0.863768115942029\n",
      "EPOCH=215\n",
      "train: loss=0.4655007011567553 acc=0.8333333333333334\n",
      "test: loss=0.49213488729859606 acc=0.8347826086956521\n",
      "EPOCH=216\n",
      "train: loss=0.49594278184805013 acc=0.8173913043478261\n",
      "test: loss=0.4851515392872397 acc=0.855072463768116\n",
      "EPOCH=217\n",
      "train: loss=0.4226022394772187 acc=0.8608695652173913\n",
      "test: loss=0.4789653404183902 acc=0.8594202898550725\n",
      "EPOCH=218\n",
      "train: loss=0.5134151878672234 acc=0.7797101449275362\n",
      "test: loss=0.5577734370526952 acc=0.7855072463768116\n",
      "EPOCH=219\n",
      "train: loss=0.4181074085328188 acc=0.881159420289855\n",
      "test: loss=0.45260401328020067 acc=0.8666666666666667\n",
      "EPOCH=220\n",
      "train: loss=0.46458637156638494 acc=0.8217391304347826\n",
      "test: loss=0.4779011116103283 acc=0.8333333333333334\n",
      "EPOCH=221\n",
      "train: loss=0.4513051282730885 acc=0.8391304347826087\n",
      "test: loss=0.4847633274307515 acc=0.8405797101449275\n",
      "EPOCH=222\n",
      "train: loss=0.441092628533817 acc=0.8420289855072464\n",
      "test: loss=0.4496837136298553 acc=0.8695652173913043\n",
      "EPOCH=223\n",
      "train: loss=0.41471476087792086 acc=0.8710144927536232\n",
      "test: loss=0.43942816387541406 acc=0.8492753623188406\n",
      "EPOCH=224\n",
      "train: loss=0.42296507147603085 acc=0.8652173913043478\n",
      "test: loss=0.45795993859663714 acc=0.8507246376811595\n",
      "EPOCH=225\n",
      "train: loss=0.47552487365277213 acc=0.8202898550724638\n",
      "test: loss=0.48469784557245726 acc=0.827536231884058\n",
      "EPOCH=226\n",
      "train: loss=0.4216428613334057 acc=0.8347826086956521\n",
      "test: loss=0.46935872934316625 acc=0.8246376811594203\n",
      "EPOCH=227\n",
      "train: loss=0.3517435590520609 acc=0.8985507246376812\n",
      "test: loss=0.39546364276970236 acc=0.8840579710144928\n",
      "EPOCH=228\n",
      "train: loss=0.3963622168313874 acc=0.8753623188405797\n",
      "test: loss=0.41224320278725707 acc=0.8884057971014493\n",
      "EPOCH=229\n",
      "train: loss=0.43611227128303787 acc=0.8376811594202899\n",
      "test: loss=0.4700743692873073 acc=0.8246376811594203\n",
      "EPOCH=230\n",
      "train: loss=0.4017185585748782 acc=0.8884057971014493\n",
      "test: loss=0.4245864093215947 acc=0.8898550724637682\n",
      "EPOCH=231\n",
      "train: loss=0.4231467383755101 acc=0.8217391304347826\n",
      "test: loss=0.46373773617703684 acc=0.8260869565217391\n",
      "EPOCH=232\n",
      "train: loss=0.41673016906614696 acc=0.8739130434782608\n",
      "test: loss=0.4217838444870195 acc=0.8652173913043478\n",
      "EPOCH=233\n",
      "train: loss=0.37624120868782956 acc=0.8594202898550725\n",
      "test: loss=0.41032956467858733 acc=0.863768115942029\n",
      "EPOCH=234\n",
      "train: loss=0.35686332766958717 acc=0.8985507246376812\n",
      "test: loss=0.37905665591875415 acc=0.8956521739130435\n",
      "EPOCH=235\n",
      "train: loss=0.35213479382775564 acc=0.9043478260869565\n",
      "test: loss=0.41111199906251084 acc=0.8608695652173913\n",
      "EPOCH=236\n",
      "train: loss=0.40453080825770843 acc=0.8507246376811595\n",
      "test: loss=0.45786765660665446 acc=0.8188405797101449\n",
      "EPOCH=237\n",
      "train: loss=0.37293420208051153 acc=0.8768115942028986\n",
      "test: loss=0.3860889587711637 acc=0.8927536231884058\n",
      "EPOCH=238\n",
      "train: loss=0.3571586058110647 acc=0.8753623188405797\n",
      "test: loss=0.41883389066408755 acc=0.8594202898550725\n",
      "EPOCH=239\n",
      "train: loss=0.3453288942919639 acc=0.8898550724637682\n",
      "test: loss=0.37996611330300517 acc=0.8695652173913043\n",
      "EPOCH=240\n",
      "train: loss=0.40975070059674024 acc=0.8536231884057971\n",
      "test: loss=0.4355687407769965 acc=0.8376811594202899\n",
      "EPOCH=241\n",
      "train: loss=0.36549740222491917 acc=0.8724637681159421\n",
      "test: loss=0.4315999978442973 acc=0.8405797101449275\n",
      "EPOCH=242\n",
      "train: loss=0.2916204009348513 acc=0.9333333333333333\n",
      "test: loss=0.33577410606363706 acc=0.9101449275362319\n",
      "EPOCH=243\n",
      "train: loss=0.35034133455027705 acc=0.8739130434782608\n",
      "test: loss=0.3862696907475449 acc=0.8666666666666667\n",
      "EPOCH=244\n",
      "train: loss=0.3411545570253098 acc=0.8898550724637682\n",
      "test: loss=0.3589666021505263 acc=0.8797101449275362\n",
      "EPOCH=245\n",
      "train: loss=0.3052449657057034 acc=0.908695652173913\n",
      "test: loss=0.34404768430757793 acc=0.8884057971014493\n",
      "EPOCH=246\n",
      "train: loss=0.43483929978413643 acc=0.8202898550724638\n",
      "test: loss=0.4743021962788813 acc=0.8043478260869565\n",
      "EPOCH=247\n",
      "train: loss=0.30136365798349235 acc=0.9072463768115943\n",
      "test: loss=0.3196872613882052 acc=0.9130434782608695\n",
      "EPOCH=248\n",
      "train: loss=0.2917286178621841 acc=0.927536231884058\n",
      "test: loss=0.3040265069626596 acc=0.9130434782608695\n",
      "EPOCH=249\n",
      "train: loss=0.30294486954951644 acc=0.908695652173913\n",
      "test: loss=0.3258953697540784 acc=0.9028985507246376\n",
      "EPOCH=250\n",
      "train: loss=0.3146352339161398 acc=0.8927536231884058\n",
      "test: loss=0.3452205166323583 acc=0.8927536231884058\n",
      "EPOCH=251\n",
      "train: loss=0.3170324007017147 acc=0.9072463768115943\n",
      "test: loss=0.3481975053574559 acc=0.8753623188405797\n",
      "EPOCH=252\n",
      "train: loss=0.2895075125596992 acc=0.9376811594202898\n",
      "test: loss=0.29555363515704075 acc=0.9217391304347826\n",
      "EPOCH=253\n",
      "train: loss=0.42561267838776246 acc=0.8536231884057971\n",
      "test: loss=0.364153221127513 acc=0.8869565217391304\n",
      "EPOCH=254\n",
      "train: loss=0.25950538792843136 acc=0.936231884057971\n",
      "test: loss=0.3282771227575089 acc=0.8913043478260869\n",
      "EPOCH=255\n",
      "train: loss=0.32231498594468366 acc=0.8855072463768116\n",
      "test: loss=0.3426619899340659 acc=0.8884057971014493\n",
      "EPOCH=256\n",
      "train: loss=0.25740850018463907 acc=0.9347826086956522\n",
      "test: loss=0.2920968768487185 acc=0.9130434782608695\n",
      "EPOCH=257\n",
      "train: loss=0.2522127490076473 acc=0.9376811594202898\n",
      "test: loss=0.3086257510040752 acc=0.9\n",
      "EPOCH=258\n",
      "train: loss=0.31459732775313215 acc=0.908695652173913\n",
      "test: loss=0.3467150726551325 acc=0.8782608695652174\n",
      "EPOCH=259\n",
      "train: loss=0.355335467734941 acc=0.8869565217391304\n",
      "test: loss=0.34227399856326224 acc=0.8956521739130435\n",
      "EPOCH=260\n",
      "train: loss=0.3090450760123887 acc=0.9043478260869565\n",
      "test: loss=0.3187608128497917 acc=0.8884057971014493\n",
      "EPOCH=261\n",
      "train: loss=0.3346240255956376 acc=0.8652173913043478\n",
      "test: loss=0.34691451654011934 acc=0.8797101449275362\n",
      "EPOCH=262\n",
      "train: loss=0.25308281994969717 acc=0.936231884057971\n",
      "test: loss=0.2868403807235207 acc=0.9159420289855073\n",
      "EPOCH=263\n",
      "train: loss=0.3089787061842482 acc=0.9\n",
      "test: loss=0.31625219146401345 acc=0.8927536231884058\n",
      "EPOCH=264\n",
      "train: loss=0.2825665759946813 acc=0.9318840579710145\n",
      "test: loss=0.2848577906734452 acc=0.9115942028985508\n",
      "EPOCH=265\n",
      "train: loss=0.28040526583183395 acc=0.9333333333333333\n",
      "test: loss=0.2951619803394694 acc=0.9101449275362319\n",
      "EPOCH=266\n",
      "train: loss=0.23817076199583412 acc=0.9347826086956522\n",
      "test: loss=0.28248175296821104 acc=0.9217391304347826\n",
      "EPOCH=267\n",
      "train: loss=0.2711176859020033 acc=0.9231884057971015\n",
      "test: loss=0.3041060635171311 acc=0.9057971014492754\n",
      "EPOCH=268\n",
      "train: loss=0.27929407238093057 acc=0.908695652173913\n",
      "test: loss=0.2879035096299207 acc=0.9246376811594202\n",
      "EPOCH=269\n",
      "train: loss=0.22351822857652198 acc=0.9492753623188406\n",
      "test: loss=0.2550826174339475 acc=0.9260869565217391\n",
      "EPOCH=270\n",
      "train: loss=0.24768097374033168 acc=0.9289855072463769\n",
      "test: loss=0.2837777449219214 acc=0.9144927536231884\n",
      "EPOCH=271\n",
      "train: loss=0.25601363224458235 acc=0.9318840579710145\n",
      "test: loss=0.2655617805350103 acc=0.9246376811594202\n",
      "EPOCH=272\n",
      "train: loss=0.24436853287422666 acc=0.9376811594202898\n",
      "test: loss=0.26794105676577284 acc=0.9304347826086956\n",
      "EPOCH=273\n",
      "train: loss=0.2493726641066251 acc=0.9478260869565217\n",
      "test: loss=0.25152470102589375 acc=0.9304347826086956\n",
      "EPOCH=274\n",
      "train: loss=0.22927822047841154 acc=0.9434782608695652\n",
      "test: loss=0.26115966227248927 acc=0.927536231884058\n",
      "EPOCH=275\n",
      "train: loss=0.20856881983201964 acc=0.946376811594203\n",
      "test: loss=0.25986155328864696 acc=0.9304347826086956\n",
      "EPOCH=276\n",
      "train: loss=0.22143820772796155 acc=0.9507246376811594\n",
      "test: loss=0.24822252834113687 acc=0.9347826086956522\n",
      "EPOCH=277\n",
      "train: loss=0.21342995585430838 acc=0.9521739130434783\n",
      "test: loss=0.23354640912418118 acc=0.9347826086956522\n",
      "EPOCH=278\n",
      "train: loss=0.17802107930301514 acc=0.9623188405797102\n",
      "test: loss=0.23867351506289222 acc=0.9405797101449276\n",
      "EPOCH=279\n",
      "train: loss=0.19667236903476515 acc=0.9536231884057971\n",
      "test: loss=0.24895380520543584 acc=0.9333333333333333\n",
      "EPOCH=280\n",
      "train: loss=0.20511345868028424 acc=0.9492753623188406\n",
      "test: loss=0.23572156073768502 acc=0.9405797101449276\n",
      "EPOCH=281\n",
      "train: loss=0.23560803078399276 acc=0.9318840579710145\n",
      "test: loss=0.28566629321177733 acc=0.9115942028985508\n",
      "EPOCH=282\n",
      "train: loss=0.17475145628901056 acc=0.9623188405797102\n",
      "test: loss=0.20853153026495735 acc=0.9507246376811594\n",
      "EPOCH=283\n",
      "train: loss=0.24365928035711978 acc=0.9318840579710145\n",
      "test: loss=0.2380584008532595 acc=0.9318840579710145\n",
      "EPOCH=284\n",
      "train: loss=0.2138654584592534 acc=0.946376811594203\n",
      "test: loss=0.24732369358695205 acc=0.927536231884058\n",
      "EPOCH=285\n",
      "train: loss=0.2397745504816724 acc=0.927536231884058\n",
      "test: loss=0.26016071477366703 acc=0.9260869565217391\n",
      "EPOCH=286\n",
      "train: loss=0.2968232771993338 acc=0.8913043478260869\n",
      "test: loss=0.3108171581377552 acc=0.8797101449275362\n",
      "EPOCH=287\n",
      "train: loss=0.2656427444178574 acc=0.9231884057971015\n",
      "test: loss=0.2996169981971869 acc=0.8927536231884058\n",
      "EPOCH=288\n",
      "train: loss=0.2760781732978775 acc=0.9144927536231884\n",
      "test: loss=0.2938591307343787 acc=0.908695652173913\n",
      "EPOCH=289\n",
      "train: loss=0.3090930571101785 acc=0.8826086956521739\n",
      "test: loss=0.38757225664507977 acc=0.8449275362318841\n",
      "EPOCH=290\n",
      "train: loss=0.3087768965027045 acc=0.8898550724637682\n",
      "test: loss=0.31953011849674523 acc=0.8797101449275362\n",
      "EPOCH=291\n",
      "train: loss=0.14721866887639803 acc=0.9695652173913043\n",
      "test: loss=0.21774924185436617 acc=0.9391304347826087\n",
      "EPOCH=292\n",
      "train: loss=0.29782277364520005 acc=0.8855072463768116\n",
      "test: loss=0.33043226552895183 acc=0.8753623188405797\n",
      "EPOCH=293\n",
      "train: loss=0.1911910892543729 acc=0.9579710144927536\n",
      "test: loss=0.23183878606965216 acc=0.9420289855072463\n",
      "EPOCH=294\n",
      "train: loss=0.1793906049263424 acc=0.9594202898550724\n",
      "test: loss=0.2003108411135896 acc=0.9449275362318841\n",
      "EPOCH=295\n",
      "train: loss=0.18132385008819096 acc=0.9666666666666667\n",
      "test: loss=0.23372596404943327 acc=0.927536231884058\n",
      "EPOCH=296\n",
      "train: loss=0.18301369930373715 acc=0.9420289855072463\n",
      "test: loss=0.20433800350390652 acc=0.946376811594203\n",
      "EPOCH=297\n",
      "train: loss=0.19180394150645716 acc=0.9536231884057971\n",
      "test: loss=0.23566029472618222 acc=0.9420289855072463\n",
      "EPOCH=298\n",
      "train: loss=0.16188742431931777 acc=0.9666666666666667\n",
      "test: loss=0.20310917051869234 acc=0.9449275362318841\n",
      "EPOCH=299\n",
      "train: loss=0.21039597222225512 acc=0.9333333333333333\n",
      "test: loss=0.22636997603334275 acc=0.936231884057971\n",
      "EPOCH=300\n",
      "train: loss=0.17089992254536743 acc=0.9652173913043478\n",
      "test: loss=0.21071916965411747 acc=0.9376811594202898\n",
      "EPOCH=301\n",
      "train: loss=0.19325279180062563 acc=0.9536231884057971\n",
      "test: loss=0.2059828396354704 acc=0.9536231884057971\n",
      "EPOCH=302\n",
      "train: loss=0.16006786064080306 acc=0.9623188405797102\n",
      "test: loss=0.19156572377247832 acc=0.9521739130434783\n",
      "EPOCH=303\n",
      "train: loss=0.17195174646506042 acc=0.9565217391304348\n",
      "test: loss=0.19135604971986891 acc=0.9449275362318841\n",
      "EPOCH=304\n",
      "train: loss=0.2207758327818474 acc=0.9478260869565217\n",
      "test: loss=0.2522420087692149 acc=0.9130434782608695\n",
      "EPOCH=305\n",
      "train: loss=0.18943729374283144 acc=0.9507246376811594\n",
      "test: loss=0.2225147340212509 acc=0.9333333333333333\n",
      "EPOCH=306\n",
      "train: loss=0.17660824737449993 acc=0.9594202898550724\n",
      "test: loss=0.21123202868630356 acc=0.9420289855072463\n",
      "EPOCH=307\n",
      "train: loss=0.17706026390279883 acc=0.9521739130434783\n",
      "test: loss=0.19876372230584452 acc=0.9521739130434783\n",
      "EPOCH=308\n",
      "train: loss=0.1756757001520347 acc=0.9536231884057971\n",
      "test: loss=0.22746004301502515 acc=0.9405797101449276\n",
      "EPOCH=309\n",
      "train: loss=0.16405208545811414 acc=0.9652173913043478\n",
      "test: loss=0.18611029703145807 acc=0.9507246376811594\n",
      "EPOCH=310\n",
      "train: loss=0.18174669984489783 acc=0.9536231884057971\n",
      "test: loss=0.20222757080147225 acc=0.9376811594202898\n",
      "EPOCH=311\n",
      "train: loss=0.18627077797443592 acc=0.9492753623188406\n",
      "test: loss=0.22881276821800925 acc=0.9318840579710145\n",
      "EPOCH=312\n",
      "train: loss=0.21359576042065612 acc=0.9391304347826087\n",
      "test: loss=0.21599334582031166 acc=0.936231884057971\n",
      "EPOCH=313\n",
      "train: loss=0.1494376200972972 acc=0.9565217391304348\n",
      "test: loss=0.19353076188685456 acc=0.9405797101449276\n",
      "EPOCH=314\n",
      "train: loss=0.19087103544308406 acc=0.9420289855072463\n",
      "test: loss=0.1926142035140757 acc=0.946376811594203\n",
      "EPOCH=315\n",
      "train: loss=0.18814784478842436 acc=0.9391304347826087\n",
      "test: loss=0.19197472237227475 acc=0.946376811594203\n",
      "EPOCH=316\n",
      "train: loss=0.1836155975158824 acc=0.9565217391304348\n",
      "test: loss=0.2008119236559677 acc=0.9391304347826087\n",
      "EPOCH=317\n",
      "train: loss=0.12699405850765197 acc=0.9840579710144928\n",
      "test: loss=0.17804849100264591 acc=0.9550724637681159\n",
      "EPOCH=318\n",
      "train: loss=0.1813185760538413 acc=0.9405797101449276\n",
      "test: loss=0.18654507119019823 acc=0.9434782608695652\n",
      "EPOCH=319\n",
      "train: loss=0.14461903601385648 acc=0.9652173913043478\n",
      "test: loss=0.1841993831754234 acc=0.9492753623188406\n",
      "EPOCH=320\n",
      "train: loss=0.17382150572093846 acc=0.9478260869565217\n",
      "test: loss=0.18088655064581172 acc=0.9478260869565217\n",
      "EPOCH=321\n",
      "train: loss=0.15516453151990983 acc=0.9565217391304348\n",
      "test: loss=0.17084137734768182 acc=0.9536231884057971\n",
      "EPOCH=322\n",
      "train: loss=0.15166028443162086 acc=0.9652173913043478\n",
      "test: loss=0.18939683805418073 acc=0.9405797101449276\n",
      "EPOCH=323\n",
      "train: loss=0.1260685242679176 acc=0.9797101449275363\n",
      "test: loss=0.16311009827734688 acc=0.9579710144927536\n",
      "EPOCH=324\n",
      "train: loss=0.15704190351492983 acc=0.9608695652173913\n",
      "test: loss=0.1768310835128722 acc=0.9550724637681159\n",
      "EPOCH=325\n",
      "train: loss=0.14138716037400562 acc=0.9594202898550724\n",
      "test: loss=0.17159384850578582 acc=0.9608695652173913\n",
      "EPOCH=326\n",
      "train: loss=0.14099921168199744 acc=0.9637681159420289\n",
      "test: loss=0.17845981843370215 acc=0.9608695652173913\n",
      "EPOCH=327\n",
      "train: loss=0.16131967544457385 acc=0.9608695652173913\n",
      "test: loss=0.17478464779764497 acc=0.9565217391304348\n",
      "EPOCH=328\n",
      "train: loss=0.15498150264075788 acc=0.9666666666666667\n",
      "test: loss=0.1955922232909053 acc=0.946376811594203\n",
      "EPOCH=329\n",
      "train: loss=0.12826856385945049 acc=0.9710144927536232\n",
      "test: loss=0.16782619178716826 acc=0.9550724637681159\n",
      "EPOCH=330\n",
      "train: loss=0.146736444400491 acc=0.9681159420289855\n",
      "test: loss=0.17075468682831316 acc=0.9565217391304348\n",
      "EPOCH=331\n",
      "train: loss=0.11649162347709627 acc=0.9681159420289855\n",
      "test: loss=0.1662442227893695 acc=0.9550724637681159\n",
      "EPOCH=332\n",
      "train: loss=0.16200755079303433 acc=0.9637681159420289\n",
      "test: loss=0.1630112513495935 acc=0.9521739130434783\n",
      "EPOCH=333\n",
      "train: loss=0.1466446236329553 acc=0.9681159420289855\n",
      "test: loss=0.16734933753014838 acc=0.9536231884057971\n",
      "EPOCH=334\n",
      "train: loss=0.14657532567724813 acc=0.9521739130434783\n",
      "test: loss=0.1729038577966687 acc=0.9579710144927536\n",
      "EPOCH=335\n",
      "train: loss=0.12266698072780327 acc=0.9739130434782609\n",
      "test: loss=0.169980141023895 acc=0.9536231884057971\n",
      "EPOCH=336\n",
      "train: loss=0.12142228226134055 acc=0.9753623188405797\n",
      "test: loss=0.16868531032567893 acc=0.9521739130434783\n",
      "EPOCH=337\n",
      "train: loss=0.1572209830143433 acc=0.9739130434782609\n",
      "test: loss=0.18906805742998836 acc=0.9492753623188406\n",
      "EPOCH=338\n",
      "train: loss=0.14130479960882783 acc=0.9623188405797102\n",
      "test: loss=0.16423997763692003 acc=0.9608695652173913\n",
      "EPOCH=339\n",
      "train: loss=0.1481567031444433 acc=0.9594202898550724\n",
      "test: loss=0.16899316493277644 acc=0.9492753623188406\n",
      "EPOCH=340\n",
      "train: loss=0.13899729326938381 acc=0.9637681159420289\n",
      "test: loss=0.1659116033162507 acc=0.9550724637681159\n",
      "EPOCH=341\n",
      "train: loss=0.12915026033418975 acc=0.9753623188405797\n",
      "test: loss=0.16457924253437617 acc=0.9579710144927536\n",
      "EPOCH=342\n",
      "train: loss=0.11649751034945412 acc=0.972463768115942\n",
      "test: loss=0.15620807083626762 acc=0.9579710144927536\n",
      "EPOCH=343\n",
      "train: loss=0.15388751469735576 acc=0.9623188405797102\n",
      "test: loss=0.17048945179977254 acc=0.9550724637681159\n",
      "EPOCH=344\n",
      "train: loss=0.1290732329298803 acc=0.9637681159420289\n",
      "test: loss=0.15724383978095316 acc=0.9579710144927536\n",
      "EPOCH=345\n",
      "train: loss=0.15115218005617942 acc=0.9637681159420289\n",
      "test: loss=0.16008166091130038 acc=0.9492753623188406\n",
      "EPOCH=346\n",
      "train: loss=0.16869672769990637 acc=0.9478260869565217\n",
      "test: loss=0.19346181657905803 acc=0.9478260869565217\n",
      "EPOCH=347\n",
      "train: loss=0.1365230724498479 acc=0.9623188405797102\n",
      "test: loss=0.1553446292726933 acc=0.9623188405797102\n",
      "EPOCH=348\n",
      "train: loss=0.14668794892006215 acc=0.9710144927536232\n",
      "test: loss=0.1727457584542946 acc=0.9492753623188406\n",
      "EPOCH=349\n",
      "train: loss=0.14620709305015053 acc=0.9550724637681159\n",
      "test: loss=0.16433767268876365 acc=0.9478260869565217\n",
      "EPOCH=350\n",
      "train: loss=0.12622731827014186 acc=0.9652173913043478\n",
      "test: loss=0.1473599943646242 acc=0.9608695652173913\n",
      "EPOCH=351\n",
      "train: loss=0.1239207754255377 acc=0.9739130434782609\n",
      "test: loss=0.148737217288228 acc=0.9608695652173913\n",
      "EPOCH=352\n",
      "train: loss=0.12060282401491904 acc=0.9695652173913043\n",
      "test: loss=0.14770381169825103 acc=0.9550724637681159\n",
      "EPOCH=353\n",
      "train: loss=0.13574844628177402 acc=0.9623188405797102\n",
      "test: loss=0.15576585507890253 acc=0.9579710144927536\n",
      "EPOCH=354\n",
      "train: loss=0.10655908722581153 acc=0.9768115942028985\n",
      "test: loss=0.15028532930051186 acc=0.9579710144927536\n",
      "EPOCH=355\n",
      "train: loss=0.1447199050782755 acc=0.9478260869565217\n",
      "test: loss=0.1428845369232516 acc=0.9550724637681159\n",
      "EPOCH=356\n",
      "train: loss=0.10016987976775019 acc=0.9826086956521739\n",
      "test: loss=0.14400379491841664 acc=0.9594202898550724\n",
      "EPOCH=357\n",
      "train: loss=0.11322409641665138 acc=0.9710144927536232\n",
      "test: loss=0.14291788015948376 acc=0.9594202898550724\n",
      "EPOCH=358\n",
      "train: loss=0.19463781555804271 acc=0.9449275362318841\n",
      "test: loss=0.19315048185285708 acc=0.9434782608695652\n",
      "EPOCH=359\n",
      "train: loss=0.1550347247971766 acc=0.9608695652173913\n",
      "test: loss=0.20709949008373044 acc=0.9231884057971015\n",
      "EPOCH=360\n",
      "train: loss=0.15295030197937287 acc=0.9507246376811594\n",
      "test: loss=0.1784137357841404 acc=0.9434782608695652\n",
      "EPOCH=361\n",
      "train: loss=0.13998290905059338 acc=0.9565217391304348\n",
      "test: loss=0.14785987880114998 acc=0.9507246376811594\n",
      "EPOCH=362\n",
      "train: loss=0.4327268455392094 acc=0.8043478260869565\n",
      "test: loss=0.42888488003447783 acc=0.8173913043478261\n",
      "EPOCH=363\n",
      "train: loss=0.23158441969248142 acc=0.9289855072463769\n",
      "test: loss=0.24869791859026114 acc=0.9304347826086956\n",
      "EPOCH=364\n",
      "train: loss=0.24932837689378082 acc=0.908695652173913\n",
      "test: loss=0.2956720980628102 acc=0.8942028985507247\n",
      "EPOCH=365\n",
      "train: loss=0.12484805166659763 acc=0.9594202898550724\n",
      "test: loss=0.16459549010670751 acc=0.9521739130434783\n",
      "EPOCH=366\n",
      "train: loss=0.14803901291521945 acc=0.9536231884057971\n",
      "test: loss=0.1882928538444305 acc=0.9347826086956522\n",
      "EPOCH=367\n",
      "train: loss=0.1495477753241426 acc=0.9623188405797102\n",
      "test: loss=0.1883399696334759 acc=0.9333333333333333\n",
      "EPOCH=368\n",
      "train: loss=0.14258695072442779 acc=0.9594202898550724\n",
      "test: loss=0.16380897550655166 acc=0.9405797101449276\n",
      "EPOCH=369\n",
      "train: loss=0.13674671067869962 acc=0.9594202898550724\n",
      "test: loss=0.16930628499989245 acc=0.9507246376811594\n",
      "EPOCH=370\n",
      "train: loss=0.14456346511113857 acc=0.9478260869565217\n",
      "test: loss=0.17932334353106166 acc=0.9434782608695652\n",
      "EPOCH=371\n",
      "train: loss=0.1350495094591521 acc=0.9565217391304348\n",
      "test: loss=0.153453318761614 acc=0.9507246376811594\n",
      "EPOCH=372\n",
      "train: loss=0.11875192938332212 acc=0.972463768115942\n",
      "test: loss=0.15021867796168817 acc=0.9550724637681159\n",
      "EPOCH=373\n",
      "train: loss=0.09532482494473632 acc=0.9710144927536232\n",
      "test: loss=0.14204974672954016 acc=0.9565217391304348\n",
      "EPOCH=374\n",
      "train: loss=0.12590638188120057 acc=0.9623188405797102\n",
      "test: loss=0.15713101797445617 acc=0.9420289855072463\n",
      "EPOCH=375\n",
      "train: loss=0.18575696052212678 acc=0.9318840579710145\n",
      "test: loss=0.21924647815020748 acc=0.9289855072463769\n",
      "EPOCH=376\n",
      "train: loss=0.11424466889977233 acc=0.9695652173913043\n",
      "test: loss=0.13625639689013516 acc=0.9579710144927536\n",
      "EPOCH=377\n",
      "train: loss=0.1612429951985362 acc=0.9579710144927536\n",
      "test: loss=0.21319940288506495 acc=0.9217391304347826\n",
      "EPOCH=378\n",
      "train: loss=0.14695981547188833 acc=0.9507246376811594\n",
      "test: loss=0.16605545054488016 acc=0.9478260869565217\n",
      "EPOCH=379\n",
      "train: loss=0.13433072452134828 acc=0.9579710144927536\n",
      "test: loss=0.17234924950443584 acc=0.9420289855072463\n",
      "EPOCH=380\n",
      "train: loss=0.17876099461461928 acc=0.9405797101449276\n",
      "test: loss=0.21791688565869624 acc=0.9188405797101449\n",
      "EPOCH=381\n",
      "train: loss=0.19579129601733639 acc=0.9376811594202898\n",
      "test: loss=0.2151984142541498 acc=0.9202898550724637\n",
      "EPOCH=382\n",
      "train: loss=0.09716725583588202 acc=0.9681159420289855\n",
      "test: loss=0.14451860737686545 acc=0.9507246376811594\n",
      "EPOCH=383\n",
      "train: loss=0.10639241572648842 acc=0.9710144927536232\n",
      "test: loss=0.1467211767802745 acc=0.9521739130434783\n",
      "EPOCH=384\n",
      "train: loss=0.09979605548783117 acc=0.9739130434782609\n",
      "test: loss=0.13003045849448994 acc=0.9666666666666667\n",
      "EPOCH=385\n",
      "train: loss=0.11191088549570903 acc=0.9652173913043478\n",
      "test: loss=0.13714385281885938 acc=0.9594202898550724\n",
      "EPOCH=386\n",
      "train: loss=0.11371016198188928 acc=0.9666666666666667\n",
      "test: loss=0.13367454354643415 acc=0.9565217391304348\n",
      "EPOCH=387\n",
      "train: loss=0.12135582521782519 acc=0.9695652173913043\n",
      "test: loss=0.136872773558928 acc=0.9579710144927536\n",
      "EPOCH=388\n",
      "train: loss=0.11090938131455651 acc=0.9710144927536232\n",
      "test: loss=0.14409405466531758 acc=0.9623188405797102\n",
      "EPOCH=389\n",
      "train: loss=0.12133797566678539 acc=0.9536231884057971\n",
      "test: loss=0.13419345116506626 acc=0.9594202898550724\n",
      "EPOCH=390\n",
      "train: loss=0.11560856107283579 acc=0.9594202898550724\n",
      "test: loss=0.14161148824872252 acc=0.9521739130434783\n",
      "EPOCH=391\n",
      "train: loss=0.08962226684524058 acc=0.9739130434782609\n",
      "test: loss=0.13265426489987528 acc=0.9521739130434783\n",
      "EPOCH=392\n",
      "train: loss=0.11471905382813534 acc=0.9623188405797102\n",
      "test: loss=0.13498682529207146 acc=0.9594202898550724\n",
      "EPOCH=393\n",
      "train: loss=0.10052139577680161 acc=0.9826086956521739\n",
      "test: loss=0.13996286293088592 acc=0.9565217391304348\n",
      "EPOCH=394\n",
      "train: loss=0.0793519475531998 acc=0.9855072463768116\n",
      "test: loss=0.1262816978902924 acc=0.9652173913043478\n",
      "EPOCH=395\n",
      "train: loss=0.0975724963524255 acc=0.981159420289855\n",
      "test: loss=0.13438666149759096 acc=0.9594202898550724\n",
      "EPOCH=396\n",
      "train: loss=0.12203355142998813 acc=0.9666666666666667\n",
      "test: loss=0.13720139129480863 acc=0.9579710144927536\n",
      "EPOCH=397\n",
      "train: loss=0.10971131725840468 acc=0.9710144927536232\n",
      "test: loss=0.12360813548781363 acc=0.9579710144927536\n",
      "EPOCH=398\n",
      "train: loss=0.1210110007215693 acc=0.9666666666666667\n",
      "test: loss=0.13067841784361298 acc=0.9550724637681159\n",
      "EPOCH=399\n",
      "train: loss=0.11249591398620026 acc=0.9681159420289855\n",
      "test: loss=0.144350772760601 acc=0.9492753623188406\n",
      "EPOCH=400\n",
      "train: loss=0.10667044008272095 acc=0.9768115942028985\n",
      "test: loss=0.14331925767072776 acc=0.9536231884057971\n",
      "EPOCH=401\n",
      "train: loss=0.09072047958289804 acc=0.9782608695652174\n",
      "test: loss=0.12759801747160304 acc=0.9623188405797102\n",
      "EPOCH=402\n",
      "train: loss=0.1053509256648887 acc=0.9666666666666667\n",
      "test: loss=0.1180055967894354 acc=0.9637681159420289\n",
      "EPOCH=403\n",
      "train: loss=0.1046565353088424 acc=0.9695652173913043\n",
      "test: loss=0.1252795525761531 acc=0.9565217391304348\n",
      "EPOCH=404\n",
      "train: loss=0.10448415957467079 acc=0.9695652173913043\n",
      "test: loss=0.12360788284247091 acc=0.9623188405797102\n",
      "EPOCH=405\n",
      "train: loss=0.12380334876974487 acc=0.9608695652173913\n",
      "test: loss=0.12443286775919989 acc=0.9594202898550724\n",
      "EPOCH=406\n",
      "train: loss=0.10476223452769773 acc=0.9695652173913043\n",
      "test: loss=0.12367603349171341 acc=0.9652173913043478\n",
      "EPOCH=407\n",
      "train: loss=0.10187613755778975 acc=0.9695652173913043\n",
      "test: loss=0.12855204677910717 acc=0.9637681159420289\n",
      "EPOCH=408\n",
      "train: loss=0.12619688565059667 acc=0.9507246376811594\n",
      "test: loss=0.12233777385028087 acc=0.9623188405797102\n",
      "EPOCH=409\n",
      "train: loss=0.10637943632281995 acc=0.9637681159420289\n",
      "test: loss=0.11902267074319621 acc=0.9623188405797102\n",
      "EPOCH=410\n",
      "train: loss=0.1042845913354593 acc=0.9753623188405797\n",
      "test: loss=0.13200089556134234 acc=0.9536231884057971\n",
      "EPOCH=411\n",
      "train: loss=0.08984698735729699 acc=0.9768115942028985\n",
      "test: loss=0.12658264033794114 acc=0.9608695652173913\n",
      "EPOCH=412\n",
      "train: loss=0.1181818897937062 acc=0.9608695652173913\n",
      "test: loss=0.12254729841861922 acc=0.9666666666666667\n",
      "EPOCH=413\n",
      "train: loss=0.08545766330603657 acc=0.9739130434782609\n",
      "test: loss=0.11677341607312527 acc=0.9652173913043478\n",
      "EPOCH=414\n",
      "train: loss=0.08136666771671261 acc=0.9782608695652174\n",
      "test: loss=0.124593464678035 acc=0.9579710144927536\n",
      "EPOCH=415\n",
      "train: loss=0.31013078507085745 acc=0.8739130434782608\n",
      "test: loss=0.36907199192688045 acc=0.8623188405797102\n",
      "EPOCH=416\n",
      "train: loss=0.14991849183833425 acc=0.9478260869565217\n",
      "test: loss=0.17796856345370463 acc=0.9391304347826087\n",
      "EPOCH=417\n",
      "train: loss=0.1402543168986072 acc=0.9579710144927536\n",
      "test: loss=0.13656110335764357 acc=0.9536231884057971\n",
      "EPOCH=418\n",
      "train: loss=0.09026980573753594 acc=0.9768115942028985\n",
      "test: loss=0.13641828802620443 acc=0.9507246376811594\n",
      "EPOCH=419\n",
      "train: loss=0.10937405352613733 acc=0.9695652173913043\n",
      "test: loss=0.12963469725306256 acc=0.9521739130434783\n",
      "EPOCH=420\n",
      "train: loss=0.09230009947778998 acc=0.9768115942028985\n",
      "test: loss=0.1398869221390893 acc=0.9536231884057971\n",
      "EPOCH=421\n",
      "train: loss=0.06911832611478029 acc=0.9826086956521739\n",
      "test: loss=0.12492320279526752 acc=0.9507246376811594\n",
      "EPOCH=422\n",
      "train: loss=0.11129079316071933 acc=0.9637681159420289\n",
      "test: loss=0.13911261783885334 acc=0.9507246376811594\n",
      "EPOCH=423\n",
      "train: loss=0.1100385745969752 acc=0.9550724637681159\n",
      "test: loss=0.11832436652786281 acc=0.9623188405797102\n",
      "EPOCH=424\n",
      "train: loss=0.12378278815561487 acc=0.9681159420289855\n",
      "test: loss=0.14395880903968014 acc=0.9536231884057971\n",
      "EPOCH=425\n",
      "train: loss=0.09324847331408291 acc=0.9666666666666667\n",
      "test: loss=0.1237464852940775 acc=0.9594202898550724\n",
      "EPOCH=426\n",
      "train: loss=0.11092421049357074 acc=0.9623188405797102\n",
      "test: loss=0.1341677541987713 acc=0.946376811594203\n",
      "EPOCH=427\n",
      "train: loss=0.0944195085739222 acc=0.9739130434782609\n",
      "test: loss=0.12856052185055017 acc=0.9521739130434783\n",
      "EPOCH=428\n",
      "train: loss=0.09724402188081856 acc=0.972463768115942\n",
      "test: loss=0.12971908508455574 acc=0.9507246376811594\n",
      "EPOCH=429\n",
      "train: loss=0.09471067188673576 acc=0.9710144927536232\n",
      "test: loss=0.12476163584516552 acc=0.9579710144927536\n",
      "EPOCH=430\n",
      "train: loss=0.10102513829464363 acc=0.9768115942028985\n",
      "test: loss=0.12123477627321362 acc=0.9608695652173913\n",
      "EPOCH=431\n",
      "train: loss=0.12279397129305254 acc=0.9521739130434783\n",
      "test: loss=0.12332161237849656 acc=0.9579710144927536\n",
      "EPOCH=432\n",
      "train: loss=0.09859975349556302 acc=0.9753623188405797\n",
      "test: loss=0.1251015270395661 acc=0.9594202898550724\n",
      "EPOCH=433\n",
      "train: loss=0.10525774758852081 acc=0.9594202898550724\n",
      "test: loss=0.13038108134401621 acc=0.9478260869565217\n",
      "EPOCH=434\n",
      "train: loss=0.10348571468295098 acc=0.972463768115942\n",
      "test: loss=0.12294133103798653 acc=0.9550724637681159\n",
      "EPOCH=435\n",
      "train: loss=0.12173962176701987 acc=0.9565217391304348\n",
      "test: loss=0.11967328816913389 acc=0.9608695652173913\n",
      "EPOCH=436\n",
      "train: loss=0.11348244313530813 acc=0.9710144927536232\n",
      "test: loss=0.11988886932213096 acc=0.9652173913043478\n",
      "EPOCH=437\n",
      "train: loss=0.07686224362199857 acc=0.9768115942028985\n",
      "test: loss=0.11977950292730713 acc=0.9608695652173913\n",
      "EPOCH=438\n",
      "train: loss=0.11659151917768648 acc=0.9623188405797102\n",
      "test: loss=0.1393820114952329 acc=0.9579710144927536\n",
      "EPOCH=439\n",
      "train: loss=0.07979352356628558 acc=0.9739130434782609\n",
      "test: loss=0.10973980853539457 acc=0.9608695652173913\n",
      "EPOCH=440\n",
      "train: loss=0.09921706475410036 acc=0.9652173913043478\n",
      "test: loss=0.12454271063289143 acc=0.9565217391304348\n",
      "EPOCH=441\n",
      "train: loss=0.10821145518858673 acc=0.9652173913043478\n",
      "test: loss=0.12740257713191397 acc=0.9492753623188406\n",
      "EPOCH=442\n",
      "train: loss=0.11793893190805964 acc=0.9666666666666667\n",
      "test: loss=0.1175887474570243 acc=0.9623188405797102\n",
      "EPOCH=443\n",
      "train: loss=0.09575724606771256 acc=0.9797101449275363\n",
      "test: loss=0.12228673857176295 acc=0.9507246376811594\n",
      "EPOCH=444\n",
      "train: loss=0.09643482993047141 acc=0.9695652173913043\n",
      "test: loss=0.11164310239544192 acc=0.9710144927536232\n",
      "EPOCH=445\n",
      "train: loss=0.10076229023504403 acc=0.9666666666666667\n",
      "test: loss=0.11935703693701713 acc=0.9608695652173913\n",
      "EPOCH=446\n",
      "train: loss=0.10595449207200802 acc=0.9681159420289855\n",
      "test: loss=0.11347720494311916 acc=0.9608695652173913\n",
      "EPOCH=447\n",
      "train: loss=0.08069248027079111 acc=0.9797101449275363\n",
      "test: loss=0.11412465253644663 acc=0.9608695652173913\n",
      "EPOCH=448\n",
      "train: loss=0.0694081883106818 acc=0.9869565217391304\n",
      "test: loss=0.11224368612056423 acc=0.9550724637681159\n",
      "EPOCH=449\n",
      "train: loss=0.09957225969925618 acc=0.9710144927536232\n",
      "test: loss=0.11570146615238074 acc=0.9594202898550724\n",
      "EPOCH=450\n",
      "train: loss=0.08284411374752183 acc=0.9782608695652174\n",
      "test: loss=0.11701656809055268 acc=0.9637681159420289\n",
      "EPOCH=451\n",
      "train: loss=0.0737648790747336 acc=0.9797101449275363\n",
      "test: loss=0.11223849502499711 acc=0.9623188405797102\n",
      "EPOCH=452\n",
      "train: loss=0.1019373665908553 acc=0.9652173913043478\n",
      "test: loss=0.11391607068564712 acc=0.9579710144927536\n",
      "EPOCH=453\n",
      "train: loss=0.09479485938732925 acc=0.9681159420289855\n",
      "test: loss=0.10880962395304626 acc=0.9608695652173913\n",
      "EPOCH=454\n",
      "train: loss=0.07450404313239484 acc=0.9782608695652174\n",
      "test: loss=0.117144397263389 acc=0.9608695652173913\n",
      "EPOCH=455\n",
      "train: loss=0.10799205119997234 acc=0.9637681159420289\n",
      "test: loss=0.10802238407680972 acc=0.9594202898550724\n",
      "EPOCH=456\n",
      "train: loss=0.09524762158259192 acc=0.9753623188405797\n",
      "test: loss=0.11385566249834904 acc=0.9681159420289855\n",
      "EPOCH=457\n",
      "train: loss=0.0775796076117634 acc=0.9768115942028985\n",
      "test: loss=0.10831346317629925 acc=0.9652173913043478\n",
      "EPOCH=458\n",
      "train: loss=0.0875417569849336 acc=0.9739130434782609\n",
      "test: loss=0.11322016728695482 acc=0.9565217391304348\n",
      "EPOCH=459\n",
      "train: loss=0.08663725297077479 acc=0.9739130434782609\n",
      "test: loss=0.10553946526801736 acc=0.9695652173913043\n",
      "EPOCH=460\n",
      "train: loss=0.0635957799498325 acc=0.9826086956521739\n",
      "test: loss=0.11301623869402172 acc=0.9608695652173913\n",
      "EPOCH=461\n",
      "train: loss=0.08058306944063404 acc=0.9739130434782609\n",
      "test: loss=0.1061903323990477 acc=0.9637681159420289\n",
      "EPOCH=462\n",
      "train: loss=0.0754090869143603 acc=0.9768115942028985\n",
      "test: loss=0.11097884657686244 acc=0.9608695652173913\n",
      "EPOCH=463\n",
      "train: loss=0.08773609640078496 acc=0.9681159420289855\n",
      "test: loss=0.10566843849766185 acc=0.9666666666666667\n",
      "EPOCH=464\n",
      "train: loss=0.080496181021996 acc=0.9739130434782609\n",
      "test: loss=0.11298765991146813 acc=0.9637681159420289\n",
      "EPOCH=465\n",
      "train: loss=0.07048746472758868 acc=0.9869565217391304\n",
      "test: loss=0.10760367340614714 acc=0.9652173913043478\n",
      "EPOCH=466\n",
      "train: loss=0.1015556837546584 acc=0.9608695652173913\n",
      "test: loss=0.10300071923508135 acc=0.9681159420289855\n",
      "EPOCH=467\n",
      "train: loss=0.0658674970540371 acc=0.9768115942028985\n",
      "test: loss=0.10467010605648053 acc=0.9637681159420289\n",
      "EPOCH=468\n",
      "train: loss=0.08505710058114517 acc=0.9739130434782609\n",
      "test: loss=0.1047417882831338 acc=0.9608695652173913\n",
      "EPOCH=469\n",
      "train: loss=0.07717546664649239 acc=0.9782608695652174\n",
      "test: loss=0.10325368715553435 acc=0.9652173913043478\n",
      "EPOCH=470\n",
      "train: loss=0.08354165161833697 acc=0.9768115942028985\n",
      "test: loss=0.10966811865799014 acc=0.9623188405797102\n",
      "EPOCH=471\n",
      "train: loss=0.05668338726651675 acc=0.991304347826087\n",
      "test: loss=0.10318069838968122 acc=0.9681159420289855\n",
      "EPOCH=472\n",
      "train: loss=0.09293906359073291 acc=0.9681159420289855\n",
      "test: loss=0.11196016016732281 acc=0.9652173913043478\n",
      "EPOCH=473\n",
      "train: loss=0.08692086599331442 acc=0.9695652173913043\n",
      "test: loss=0.10677912257767845 acc=0.9623188405797102\n",
      "EPOCH=474\n",
      "train: loss=0.0982946631999595 acc=0.9666666666666667\n",
      "test: loss=0.11168528167585252 acc=0.9594202898550724\n",
      "EPOCH=475\n",
      "train: loss=0.07986083716157744 acc=0.9739130434782609\n",
      "test: loss=0.10348107284375233 acc=0.9652173913043478\n",
      "EPOCH=476\n",
      "train: loss=0.072713494333072 acc=0.9768115942028985\n",
      "test: loss=0.10573907415870198 acc=0.9608695652173913\n",
      "EPOCH=477\n",
      "train: loss=0.08716904847420678 acc=0.9652173913043478\n",
      "test: loss=0.10303517319503934 acc=0.9652173913043478\n",
      "EPOCH=478\n",
      "train: loss=0.07641997257300173 acc=0.9710144927536232\n",
      "test: loss=0.10428018439351351 acc=0.9637681159420289\n",
      "EPOCH=479\n",
      "train: loss=0.09593617067730124 acc=0.9637681159420289\n",
      "test: loss=0.10787836909066342 acc=0.9579710144927536\n",
      "EPOCH=480\n",
      "train: loss=0.05918373990938549 acc=0.9855072463768116\n",
      "test: loss=0.09964993520571176 acc=0.9637681159420289\n",
      "EPOCH=481\n",
      "train: loss=0.09531322526772174 acc=0.9710144927536232\n",
      "test: loss=0.10382562205392945 acc=0.9681159420289855\n",
      "EPOCH=482\n",
      "train: loss=0.10043723924212945 acc=0.9666666666666667\n",
      "test: loss=0.1295964083234561 acc=0.9579710144927536\n",
      "EPOCH=483\n",
      "train: loss=0.07190789320602002 acc=0.9768115942028985\n",
      "test: loss=0.10939409771671722 acc=0.9565217391304348\n",
      "EPOCH=484\n",
      "train: loss=0.10408159687613606 acc=0.9652173913043478\n",
      "test: loss=0.1294887287406149 acc=0.9507246376811594\n",
      "EPOCH=485\n",
      "train: loss=0.09460449301390247 acc=0.9681159420289855\n",
      "test: loss=0.13881753648077036 acc=0.9536231884057971\n",
      "EPOCH=486\n",
      "train: loss=0.06546916565723249 acc=0.972463768115942\n",
      "test: loss=0.11481472567183501 acc=0.9492753623188406\n",
      "EPOCH=487\n",
      "train: loss=0.08805176131658221 acc=0.9695652173913043\n",
      "test: loss=0.11600403507923279 acc=0.9550724637681159\n",
      "EPOCH=488\n",
      "train: loss=0.09327748028024845 acc=0.9608695652173913\n",
      "test: loss=0.10729927119798223 acc=0.9579710144927536\n",
      "EPOCH=489\n",
      "train: loss=0.06625601778485236 acc=0.981159420289855\n",
      "test: loss=0.10537749754777503 acc=0.9594202898550724\n",
      "EPOCH=490\n",
      "train: loss=0.07231279172068608 acc=0.9753623188405797\n",
      "test: loss=0.11224872787997085 acc=0.9637681159420289\n",
      "EPOCH=491\n",
      "train: loss=0.1095447092570629 acc=0.9536231884057971\n",
      "test: loss=0.11012628467415099 acc=0.9550724637681159\n",
      "EPOCH=492\n",
      "train: loss=0.22653567824757875 acc=0.9202898550724637\n",
      "test: loss=0.20569687746840407 acc=0.936231884057971\n",
      "EPOCH=493\n",
      "train: loss=0.10746119906897271 acc=0.9565217391304348\n",
      "test: loss=0.1272169009028706 acc=0.9565217391304348\n",
      "EPOCH=494\n",
      "train: loss=0.0973644276035693 acc=0.9695652173913043\n",
      "test: loss=0.1097618705079475 acc=0.9579710144927536\n",
      "EPOCH=495\n",
      "train: loss=0.2361739195243194 acc=0.8971014492753623\n",
      "test: loss=0.2967007507526307 acc=0.908695652173913\n",
      "EPOCH=496\n",
      "train: loss=0.0966171612440167 acc=0.9637681159420289\n",
      "test: loss=0.1202107154405759 acc=0.9550724637681159\n",
      "EPOCH=497\n",
      "train: loss=0.0917804350868221 acc=0.9681159420289855\n",
      "test: loss=0.1381906589459107 acc=0.9434782608695652\n",
      "EPOCH=498\n",
      "train: loss=0.1269798818281703 acc=0.9550724637681159\n",
      "test: loss=0.14729623426892102 acc=0.9391304347826087\n",
      "EPOCH=499\n",
      "train: loss=0.07813307287872447 acc=0.9797101449275363\n",
      "test: loss=0.10736394731467618 acc=0.9623188405797102\n",
      "EPOCH=500\n",
      "train: loss=0.10520957007584332 acc=0.9637681159420289\n",
      "test: loss=0.12503628675344902 acc=0.9478260869565217\n",
      "EPOCH=501\n",
      "train: loss=0.07703167729683542 acc=0.9739130434782609\n",
      "test: loss=0.09771218974207603 acc=0.9681159420289855\n",
      "EPOCH=502\n",
      "train: loss=0.09120656338821034 acc=0.972463768115942\n",
      "test: loss=0.12162026737571753 acc=0.9579710144927536\n",
      "EPOCH=503\n",
      "train: loss=0.07663413833865823 acc=0.9753623188405797\n",
      "test: loss=0.09949450185348513 acc=0.9623188405797102\n",
      "EPOCH=504\n",
      "train: loss=0.10625409415815475 acc=0.9579710144927536\n",
      "test: loss=0.10908866491637008 acc=0.9623188405797102\n",
      "EPOCH=505\n",
      "train: loss=0.10018909710016105 acc=0.9608695652173913\n",
      "test: loss=0.10765669095893485 acc=0.9550724637681159\n",
      "EPOCH=506\n",
      "train: loss=0.09947737913485169 acc=0.9652173913043478\n",
      "test: loss=0.10305602200458887 acc=0.9637681159420289\n",
      "EPOCH=507\n",
      "train: loss=0.08115156828914777 acc=0.972463768115942\n",
      "test: loss=0.10425557005443306 acc=0.9579710144927536\n",
      "EPOCH=508\n",
      "train: loss=0.0668026361321269 acc=0.9753623188405797\n",
      "test: loss=0.10203283338745088 acc=0.9739130434782609\n",
      "EPOCH=509\n",
      "train: loss=0.08254059303478434 acc=0.9666666666666667\n",
      "test: loss=0.10342438085993644 acc=0.9608695652173913\n",
      "EPOCH=510\n",
      "train: loss=0.07621237576446563 acc=0.9797101449275363\n",
      "test: loss=0.10431020716229261 acc=0.9594202898550724\n",
      "EPOCH=511\n",
      "train: loss=0.07445172877188086 acc=0.9739130434782609\n",
      "test: loss=0.09834125634047877 acc=0.9623188405797102\n",
      "EPOCH=512\n",
      "train: loss=0.10005082127123087 acc=0.9666666666666667\n",
      "test: loss=0.11003642802648472 acc=0.9550724637681159\n",
      "EPOCH=513\n",
      "train: loss=0.07525149801631678 acc=0.981159420289855\n",
      "test: loss=0.1029520640602384 acc=0.9652173913043478\n",
      "EPOCH=514\n",
      "train: loss=0.07644429394668291 acc=0.9768115942028985\n",
      "test: loss=0.10381529003791885 acc=0.9652173913043478\n",
      "EPOCH=515\n",
      "train: loss=0.06569665759254244 acc=0.9826086956521739\n",
      "test: loss=0.11414193671376496 acc=0.9565217391304348\n",
      "EPOCH=516\n",
      "train: loss=0.09072712172375703 acc=0.9666666666666667\n",
      "test: loss=0.1115933262503231 acc=0.9652173913043478\n",
      "EPOCH=517\n",
      "train: loss=0.06962493761840657 acc=0.9797101449275363\n",
      "test: loss=0.10760248419816619 acc=0.9637681159420289\n",
      "EPOCH=518\n",
      "train: loss=0.08135390233290678 acc=0.9666666666666667\n",
      "test: loss=0.10124542527394943 acc=0.9695652173913043\n",
      "EPOCH=519\n",
      "train: loss=0.0861918347769243 acc=0.9739130434782609\n",
      "test: loss=0.09793563382462728 acc=0.9681159420289855\n",
      "EPOCH=520\n",
      "train: loss=0.08709663250503047 acc=0.972463768115942\n",
      "test: loss=0.10097164372798308 acc=0.9608695652173913\n",
      "EPOCH=521\n",
      "train: loss=0.09247150341850227 acc=0.9695652173913043\n",
      "test: loss=0.09775890029441259 acc=0.9681159420289855\n",
      "EPOCH=522\n",
      "train: loss=0.08435430669193913 acc=0.9681159420289855\n",
      "test: loss=0.09287618261246 acc=0.9666666666666667\n",
      "EPOCH=523\n",
      "train: loss=0.09145861228777716 acc=0.9652173913043478\n",
      "test: loss=0.09953612126654879 acc=0.9666666666666667\n",
      "EPOCH=524\n",
      "train: loss=0.09603516446219044 acc=0.9652173913043478\n",
      "test: loss=0.09654109658342426 acc=0.9666666666666667\n",
      "EPOCH=525\n",
      "train: loss=0.08030301077831582 acc=0.9710144927536232\n",
      "test: loss=0.10016571306241959 acc=0.9594202898550724\n",
      "EPOCH=526\n",
      "train: loss=0.09573900157305142 acc=0.9695652173913043\n",
      "test: loss=0.10377249803152246 acc=0.9579710144927536\n",
      "EPOCH=527\n",
      "train: loss=0.0905400567648414 acc=0.9652173913043478\n",
      "test: loss=0.09850696370550376 acc=0.9637681159420289\n",
      "EPOCH=528\n",
      "train: loss=0.06574867602633382 acc=0.9782608695652174\n",
      "test: loss=0.09791221913486735 acc=0.9681159420289855\n",
      "EPOCH=529\n",
      "train: loss=0.07387433715607025 acc=0.9695652173913043\n",
      "test: loss=0.09667177143564189 acc=0.9666666666666667\n",
      "EPOCH=530\n",
      "train: loss=0.07506547398384693 acc=0.9739130434782609\n",
      "test: loss=0.10262308586695847 acc=0.9550724637681159\n",
      "EPOCH=531\n",
      "train: loss=0.08782499185696735 acc=0.9710144927536232\n",
      "test: loss=0.09404227271770882 acc=0.9666666666666667\n",
      "EPOCH=532\n",
      "train: loss=0.06866017851037055 acc=0.9710144927536232\n",
      "test: loss=0.10255230922889924 acc=0.9637681159420289\n",
      "EPOCH=533\n",
      "train: loss=0.0731502945043415 acc=0.9753623188405797\n",
      "test: loss=0.09631468966359433 acc=0.9739130434782609\n",
      "EPOCH=534\n",
      "train: loss=0.06480335349768118 acc=0.9753623188405797\n",
      "test: loss=0.09057434181003335 acc=0.972463768115942\n",
      "EPOCH=535\n",
      "train: loss=0.09110581374115811 acc=0.9695652173913043\n",
      "test: loss=0.09427907564771873 acc=0.9637681159420289\n",
      "EPOCH=536\n",
      "train: loss=0.06650446543491913 acc=0.9768115942028985\n",
      "test: loss=0.09116924700430432 acc=0.9652173913043478\n",
      "EPOCH=537\n",
      "train: loss=0.0634304928451687 acc=0.9782608695652174\n",
      "test: loss=0.10169850706326981 acc=0.9565217391304348\n",
      "EPOCH=538\n",
      "train: loss=0.07212181976859809 acc=0.9782608695652174\n",
      "test: loss=0.09574134406779775 acc=0.9637681159420289\n",
      "EPOCH=539\n",
      "train: loss=0.06752350132918833 acc=0.9797101449275363\n",
      "test: loss=0.09198795673089881 acc=0.9666666666666667\n",
      "EPOCH=540\n",
      "train: loss=0.0902367717833369 acc=0.9594202898550724\n",
      "test: loss=0.09611245040763775 acc=0.9695652173913043\n",
      "EPOCH=541\n",
      "train: loss=0.09115427141530752 acc=0.9608695652173913\n",
      "test: loss=0.09235138550721139 acc=0.972463768115942\n",
      "EPOCH=542\n",
      "train: loss=0.07033119098522204 acc=0.972463768115942\n",
      "test: loss=0.09283842133694357 acc=0.972463768115942\n",
      "EPOCH=543\n",
      "train: loss=0.11414407551107181 acc=0.9550724637681159\n",
      "test: loss=0.09020816663300521 acc=0.9681159420289855\n",
      "EPOCH=544\n",
      "train: loss=0.09338673996920877 acc=0.9608695652173913\n",
      "test: loss=0.10045035852364982 acc=0.9608695652173913\n",
      "EPOCH=545\n",
      "train: loss=0.06883489747889057 acc=0.972463768115942\n",
      "test: loss=0.09280228221018035 acc=0.9681159420289855\n",
      "EPOCH=546\n",
      "train: loss=0.05754107433357953 acc=0.9840579710144928\n",
      "test: loss=0.09227504643880148 acc=0.9637681159420289\n",
      "EPOCH=547\n",
      "train: loss=0.06795971693082535 acc=0.9768115942028985\n",
      "test: loss=0.09530164693267965 acc=0.9637681159420289\n",
      "EPOCH=548\n",
      "train: loss=0.07664487366474414 acc=0.9695652173913043\n",
      "test: loss=0.09100697680691652 acc=0.9695652173913043\n",
      "EPOCH=549\n",
      "train: loss=0.0714503597428166 acc=0.9739130434782609\n",
      "test: loss=0.09474753534172654 acc=0.9681159420289855\n",
      "EPOCH=550\n",
      "train: loss=0.05587218813577017 acc=0.9884057971014493\n",
      "test: loss=0.08916327719722321 acc=0.9666666666666667\n",
      "EPOCH=551\n",
      "train: loss=0.0943986181944595 acc=0.9652173913043478\n",
      "test: loss=0.09125361484054545 acc=0.9637681159420289\n",
      "EPOCH=552\n",
      "train: loss=0.09923157805728212 acc=0.9579710144927536\n",
      "test: loss=0.0895126403752964 acc=0.9681159420289855\n",
      "EPOCH=553\n",
      "train: loss=0.0819887146020096 acc=0.9666666666666667\n",
      "test: loss=0.0919564851437192 acc=0.9637681159420289\n",
      "EPOCH=554\n",
      "train: loss=0.07381965496270869 acc=0.9753623188405797\n",
      "test: loss=0.08948971990180968 acc=0.9637681159420289\n",
      "EPOCH=555\n",
      "train: loss=0.10753334088519421 acc=0.9492753623188406\n",
      "test: loss=0.09086073095305401 acc=0.9695652173913043\n",
      "EPOCH=556\n",
      "train: loss=0.0630481004516106 acc=0.9782608695652174\n",
      "test: loss=0.08758835502468944 acc=0.9695652173913043\n",
      "EPOCH=557\n",
      "train: loss=0.06857136957979604 acc=0.9768115942028985\n",
      "test: loss=0.08722783182508426 acc=0.9710144927536232\n",
      "EPOCH=558\n",
      "train: loss=0.07673507570895076 acc=0.9695652173913043\n",
      "test: loss=0.0927211824531818 acc=0.9739130434782609\n",
      "EPOCH=559\n",
      "train: loss=0.06586944578590057 acc=0.9739130434782609\n",
      "test: loss=0.0894604848669228 acc=0.9652173913043478\n",
      "EPOCH=560\n",
      "train: loss=0.07720620227006339 acc=0.9753623188405797\n",
      "test: loss=0.092124874011933 acc=0.9666666666666667\n",
      "EPOCH=561\n",
      "train: loss=0.056258160413194605 acc=0.981159420289855\n",
      "test: loss=0.0921800359732357 acc=0.9695652173913043\n",
      "EPOCH=562\n",
      "train: loss=0.09114310399170623 acc=0.9637681159420289\n",
      "test: loss=0.08614179973465219 acc=0.972463768115942\n",
      "EPOCH=563\n",
      "train: loss=0.07780929686152337 acc=0.9739130434782609\n",
      "test: loss=0.0903868382413326 acc=0.9666666666666667\n",
      "EPOCH=564\n",
      "train: loss=0.07846206554984637 acc=0.9681159420289855\n",
      "test: loss=0.08916380693420138 acc=0.9666666666666667\n",
      "EPOCH=565\n",
      "train: loss=0.06905127181104839 acc=0.972463768115942\n",
      "test: loss=0.08780375621658275 acc=0.9681159420289855\n",
      "EPOCH=566\n",
      "train: loss=0.07709096370832196 acc=0.9753623188405797\n",
      "test: loss=0.08709671730324363 acc=0.9695652173913043\n",
      "EPOCH=567\n",
      "train: loss=0.058868820499883345 acc=0.9840579710144928\n",
      "test: loss=0.08639899187402456 acc=0.9695652173913043\n",
      "EPOCH=568\n",
      "train: loss=0.07362831640576468 acc=0.9695652173913043\n",
      "test: loss=0.08702282483957696 acc=0.9695652173913043\n",
      "EPOCH=569\n",
      "train: loss=0.07022977619994956 acc=0.9710144927536232\n",
      "test: loss=0.08981565365349822 acc=0.9666666666666667\n",
      "EPOCH=570\n",
      "train: loss=0.07807718492269447 acc=0.9681159420289855\n",
      "test: loss=0.08728343714057216 acc=0.9666666666666667\n",
      "EPOCH=571\n",
      "train: loss=0.05410165378511607 acc=0.981159420289855\n",
      "test: loss=0.08480065733465347 acc=0.9695652173913043\n",
      "EPOCH=572\n",
      "train: loss=0.06403698349682663 acc=0.9797101449275363\n",
      "test: loss=0.09033819152650392 acc=0.9666666666666667\n",
      "EPOCH=573\n",
      "train: loss=0.07442546882225905 acc=0.9739130434782609\n",
      "test: loss=0.08596769760793292 acc=0.9681159420289855\n",
      "EPOCH=574\n",
      "train: loss=0.05268511948490456 acc=0.9826086956521739\n",
      "test: loss=0.08493566299129751 acc=0.9623188405797102\n",
      "EPOCH=575\n",
      "train: loss=0.06763905192294682 acc=0.9753623188405797\n",
      "test: loss=0.0873067437515224 acc=0.9666666666666667\n",
      "EPOCH=576\n",
      "train: loss=0.08208063041088083 acc=0.9594202898550724\n",
      "test: loss=0.08894703957287253 acc=0.9666666666666667\n",
      "EPOCH=577\n",
      "train: loss=0.07358073112660857 acc=0.9710144927536232\n",
      "test: loss=0.08779700636281355 acc=0.9652173913043478\n",
      "EPOCH=578\n",
      "train: loss=0.08734748522601152 acc=0.9652173913043478\n",
      "test: loss=0.08478878017956586 acc=0.9652173913043478\n",
      "EPOCH=579\n",
      "train: loss=0.05710053449834379 acc=0.9753623188405797\n",
      "test: loss=0.08864322581005514 acc=0.9637681159420289\n",
      "EPOCH=580\n",
      "train: loss=0.1047235017132284 acc=0.9594202898550724\n",
      "test: loss=0.0873539179343895 acc=0.9623188405797102\n",
      "EPOCH=581\n",
      "train: loss=0.07179703684738174 acc=0.9782608695652174\n",
      "test: loss=0.0869130372476941 acc=0.9739130434782609\n",
      "EPOCH=582\n",
      "train: loss=0.06926992034976096 acc=0.9695652173913043\n",
      "test: loss=0.09054375927916782 acc=0.9652173913043478\n",
      "EPOCH=583\n",
      "train: loss=0.06905370421568098 acc=0.9768115942028985\n",
      "test: loss=0.08353143047190663 acc=0.9695652173913043\n",
      "EPOCH=584\n",
      "train: loss=0.07818590212680868 acc=0.9681159420289855\n",
      "test: loss=0.08998367180891692 acc=0.972463768115942\n",
      "EPOCH=585\n",
      "train: loss=0.043918452590993924 acc=0.9869565217391304\n",
      "test: loss=0.08373704160469919 acc=0.9637681159420289\n",
      "EPOCH=586\n",
      "train: loss=0.06249724066668227 acc=0.9768115942028985\n",
      "test: loss=0.08371864031409687 acc=0.9681159420289855\n",
      "EPOCH=587\n",
      "train: loss=0.0655921650763666 acc=0.9753623188405797\n",
      "test: loss=0.08603748179904945 acc=0.9666666666666667\n",
      "EPOCH=588\n",
      "train: loss=0.07869281606795175 acc=0.972463768115942\n",
      "test: loss=0.08620769221673112 acc=0.972463768115942\n",
      "EPOCH=589\n",
      "train: loss=0.05845191179056799 acc=0.981159420289855\n",
      "test: loss=0.08217635508589924 acc=0.9681159420289855\n",
      "EPOCH=590\n",
      "train: loss=0.07125403277976763 acc=0.9739130434782609\n",
      "test: loss=0.08505271574698967 acc=0.9637681159420289\n",
      "EPOCH=591\n",
      "train: loss=0.06598683990838217 acc=0.9782608695652174\n",
      "test: loss=0.08181214526847341 acc=0.9666666666666667\n",
      "EPOCH=592\n",
      "train: loss=0.10141286382276582 acc=0.946376811594203\n",
      "test: loss=0.08213858772864875 acc=0.9753623188405797\n",
      "EPOCH=593\n",
      "train: loss=0.07434683521393784 acc=0.9710144927536232\n",
      "test: loss=0.08201535131196104 acc=0.9753623188405797\n",
      "EPOCH=594\n",
      "train: loss=0.07469814847702211 acc=0.9681159420289855\n",
      "test: loss=0.08286272043539589 acc=0.9652173913043478\n",
      "EPOCH=595\n",
      "train: loss=0.07030846194498518 acc=0.9652173913043478\n",
      "test: loss=0.08492138858350519 acc=0.9695652173913043\n",
      "EPOCH=596\n",
      "train: loss=0.06200113838770785 acc=0.9782608695652174\n",
      "test: loss=0.08611575424818699 acc=0.9681159420289855\n",
      "EPOCH=597\n",
      "train: loss=0.06322463139153996 acc=0.981159420289855\n",
      "test: loss=0.08048445051593137 acc=0.9710144927536232\n",
      "EPOCH=598\n",
      "train: loss=0.07481133233428636 acc=0.9681159420289855\n",
      "test: loss=0.08212444318591985 acc=0.972463768115942\n",
      "EPOCH=599\n",
      "train: loss=0.06296422829843999 acc=0.981159420289855\n",
      "test: loss=0.0799315423090419 acc=0.9681159420289855\n",
      "EPOCH=600\n",
      "train: loss=0.07039462025954923 acc=0.9695652173913043\n",
      "test: loss=0.08070602371802024 acc=0.972463768115942\n",
      "EPOCH=601\n",
      "train: loss=0.07374435650930951 acc=0.9695652173913043\n",
      "test: loss=0.08102117960694419 acc=0.9695652173913043\n",
      "EPOCH=602\n",
      "train: loss=0.06395278536123676 acc=0.972463768115942\n",
      "test: loss=0.08405137804918895 acc=0.9637681159420289\n",
      "EPOCH=603\n",
      "train: loss=0.07749424036374708 acc=0.9710144927536232\n",
      "test: loss=0.08244666340017573 acc=0.9681159420289855\n",
      "EPOCH=604\n",
      "train: loss=0.0864607683383267 acc=0.9739130434782609\n",
      "test: loss=0.07835483748514652 acc=0.9710144927536232\n",
      "EPOCH=605\n",
      "train: loss=0.08799097931537446 acc=0.9623188405797102\n",
      "test: loss=0.08108388861679006 acc=0.9637681159420289\n",
      "EPOCH=606\n",
      "train: loss=0.046422152289717626 acc=0.9840579710144928\n",
      "test: loss=0.07839691431754463 acc=0.9739130434782609\n",
      "EPOCH=607\n",
      "train: loss=0.06835812924841396 acc=0.9695652173913043\n",
      "test: loss=0.08172752289560423 acc=0.9681159420289855\n",
      "EPOCH=608\n",
      "train: loss=0.04066779752741593 acc=0.9927536231884058\n",
      "test: loss=0.07825025190244196 acc=0.9768115942028985\n",
      "EPOCH=609\n",
      "train: loss=0.05396416142797175 acc=0.9753623188405797\n",
      "test: loss=0.08185204093592628 acc=0.9681159420289855\n",
      "EPOCH=610\n",
      "train: loss=0.05896937646170897 acc=0.972463768115942\n",
      "test: loss=0.077773630956936 acc=0.972463768115942\n",
      "EPOCH=611\n",
      "train: loss=0.06614700815945669 acc=0.9710144927536232\n",
      "test: loss=0.08272263412475075 acc=0.9739130434782609\n",
      "EPOCH=612\n",
      "train: loss=0.05171519266296192 acc=0.981159420289855\n",
      "test: loss=0.0791267033765714 acc=0.9695652173913043\n",
      "EPOCH=613\n",
      "train: loss=0.05847746194582837 acc=0.9782608695652174\n",
      "test: loss=0.07820818950793522 acc=0.9753623188405797\n",
      "EPOCH=614\n",
      "train: loss=0.06694553552688977 acc=0.9681159420289855\n",
      "test: loss=0.08172419979655833 acc=0.9681159420289855\n",
      "EPOCH=615\n",
      "train: loss=0.05197943995369159 acc=0.9840579710144928\n",
      "test: loss=0.07774906693967623 acc=0.9695652173913043\n",
      "EPOCH=616\n",
      "train: loss=0.08414381804770311 acc=0.9608695652173913\n",
      "test: loss=0.08167547610674333 acc=0.9652173913043478\n",
      "EPOCH=617\n",
      "train: loss=0.06281672962546135 acc=0.9782608695652174\n",
      "test: loss=0.07944357117362366 acc=0.972463768115942\n",
      "EPOCH=618\n",
      "train: loss=0.07120167416343029 acc=0.9768115942028985\n",
      "test: loss=0.08089409262419105 acc=0.9637681159420289\n",
      "EPOCH=619\n",
      "train: loss=0.06429055957956023 acc=0.9710144927536232\n",
      "test: loss=0.07541194188866004 acc=0.9739130434782609\n",
      "EPOCH=620\n",
      "train: loss=0.0517601016384749 acc=0.9840579710144928\n",
      "test: loss=0.0772353376388526 acc=0.9753623188405797\n",
      "EPOCH=621\n",
      "train: loss=0.06358286068600802 acc=0.9826086956521739\n",
      "test: loss=0.0803084211843414 acc=0.9753623188405797\n",
      "EPOCH=622\n",
      "train: loss=0.04145542461918826 acc=0.9840579710144928\n",
      "test: loss=0.07572375012492406 acc=0.9695652173913043\n",
      "EPOCH=623\n",
      "train: loss=0.05449647601362129 acc=0.981159420289855\n",
      "test: loss=0.07822130656630104 acc=0.9695652173913043\n",
      "EPOCH=624\n",
      "train: loss=0.037003087946514704 acc=0.9855072463768116\n",
      "test: loss=0.07654936422942077 acc=0.9652173913043478\n",
      "EPOCH=625\n",
      "train: loss=0.05828293452794578 acc=0.9768115942028985\n",
      "test: loss=0.08276964641161166 acc=0.9652173913043478\n",
      "EPOCH=626\n",
      "train: loss=0.07013826481341895 acc=0.9710144927536232\n",
      "test: loss=0.07963762465399485 acc=0.9695652173913043\n",
      "EPOCH=627\n",
      "train: loss=0.07606589208341671 acc=0.9681159420289855\n",
      "test: loss=0.07810777818960615 acc=0.972463768115942\n",
      "EPOCH=628\n",
      "train: loss=0.08251068016184258 acc=0.9594202898550724\n",
      "test: loss=0.07702972237588267 acc=0.9695652173913043\n",
      "EPOCH=629\n",
      "train: loss=0.05777845523197117 acc=0.981159420289855\n",
      "test: loss=0.07634531025541674 acc=0.9739130434782609\n",
      "EPOCH=630\n",
      "train: loss=0.04528005185314995 acc=0.9826086956521739\n",
      "test: loss=0.07939576546937104 acc=0.9681159420289855\n",
      "EPOCH=631\n",
      "train: loss=0.05728900385159223 acc=0.9782608695652174\n",
      "test: loss=0.0777705202514013 acc=0.9652173913043478\n",
      "EPOCH=632\n",
      "train: loss=0.05686537911869588 acc=0.981159420289855\n",
      "test: loss=0.07588383490358357 acc=0.9710144927536232\n",
      "EPOCH=633\n",
      "train: loss=0.07448411045486768 acc=0.9681159420289855\n",
      "test: loss=0.07627916593645391 acc=0.972463768115942\n",
      "EPOCH=634\n",
      "train: loss=0.062130043946470356 acc=0.9695652173913043\n",
      "test: loss=0.07219683187546834 acc=0.9681159420289855\n",
      "EPOCH=635\n",
      "train: loss=0.08020305239139183 acc=0.9608695652173913\n",
      "test: loss=0.07738611775284081 acc=0.9681159420289855\n",
      "EPOCH=636\n",
      "train: loss=0.06115430041276764 acc=0.9782608695652174\n",
      "test: loss=0.0757337061356616 acc=0.9652173913043478\n",
      "EPOCH=637\n",
      "train: loss=0.039661370897166606 acc=0.991304347826087\n",
      "test: loss=0.07785875779825577 acc=0.9710144927536232\n",
      "EPOCH=638\n",
      "train: loss=0.051590018111373356 acc=0.9753623188405797\n",
      "test: loss=0.07762420802066103 acc=0.972463768115942\n",
      "EPOCH=639\n",
      "train: loss=0.061805182060742404 acc=0.9753623188405797\n",
      "test: loss=0.07717661774507688 acc=0.9710144927536232\n",
      "EPOCH=640\n",
      "train: loss=0.054035717689343926 acc=0.9797101449275363\n",
      "test: loss=0.07461762209703643 acc=0.972463768115942\n",
      "EPOCH=641\n",
      "train: loss=0.049684359369424835 acc=0.9782608695652174\n",
      "test: loss=0.0781661679087503 acc=0.9681159420289855\n",
      "EPOCH=642\n",
      "train: loss=0.048212912014002504 acc=0.9826086956521739\n",
      "test: loss=0.07336280966880945 acc=0.9753623188405797\n",
      "EPOCH=643\n",
      "train: loss=0.0661843635881076 acc=0.9739130434782609\n",
      "test: loss=0.07819355444700027 acc=0.972463768115942\n",
      "EPOCH=644\n",
      "train: loss=0.04592466318688684 acc=0.9840579710144928\n",
      "test: loss=0.07411203707737261 acc=0.9681159420289855\n",
      "EPOCH=645\n",
      "train: loss=0.06564818789197756 acc=0.9797101449275363\n",
      "test: loss=0.07890927609758931 acc=0.9666666666666667\n",
      "EPOCH=646\n",
      "train: loss=0.05890098172864619 acc=0.9739130434782609\n",
      "test: loss=0.07727604641034796 acc=0.9637681159420289\n",
      "EPOCH=647\n",
      "train: loss=0.04572945438392084 acc=0.9826086956521739\n",
      "test: loss=0.07654589852177766 acc=0.9739130434782609\n",
      "EPOCH=648\n",
      "train: loss=0.04996447956736922 acc=0.9826086956521739\n",
      "test: loss=0.07217629823839478 acc=0.972463768115942\n",
      "EPOCH=649\n",
      "train: loss=0.07619931929479376 acc=0.972463768115942\n",
      "test: loss=0.07132157150139205 acc=0.9739130434782609\n",
      "EPOCH=650\n",
      "train: loss=0.05415697068026178 acc=0.9797101449275363\n",
      "test: loss=0.0749673657171842 acc=0.9681159420289855\n",
      "EPOCH=651\n",
      "train: loss=0.05375831497312144 acc=0.9768115942028985\n",
      "test: loss=0.07644833874338841 acc=0.9695652173913043\n",
      "EPOCH=652\n",
      "train: loss=0.06538227618536874 acc=0.972463768115942\n",
      "test: loss=0.07200663830750953 acc=0.9753623188405797\n",
      "EPOCH=653\n",
      "train: loss=0.05271726972803822 acc=0.9826086956521739\n",
      "test: loss=0.0713420332650367 acc=0.9739130434782609\n",
      "EPOCH=654\n",
      "train: loss=0.06698648306986157 acc=0.9652173913043478\n",
      "test: loss=0.0730256407738657 acc=0.9753623188405797\n",
      "EPOCH=655\n",
      "train: loss=0.06704221290141268 acc=0.9710144927536232\n",
      "test: loss=0.07509133720146105 acc=0.9753623188405797\n",
      "EPOCH=656\n",
      "train: loss=0.08306005997812191 acc=0.9623188405797102\n",
      "test: loss=0.07439296724029894 acc=0.9739130434782609\n",
      "EPOCH=657\n",
      "train: loss=0.060251564559752974 acc=0.9739130434782609\n",
      "test: loss=0.07312806364476114 acc=0.9739130434782609\n",
      "EPOCH=658\n",
      "train: loss=0.0634708796480752 acc=0.9797101449275363\n",
      "test: loss=0.07340180575871677 acc=0.9681159420289855\n",
      "EPOCH=659\n",
      "train: loss=0.09056601750102847 acc=0.9608695652173913\n",
      "test: loss=0.07283074767172179 acc=0.9710144927536232\n",
      "EPOCH=660\n",
      "train: loss=0.0551389662935785 acc=0.981159420289855\n",
      "test: loss=0.07552420356424798 acc=0.972463768115942\n",
      "EPOCH=661\n",
      "train: loss=0.04741822492364531 acc=0.9826086956521739\n",
      "test: loss=0.07298826966842951 acc=0.9681159420289855\n",
      "EPOCH=662\n",
      "train: loss=0.04096340132962905 acc=0.9855072463768116\n",
      "test: loss=0.07130249972933107 acc=0.9710144927536232\n",
      "EPOCH=663\n",
      "train: loss=0.07742224013344536 acc=0.9594202898550724\n",
      "test: loss=0.07397073883744512 acc=0.9739130434782609\n",
      "EPOCH=664\n",
      "train: loss=0.05913980595758142 acc=0.9753623188405797\n",
      "test: loss=0.07026092877281141 acc=0.972463768115942\n",
      "EPOCH=665\n",
      "train: loss=0.05729742809297935 acc=0.9739130434782609\n",
      "test: loss=0.0722828940133869 acc=0.9681159420289855\n",
      "EPOCH=666\n",
      "train: loss=0.04405683598950544 acc=0.9840579710144928\n",
      "test: loss=0.0738606542778974 acc=0.9695652173913043\n",
      "EPOCH=667\n",
      "train: loss=0.04587262246493814 acc=0.9826086956521739\n",
      "test: loss=0.0719496353338808 acc=0.9695652173913043\n",
      "EPOCH=668\n",
      "train: loss=0.06619047326049486 acc=0.9666666666666667\n",
      "test: loss=0.07439824924370263 acc=0.9753623188405797\n",
      "EPOCH=669\n",
      "train: loss=0.08755841447285297 acc=0.9565217391304348\n",
      "test: loss=0.0704413155749931 acc=0.9695652173913043\n",
      "EPOCH=670\n",
      "train: loss=0.036421348104127234 acc=0.9855072463768116\n",
      "test: loss=0.06837972445119107 acc=0.9681159420289855\n",
      "EPOCH=671\n",
      "train: loss=0.07279509568483374 acc=0.9681159420289855\n",
      "test: loss=0.07025162810533964 acc=0.9739130434782609\n",
      "EPOCH=672\n",
      "train: loss=0.061909412807554326 acc=0.9753623188405797\n",
      "test: loss=0.07486979223309127 acc=0.9666666666666667\n",
      "EPOCH=673\n",
      "train: loss=0.04686368938363519 acc=0.9826086956521739\n",
      "test: loss=0.06964586150977008 acc=0.9681159420289855\n",
      "EPOCH=674\n",
      "train: loss=0.07826142634920125 acc=0.9666666666666667\n",
      "test: loss=0.07254868976396824 acc=0.9695652173913043\n",
      "EPOCH=675\n",
      "train: loss=0.03620224236850107 acc=0.9869565217391304\n",
      "test: loss=0.06918396059847041 acc=0.9710144927536232\n",
      "EPOCH=676\n",
      "train: loss=0.07301953609355555 acc=0.972463768115942\n",
      "test: loss=0.06907481525809975 acc=0.9710144927536232\n",
      "EPOCH=677\n",
      "train: loss=0.04808660243904451 acc=0.9782608695652174\n",
      "test: loss=0.06997198512050373 acc=0.9710144927536232\n",
      "EPOCH=678\n",
      "train: loss=0.06369418280747662 acc=0.9768115942028985\n",
      "test: loss=0.06961536967900547 acc=0.9695652173913043\n",
      "EPOCH=679\n",
      "train: loss=0.03139049865904625 acc=0.9942028985507246\n",
      "test: loss=0.07172378863001491 acc=0.9681159420289855\n",
      "EPOCH=680\n",
      "train: loss=0.07042974705674707 acc=0.9695652173913043\n",
      "test: loss=0.07077375266656405 acc=0.9695652173913043\n",
      "EPOCH=681\n",
      "train: loss=0.06096480695044389 acc=0.9739130434782609\n",
      "test: loss=0.06660340245063175 acc=0.972463768115942\n",
      "EPOCH=682\n",
      "train: loss=0.0620374047204771 acc=0.9797101449275363\n",
      "test: loss=0.07066426166120401 acc=0.9695652173913043\n",
      "EPOCH=683\n",
      "train: loss=0.03498797497770644 acc=0.9826086956521739\n",
      "test: loss=0.06808531208008409 acc=0.9710144927536232\n",
      "EPOCH=684\n",
      "train: loss=0.07619087268925832 acc=0.9608695652173913\n",
      "test: loss=0.06975451417465328 acc=0.9739130434782609\n",
      "EPOCH=685\n",
      "train: loss=0.03363844355992627 acc=0.9855072463768116\n",
      "test: loss=0.07104556742069285 acc=0.9710144927536232\n",
      "EPOCH=686\n",
      "train: loss=0.05376154110140182 acc=0.9826086956521739\n",
      "test: loss=0.06827161326747598 acc=0.972463768115942\n",
      "EPOCH=687\n",
      "train: loss=0.048874050184500355 acc=0.981159420289855\n",
      "test: loss=0.07061480189355511 acc=0.9666666666666667\n",
      "EPOCH=688\n",
      "train: loss=0.056811503197027574 acc=0.9710144927536232\n",
      "test: loss=0.06898548270880704 acc=0.972463768115942\n",
      "EPOCH=689\n",
      "train: loss=0.04338276048181906 acc=0.9826086956521739\n",
      "test: loss=0.0685140592028696 acc=0.9739130434782609\n",
      "EPOCH=690\n",
      "train: loss=0.045339722586573476 acc=0.9826086956521739\n",
      "test: loss=0.06754187660355859 acc=0.972463768115942\n",
      "EPOCH=691\n",
      "train: loss=0.05900564440377003 acc=0.972463768115942\n",
      "test: loss=0.07199600522959457 acc=0.9695652173913043\n",
      "EPOCH=692\n",
      "train: loss=0.0444535036553564 acc=0.9739130434782609\n",
      "test: loss=0.06603596438540962 acc=0.972463768115942\n",
      "EPOCH=693\n",
      "train: loss=0.08178750853422911 acc=0.9608695652173913\n",
      "test: loss=0.08064515691493496 acc=0.9695652173913043\n",
      "EPOCH=694\n",
      "train: loss=0.18271622067236504 acc=0.9217391304347826\n",
      "test: loss=0.1689025435995542 acc=0.9376811594202898\n",
      "EPOCH=695\n",
      "train: loss=0.13261989910759964 acc=0.9652173913043478\n",
      "test: loss=0.19407726954731203 acc=0.9333333333333333\n",
      "EPOCH=696\n",
      "train: loss=0.5613721136303113 acc=0.8376811594202899\n",
      "test: loss=0.4451103645295471 acc=0.8623188405797102\n",
      "EPOCH=697\n",
      "train: loss=0.22247168307491413 acc=0.9260869565217391\n",
      "test: loss=0.2582451162364713 acc=0.9072463768115943\n",
      "EPOCH=698\n",
      "train: loss=0.16280688692310036 acc=0.9405797101449276\n",
      "test: loss=0.1930565770425745 acc=0.9333333333333333\n",
      "EPOCH=699\n",
      "train: loss=0.21585485505530888 acc=0.9173913043478261\n",
      "test: loss=0.25367529819729895 acc=0.8913043478260869\n",
      "EPOCH=700\n",
      "train: loss=0.21796439366606843 acc=0.9318840579710145\n",
      "test: loss=0.25446066591456146 acc=0.9246376811594202\n",
      "EPOCH=701\n",
      "train: loss=0.20208043961833064 acc=0.9159420289855073\n",
      "test: loss=0.2438493894670746 acc=0.9072463768115943\n",
      "EPOCH=702\n",
      "train: loss=0.1093414519745743 acc=0.9608695652173913\n",
      "test: loss=0.15455097341808635 acc=0.936231884057971\n",
      "EPOCH=703\n",
      "train: loss=0.08328847608764603 acc=0.9753623188405797\n",
      "test: loss=0.113122500325183 acc=0.9521739130434783\n",
      "EPOCH=704\n",
      "train: loss=0.06861382876423511 acc=0.9695652173913043\n",
      "test: loss=0.09823482725517296 acc=0.9637681159420289\n",
      "EPOCH=705\n",
      "train: loss=0.0961677588630812 acc=0.9666666666666667\n",
      "test: loss=0.1466777637634923 acc=0.9391304347826087\n",
      "EPOCH=706\n",
      "train: loss=0.07397854507638958 acc=0.981159420289855\n",
      "test: loss=0.11612576914862667 acc=0.9536231884057971\n",
      "EPOCH=707\n",
      "train: loss=0.06564433809472774 acc=0.9753623188405797\n",
      "test: loss=0.10403223689531187 acc=0.9565217391304348\n",
      "EPOCH=708\n",
      "train: loss=0.08343702471268549 acc=0.9637681159420289\n",
      "test: loss=0.09886985480051115 acc=0.9594202898550724\n",
      "EPOCH=709\n",
      "train: loss=0.060740027367965306 acc=0.9782608695652174\n",
      "test: loss=0.08847440410257197 acc=0.9681159420289855\n",
      "EPOCH=710\n",
      "train: loss=0.06128675399414707 acc=0.981159420289855\n",
      "test: loss=0.08569540978595935 acc=0.9608695652173913\n",
      "EPOCH=711\n",
      "train: loss=0.08150758670266514 acc=0.9695652173913043\n",
      "test: loss=0.10023512196213329 acc=0.9521739130434783\n",
      "EPOCH=712\n",
      "train: loss=0.06240790206099685 acc=0.9782608695652174\n",
      "test: loss=0.08807260367867474 acc=0.9637681159420289\n",
      "EPOCH=713\n",
      "train: loss=0.05968058429625477 acc=0.981159420289855\n",
      "test: loss=0.10810431633585184 acc=0.9565217391304348\n",
      "EPOCH=714\n",
      "train: loss=0.10170409104715637 acc=0.9608695652173913\n",
      "test: loss=0.08978888264437693 acc=0.9652173913043478\n",
      "EPOCH=715\n",
      "train: loss=0.06534514184109365 acc=0.972463768115942\n",
      "test: loss=0.08608256953336214 acc=0.9637681159420289\n",
      "EPOCH=716\n",
      "train: loss=0.08362135402266942 acc=0.9652173913043478\n",
      "test: loss=0.0870673369755272 acc=0.9652173913043478\n",
      "EPOCH=717\n",
      "train: loss=0.07607098886486499 acc=0.9797101449275363\n",
      "test: loss=0.09185595892200392 acc=0.9666666666666667\n",
      "EPOCH=718\n",
      "train: loss=0.07525812070403401 acc=0.9695652173913043\n",
      "test: loss=0.09675263256659439 acc=0.9652173913043478\n",
      "EPOCH=719\n",
      "train: loss=0.07283101196416665 acc=0.9681159420289855\n",
      "test: loss=0.09579524871430345 acc=0.9637681159420289\n",
      "EPOCH=720\n",
      "train: loss=0.05534272500012739 acc=0.981159420289855\n",
      "test: loss=0.08511632963844613 acc=0.9637681159420289\n",
      "EPOCH=721\n",
      "train: loss=0.05249208090950104 acc=0.9826086956521739\n",
      "test: loss=0.0968689765242445 acc=0.9550724637681159\n",
      "EPOCH=722\n",
      "train: loss=0.08378280914209657 acc=0.9666666666666667\n",
      "test: loss=0.08572115075109701 acc=0.9666666666666667\n",
      "EPOCH=723\n",
      "train: loss=0.06767867054417105 acc=0.972463768115942\n",
      "test: loss=0.10563223705415009 acc=0.9594202898550724\n",
      "EPOCH=724\n",
      "train: loss=0.05560086930480009 acc=0.9753623188405797\n",
      "test: loss=0.11226685263420512 acc=0.9550724637681159\n",
      "EPOCH=725\n",
      "train: loss=0.06737797161135392 acc=0.9753623188405797\n",
      "test: loss=0.08682236935061081 acc=0.9695652173913043\n",
      "EPOCH=726\n",
      "train: loss=0.07987275111181852 acc=0.9710144927536232\n",
      "test: loss=0.09055793196372723 acc=0.9608695652173913\n",
      "EPOCH=727\n",
      "train: loss=0.0663451881313148 acc=0.9666666666666667\n",
      "test: loss=0.09311022396379565 acc=0.9695652173913043\n",
      "EPOCH=728\n",
      "train: loss=0.05195672189015469 acc=0.9768115942028985\n",
      "test: loss=0.08597982185514241 acc=0.9652173913043478\n",
      "EPOCH=729\n",
      "train: loss=0.08097631038126 acc=0.9695652173913043\n",
      "test: loss=0.08227409618172918 acc=0.9637681159420289\n",
      "EPOCH=730\n",
      "train: loss=0.04596441998261467 acc=0.9855072463768116\n",
      "test: loss=0.08934931793503124 acc=0.9637681159420289\n",
      "EPOCH=731\n",
      "train: loss=0.07243701103459835 acc=0.9695652173913043\n",
      "test: loss=0.09506599761968813 acc=0.9579710144927536\n",
      "EPOCH=732\n",
      "train: loss=0.04608103796595126 acc=0.9855072463768116\n",
      "test: loss=0.08752073565486804 acc=0.9637681159420289\n",
      "EPOCH=733\n",
      "train: loss=0.06849705141121533 acc=0.9739130434782609\n",
      "test: loss=0.08898865644089436 acc=0.9623188405797102\n",
      "EPOCH=734\n",
      "train: loss=0.0715121578690021 acc=0.9637681159420289\n",
      "test: loss=0.08235537889742173 acc=0.9652173913043478\n",
      "EPOCH=735\n",
      "train: loss=0.0699681381902162 acc=0.9739130434782609\n",
      "test: loss=0.08157707190419307 acc=0.9681159420289855\n",
      "EPOCH=736\n",
      "train: loss=0.08267268400756074 acc=0.9695652173913043\n",
      "test: loss=0.08459590071317617 acc=0.9681159420289855\n",
      "EPOCH=737\n",
      "train: loss=0.049769866754347386 acc=0.9826086956521739\n",
      "test: loss=0.07806736225473405 acc=0.9681159420289855\n",
      "EPOCH=738\n",
      "train: loss=0.05704742071094543 acc=0.9739130434782609\n",
      "test: loss=0.08345077212517983 acc=0.9710144927536232\n",
      "EPOCH=739\n",
      "train: loss=0.05856806368148337 acc=0.9753623188405797\n",
      "test: loss=0.08108472447900454 acc=0.9710144927536232\n",
      "EPOCH=740\n",
      "train: loss=0.0512388617642039 acc=0.981159420289855\n",
      "test: loss=0.08751682168139663 acc=0.9536231884057971\n",
      "EPOCH=741\n",
      "train: loss=0.10767412377042344 acc=0.9492753623188406\n",
      "test: loss=0.08298539973962465 acc=0.9695652173913043\n",
      "EPOCH=742\n",
      "train: loss=0.051541880187310166 acc=0.9826086956521739\n",
      "test: loss=0.08977583695040695 acc=0.9652173913043478\n",
      "EPOCH=743\n",
      "train: loss=0.06649797005238725 acc=0.9695652173913043\n",
      "test: loss=0.086412093827079 acc=0.972463768115942\n",
      "EPOCH=744\n",
      "train: loss=0.08830266419548535 acc=0.9681159420289855\n",
      "test: loss=0.0878968970884563 acc=0.9652173913043478\n",
      "EPOCH=745\n",
      "train: loss=0.09273233379536353 acc=0.9623188405797102\n",
      "test: loss=0.08677825506451396 acc=0.9666666666666667\n",
      "EPOCH=746\n",
      "train: loss=0.06710977263850665 acc=0.9739130434782609\n",
      "test: loss=0.09168266686981427 acc=0.9579710144927536\n",
      "EPOCH=747\n",
      "train: loss=0.09832666245476847 acc=0.9565217391304348\n",
      "test: loss=0.08185709918906986 acc=0.9666666666666667\n",
      "EPOCH=748\n",
      "train: loss=0.07997619274474033 acc=0.972463768115942\n",
      "test: loss=0.0832400166948572 acc=0.9652173913043478\n",
      "EPOCH=749\n",
      "train: loss=0.06477724238020709 acc=0.9768115942028985\n",
      "test: loss=0.08614806611316131 acc=0.9695652173913043\n",
      "EPOCH=750\n",
      "train: loss=0.068317613543989 acc=0.9753623188405797\n",
      "test: loss=0.07739530463452308 acc=0.9710144927536232\n",
      "EPOCH=751\n",
      "train: loss=0.0727062724229251 acc=0.9652173913043478\n",
      "test: loss=0.08701927248722817 acc=0.9681159420289855\n",
      "EPOCH=752\n",
      "train: loss=0.07494738527213818 acc=0.9710144927536232\n",
      "test: loss=0.08155408840374362 acc=0.9695652173913043\n",
      "EPOCH=753\n",
      "train: loss=0.0723847876404896 acc=0.9695652173913043\n",
      "test: loss=0.08533692092569353 acc=0.9608695652173913\n",
      "EPOCH=754\n",
      "train: loss=0.042667879021974345 acc=0.9826086956521739\n",
      "test: loss=0.08595309914410064 acc=0.9623188405797102\n",
      "EPOCH=755\n",
      "train: loss=0.05163651839620736 acc=0.9768115942028985\n",
      "test: loss=0.07765142489702911 acc=0.9710144927536232\n",
      "EPOCH=756\n",
      "train: loss=0.04060459132858549 acc=0.9855072463768116\n",
      "test: loss=0.07669262664707023 acc=0.9666666666666667\n",
      "EPOCH=757\n",
      "train: loss=0.08183564239115612 acc=0.9695652173913043\n",
      "test: loss=0.08093453433982302 acc=0.9695652173913043\n",
      "EPOCH=758\n",
      "train: loss=0.0875914877297962 acc=0.9666666666666667\n",
      "test: loss=0.07720582388419392 acc=0.9666666666666667\n",
      "EPOCH=759\n",
      "train: loss=0.05553130869401959 acc=0.9768115942028985\n",
      "test: loss=0.08005040130993864 acc=0.9695652173913043\n",
      "EPOCH=760\n",
      "train: loss=0.05527990705053039 acc=0.9753623188405797\n",
      "test: loss=0.08007343579991437 acc=0.972463768115942\n",
      "EPOCH=761\n",
      "train: loss=0.05649737868147466 acc=0.981159420289855\n",
      "test: loss=0.08232044415631447 acc=0.9681159420289855\n",
      "EPOCH=762\n",
      "train: loss=0.06223661533937905 acc=0.9681159420289855\n",
      "test: loss=0.08509806417224196 acc=0.9652173913043478\n",
      "EPOCH=763\n",
      "train: loss=0.06719150606122228 acc=0.9681159420289855\n",
      "test: loss=0.0759156481073736 acc=0.9710144927536232\n",
      "EPOCH=764\n",
      "train: loss=0.05631028456132686 acc=0.9826086956521739\n",
      "test: loss=0.07846087585091467 acc=0.9652173913043478\n",
      "EPOCH=765\n",
      "train: loss=0.06946129348784627 acc=0.9666666666666667\n",
      "test: loss=0.07896506058564287 acc=0.9710144927536232\n",
      "EPOCH=766\n",
      "train: loss=0.0697220571008179 acc=0.9681159420289855\n",
      "test: loss=0.08556237701159757 acc=0.9695652173913043\n",
      "EPOCH=767\n",
      "train: loss=0.05246116610434461 acc=0.9782608695652174\n",
      "test: loss=0.08241673400142902 acc=0.9637681159420289\n",
      "EPOCH=768\n",
      "train: loss=0.07715888730037944 acc=0.9652173913043478\n",
      "test: loss=0.08884706361775435 acc=0.9608695652173913\n",
      "EPOCH=769\n",
      "train: loss=0.07065167090863181 acc=0.9666666666666667\n",
      "test: loss=0.08067060080905612 acc=0.9695652173913043\n",
      "EPOCH=770\n",
      "train: loss=0.10566314322685136 acc=0.9565217391304348\n",
      "test: loss=0.08275420865805264 acc=0.9579710144927536\n",
      "EPOCH=771\n",
      "train: loss=0.0495915913380339 acc=0.9797101449275363\n",
      "test: loss=0.08096720997916472 acc=0.9623188405797102\n",
      "EPOCH=772\n",
      "train: loss=0.07300446766291284 acc=0.9652173913043478\n",
      "test: loss=0.08119013522738107 acc=0.9681159420289855\n",
      "EPOCH=773\n",
      "train: loss=0.06374053000490322 acc=0.9797101449275363\n",
      "test: loss=0.08329070220126725 acc=0.9695652173913043\n",
      "EPOCH=774\n",
      "train: loss=0.04824667060758716 acc=0.9753623188405797\n",
      "test: loss=0.08468995625485518 acc=0.9637681159420289\n",
      "EPOCH=775\n",
      "train: loss=0.08453182559759814 acc=0.9565217391304348\n",
      "test: loss=0.08524060568123358 acc=0.9681159420289855\n",
      "EPOCH=776\n",
      "train: loss=0.03964410328688055 acc=0.9855072463768116\n",
      "test: loss=0.0827762639907721 acc=0.9666666666666667\n",
      "EPOCH=777\n",
      "train: loss=0.06431806193728085 acc=0.9739130434782609\n",
      "test: loss=0.07323519708723224 acc=0.9710144927536232\n",
      "EPOCH=778\n",
      "train: loss=0.07413999167331169 acc=0.9710144927536232\n",
      "test: loss=0.07360538285827095 acc=0.9710144927536232\n",
      "EPOCH=779\n",
      "train: loss=0.051080132798066155 acc=0.9826086956521739\n",
      "test: loss=0.07934801328161765 acc=0.9681159420289855\n",
      "EPOCH=780\n",
      "train: loss=0.0850686427116963 acc=0.9608695652173913\n",
      "test: loss=0.0873612572670394 acc=0.9652173913043478\n",
      "EPOCH=781\n",
      "train: loss=0.06139971660565065 acc=0.9782608695652174\n",
      "test: loss=0.07853776991996711 acc=0.9695652173913043\n",
      "EPOCH=782\n",
      "train: loss=0.08397112311224288 acc=0.9594202898550724\n",
      "test: loss=0.07397787442279422 acc=0.9695652173913043\n",
      "EPOCH=783\n",
      "train: loss=0.05009715050710532 acc=0.9855072463768116\n",
      "test: loss=0.08540802582003726 acc=0.9623188405797102\n",
      "EPOCH=784\n",
      "train: loss=0.07060842540178802 acc=0.9710144927536232\n",
      "test: loss=0.08485356980822838 acc=0.9710144927536232\n",
      "EPOCH=785\n",
      "train: loss=0.061281907339955616 acc=0.9739130434782609\n",
      "test: loss=0.08125672918945244 acc=0.9710144927536232\n",
      "EPOCH=786\n",
      "train: loss=0.0553151315541449 acc=0.9797101449275363\n",
      "test: loss=0.07614531699818099 acc=0.972463768115942\n",
      "EPOCH=787\n",
      "train: loss=0.05822078458685279 acc=0.972463768115942\n",
      "test: loss=0.08092699902803926 acc=0.9681159420289855\n",
      "EPOCH=788\n",
      "train: loss=0.056036959645338884 acc=0.9753623188405797\n",
      "test: loss=0.07593640773459566 acc=0.9681159420289855\n",
      "EPOCH=789\n",
      "train: loss=0.04556914345402068 acc=0.9869565217391304\n",
      "test: loss=0.08050582762510337 acc=0.972463768115942\n",
      "EPOCH=790\n",
      "train: loss=0.09242841450745669 acc=0.9536231884057971\n",
      "test: loss=0.07879328512582759 acc=0.9710144927536232\n",
      "EPOCH=791\n",
      "train: loss=0.06380615053987913 acc=0.9768115942028985\n",
      "test: loss=0.0800079240643846 acc=0.9666666666666667\n",
      "EPOCH=792\n",
      "train: loss=0.040995694433290944 acc=0.9869565217391304\n",
      "test: loss=0.0788778759622522 acc=0.972463768115942\n",
      "EPOCH=793\n",
      "train: loss=0.051111421254915225 acc=0.9782608695652174\n",
      "test: loss=0.07377709966711923 acc=0.9695652173913043\n",
      "EPOCH=794\n",
      "train: loss=0.06492875206453023 acc=0.9666666666666667\n",
      "test: loss=0.0822930666219533 acc=0.9666666666666667\n",
      "EPOCH=795\n",
      "train: loss=0.0588406267788872 acc=0.9797101449275363\n",
      "test: loss=0.0747059451150136 acc=0.972463768115942\n",
      "EPOCH=796\n",
      "train: loss=0.07794579136561797 acc=0.972463768115942\n",
      "test: loss=0.07371932033590335 acc=0.9710144927536232\n",
      "EPOCH=797\n",
      "train: loss=0.05531643977064519 acc=0.9797101449275363\n",
      "test: loss=0.07815332195755029 acc=0.9681159420289855\n",
      "EPOCH=798\n",
      "train: loss=0.06628743339307513 acc=0.9681159420289855\n",
      "test: loss=0.07496043779967185 acc=0.9739130434782609\n",
      "EPOCH=799\n",
      "train: loss=0.0518870697934787 acc=0.9840579710144928\n",
      "test: loss=0.07187243302469125 acc=0.9753623188405797\n",
      "EPOCH=800\n",
      "train: loss=0.03932407773431838 acc=0.9826086956521739\n",
      "test: loss=0.07650748276469423 acc=0.9652173913043478\n",
      "EPOCH=801\n",
      "train: loss=0.06928833631031435 acc=0.9695652173913043\n",
      "test: loss=0.07712918685864632 acc=0.9695652173913043\n",
      "EPOCH=802\n",
      "train: loss=0.0417547479959269 acc=0.9826086956521739\n",
      "test: loss=0.07761865488898806 acc=0.972463768115942\n",
      "EPOCH=803\n",
      "train: loss=0.055842297122201234 acc=0.9797101449275363\n",
      "test: loss=0.07772184467271806 acc=0.9637681159420289\n",
      "EPOCH=804\n",
      "train: loss=0.05928349579262777 acc=0.9739130434782609\n",
      "test: loss=0.08055006412619496 acc=0.9652173913043478\n",
      "EPOCH=805\n",
      "train: loss=0.07951639130709866 acc=0.9666666666666667\n",
      "test: loss=0.0764628679021931 acc=0.9652173913043478\n",
      "EPOCH=806\n",
      "train: loss=0.059729269134447885 acc=0.9782608695652174\n",
      "test: loss=0.07502139699781478 acc=0.9695652173913043\n",
      "EPOCH=807\n",
      "train: loss=0.059693728029798054 acc=0.9753623188405797\n",
      "test: loss=0.07438753557196942 acc=0.9681159420289855\n",
      "EPOCH=808\n",
      "train: loss=0.0618275380359449 acc=0.9710144927536232\n",
      "test: loss=0.07906951625953444 acc=0.9681159420289855\n",
      "EPOCH=809\n",
      "train: loss=0.07099726730856654 acc=0.9710144927536232\n",
      "test: loss=0.07738927222733623 acc=0.9681159420289855\n",
      "EPOCH=810\n",
      "train: loss=0.08289810230756671 acc=0.9507246376811594\n",
      "test: loss=0.0773544620211428 acc=0.9681159420289855\n",
      "EPOCH=811\n",
      "train: loss=0.07121712374459274 acc=0.9666666666666667\n",
      "test: loss=0.07453732291928233 acc=0.9695652173913043\n",
      "EPOCH=812\n",
      "train: loss=0.0863806993613374 acc=0.9623188405797102\n",
      "test: loss=0.08298132069503193 acc=0.9652173913043478\n",
      "EPOCH=813\n",
      "train: loss=0.08860149647153394 acc=0.9666666666666667\n",
      "test: loss=0.07188544943136968 acc=0.9739130434782609\n",
      "EPOCH=814\n",
      "train: loss=0.06011722611756443 acc=0.9797101449275363\n",
      "test: loss=0.07580540076579548 acc=0.9652173913043478\n",
      "EPOCH=815\n",
      "train: loss=0.08089567023645733 acc=0.9652173913043478\n",
      "test: loss=0.07367948384250232 acc=0.9681159420289855\n",
      "EPOCH=816\n",
      "train: loss=0.06831043605477416 acc=0.9753623188405797\n",
      "test: loss=0.07540122082905529 acc=0.9666666666666667\n",
      "EPOCH=817\n",
      "train: loss=0.06084199199064061 acc=0.9681159420289855\n",
      "test: loss=0.07496969011243948 acc=0.9681159420289855\n",
      "EPOCH=818\n",
      "train: loss=0.054958221584681236 acc=0.9782608695652174\n",
      "test: loss=0.0767759542510901 acc=0.9695652173913043\n",
      "EPOCH=819\n",
      "train: loss=0.05670695895794386 acc=0.9797101449275363\n",
      "test: loss=0.07442006091797951 acc=0.9695652173913043\n",
      "EPOCH=820\n",
      "train: loss=0.05845020438923905 acc=0.9782608695652174\n",
      "test: loss=0.0723681260657886 acc=0.9710144927536232\n",
      "EPOCH=821\n",
      "train: loss=0.05617089401762928 acc=0.9753623188405797\n",
      "test: loss=0.07847290222268576 acc=0.9637681159420289\n",
      "EPOCH=822\n",
      "train: loss=0.04510736833457967 acc=0.9753623188405797\n",
      "test: loss=0.06981274833079977 acc=0.9695652173913043\n",
      "EPOCH=823\n",
      "train: loss=0.06789409518777612 acc=0.9695652173913043\n",
      "test: loss=0.06992331288189585 acc=0.9710144927536232\n",
      "EPOCH=824\n",
      "train: loss=0.04995643006385296 acc=0.9768115942028985\n",
      "test: loss=0.07241673125093531 acc=0.9666666666666667\n",
      "EPOCH=825\n",
      "train: loss=0.05400220746936647 acc=0.9855072463768116\n",
      "test: loss=0.07309115850093137 acc=0.9695652173913043\n",
      "EPOCH=826\n",
      "train: loss=0.07079411148065093 acc=0.9739130434782609\n",
      "test: loss=0.08063451711961475 acc=0.9695652173913043\n",
      "EPOCH=827\n",
      "train: loss=0.05519789881651442 acc=0.9826086956521739\n",
      "test: loss=0.07523276658976365 acc=0.9710144927536232\n",
      "EPOCH=828\n",
      "train: loss=0.06672022804950536 acc=0.9753623188405797\n",
      "test: loss=0.07268397378854509 acc=0.9681159420289855\n",
      "EPOCH=829\n",
      "train: loss=0.06399876309119405 acc=0.9695652173913043\n",
      "test: loss=0.0735409708320557 acc=0.9695652173913043\n",
      "EPOCH=830\n",
      "train: loss=0.06134442641248907 acc=0.9710144927536232\n",
      "test: loss=0.07774718306206191 acc=0.9681159420289855\n",
      "EPOCH=831\n",
      "train: loss=0.0633444308153604 acc=0.9753623188405797\n",
      "test: loss=0.07526556221906047 acc=0.9710144927536232\n",
      "EPOCH=832\n",
      "train: loss=0.084459818144078 acc=0.9550724637681159\n",
      "test: loss=0.07212931385702102 acc=0.9652173913043478\n",
      "EPOCH=833\n",
      "train: loss=0.048123883247765814 acc=0.9782608695652174\n",
      "test: loss=0.0751026273246447 acc=0.9681159420289855\n",
      "EPOCH=834\n",
      "train: loss=0.0456857857974326 acc=0.9768115942028985\n",
      "test: loss=0.07574336689525296 acc=0.9710144927536232\n",
      "EPOCH=835\n",
      "train: loss=0.061273701484936154 acc=0.9681159420289855\n",
      "test: loss=0.07329937979909149 acc=0.9695652173913043\n",
      "EPOCH=836\n",
      "train: loss=0.03633655070995513 acc=0.991304347826087\n",
      "test: loss=0.07431285902350869 acc=0.9681159420289855\n",
      "EPOCH=837\n",
      "train: loss=0.053103649028462915 acc=0.9739130434782609\n",
      "test: loss=0.07335203339753307 acc=0.9695652173913043\n",
      "EPOCH=838\n",
      "train: loss=0.05424686327560233 acc=0.9768115942028985\n",
      "test: loss=0.07346773670127726 acc=0.9710144927536232\n",
      "EPOCH=839\n",
      "train: loss=0.048300873510555266 acc=0.9797101449275363\n",
      "test: loss=0.0716885318831665 acc=0.9695652173913043\n",
      "EPOCH=840\n",
      "train: loss=0.06008865520995194 acc=0.972463768115942\n",
      "test: loss=0.06891597952306543 acc=0.972463768115942\n",
      "EPOCH=841\n",
      "train: loss=0.04711515674827753 acc=0.981159420289855\n",
      "test: loss=0.07235443633809112 acc=0.9710144927536232\n",
      "EPOCH=842\n",
      "train: loss=0.06911969232301292 acc=0.9666666666666667\n",
      "test: loss=0.07869399271619403 acc=0.9666666666666667\n",
      "EPOCH=843\n",
      "train: loss=0.0617144133146715 acc=0.9768115942028985\n",
      "test: loss=0.07056774063965517 acc=0.9710144927536232\n",
      "EPOCH=844\n",
      "train: loss=0.07974749946963622 acc=0.9681159420289855\n",
      "test: loss=0.07276963257307412 acc=0.9666666666666667\n",
      "EPOCH=845\n",
      "train: loss=0.06820138698602524 acc=0.972463768115942\n",
      "test: loss=0.07149785675846201 acc=0.972463768115942\n",
      "EPOCH=846\n",
      "train: loss=0.054946794961273575 acc=0.9782608695652174\n",
      "test: loss=0.0723553561346262 acc=0.9666666666666667\n",
      "EPOCH=847\n",
      "train: loss=0.07524349778750151 acc=0.9623188405797102\n",
      "test: loss=0.06994751200621872 acc=0.972463768115942\n",
      "EPOCH=848\n",
      "train: loss=0.0685487393090393 acc=0.972463768115942\n",
      "test: loss=0.07614680077113212 acc=0.972463768115942\n",
      "EPOCH=849\n",
      "train: loss=0.05106229994271927 acc=0.9782608695652174\n",
      "test: loss=0.07482002749684721 acc=0.9681159420289855\n",
      "EPOCH=850\n",
      "train: loss=0.05905575156527099 acc=0.9826086956521739\n",
      "test: loss=0.07186693369360543 acc=0.9666666666666667\n",
      "EPOCH=851\n",
      "train: loss=0.036735926397691326 acc=0.9826086956521739\n",
      "test: loss=0.07036210623843006 acc=0.9739130434782609\n",
      "EPOCH=852\n",
      "train: loss=0.052047097714836 acc=0.9782608695652174\n",
      "test: loss=0.07076890252067428 acc=0.972463768115942\n",
      "EPOCH=853\n",
      "train: loss=0.05204470256834421 acc=0.9797101449275363\n",
      "test: loss=0.07054274339559866 acc=0.9739130434782609\n",
      "EPOCH=854\n",
      "train: loss=0.08420865338006206 acc=0.9623188405797102\n",
      "test: loss=0.06886496502753879 acc=0.9710144927536232\n",
      "EPOCH=855\n",
      "train: loss=0.0586164258152416 acc=0.9768115942028985\n",
      "test: loss=0.06833308909581187 acc=0.9753623188405797\n",
      "EPOCH=856\n",
      "train: loss=0.051660268104058805 acc=0.9753623188405797\n",
      "test: loss=0.07000386742036893 acc=0.9710144927536232\n",
      "EPOCH=857\n",
      "train: loss=0.061031917509733115 acc=0.9652173913043478\n",
      "test: loss=0.07270765116194472 acc=0.9695652173913043\n",
      "EPOCH=858\n",
      "train: loss=0.06098628639669301 acc=0.9782608695652174\n",
      "test: loss=0.07471412084330167 acc=0.9710144927536232\n",
      "EPOCH=859\n",
      "train: loss=0.06616800916767744 acc=0.972463768115942\n",
      "test: loss=0.06939882561504443 acc=0.9710144927536232\n",
      "EPOCH=860\n",
      "train: loss=0.04179353026095869 acc=0.9826086956521739\n",
      "test: loss=0.07099998324899823 acc=0.9710144927536232\n",
      "EPOCH=861\n",
      "train: loss=0.053133234958581556 acc=0.9753623188405797\n",
      "test: loss=0.07650824295297033 acc=0.9681159420289855\n",
      "EPOCH=862\n",
      "train: loss=0.0724757936946346 acc=0.9652173913043478\n",
      "test: loss=0.06838198291765497 acc=0.9797101449275363\n",
      "EPOCH=863\n",
      "train: loss=0.03838228185213204 acc=0.9869565217391304\n",
      "test: loss=0.07231289745039661 acc=0.9710144927536232\n",
      "EPOCH=864\n",
      "train: loss=0.05466230864544117 acc=0.9797101449275363\n",
      "test: loss=0.08007807835628929 acc=0.9623188405797102\n",
      "EPOCH=865\n",
      "train: loss=0.06458432589388298 acc=0.9739130434782609\n",
      "test: loss=0.07294915436306987 acc=0.9695652173913043\n",
      "EPOCH=866\n",
      "train: loss=0.05510762441956785 acc=0.9739130434782609\n",
      "test: loss=0.06947909080752002 acc=0.9695652173913043\n",
      "EPOCH=867\n",
      "train: loss=0.07621130393326253 acc=0.9666666666666667\n",
      "test: loss=0.07064770738054243 acc=0.9695652173913043\n",
      "EPOCH=868\n",
      "train: loss=0.056572297638974325 acc=0.9753623188405797\n",
      "test: loss=0.06471438249690205 acc=0.9695652173913043\n",
      "EPOCH=869\n",
      "train: loss=0.056764930709791266 acc=0.972463768115942\n",
      "test: loss=0.07215218115877765 acc=0.9695652173913043\n",
      "EPOCH=870\n",
      "train: loss=0.07368949215246874 acc=0.9695652173913043\n",
      "test: loss=0.07121826561463017 acc=0.9666666666666667\n",
      "EPOCH=871\n",
      "train: loss=0.05455546786194468 acc=0.972463768115942\n",
      "test: loss=0.06887873681562648 acc=0.9681159420289855\n",
      "EPOCH=872\n",
      "train: loss=0.06415723300501532 acc=0.972463768115942\n",
      "test: loss=0.06782036680003437 acc=0.9753623188405797\n",
      "EPOCH=873\n",
      "train: loss=0.07152298794065899 acc=0.9739130434782609\n",
      "test: loss=0.07027407453096746 acc=0.972463768115942\n",
      "EPOCH=874\n",
      "train: loss=0.06413710935209237 acc=0.9695652173913043\n",
      "test: loss=0.06706445683299177 acc=0.9695652173913043\n",
      "EPOCH=875\n",
      "train: loss=0.05839040739014154 acc=0.9739130434782609\n",
      "test: loss=0.07029420708687745 acc=0.9710144927536232\n",
      "EPOCH=876\n",
      "train: loss=0.052212376672713945 acc=0.9797101449275363\n",
      "test: loss=0.06952234813658785 acc=0.972463768115942\n",
      "EPOCH=877\n",
      "train: loss=0.0499029257897241 acc=0.9869565217391304\n",
      "test: loss=0.07213708501900676 acc=0.9681159420289855\n",
      "EPOCH=878\n",
      "train: loss=0.06335025538457888 acc=0.972463768115942\n",
      "test: loss=0.07357779875445382 acc=0.9681159420289855\n",
      "EPOCH=879\n",
      "train: loss=0.06381395358270417 acc=0.9695652173913043\n",
      "test: loss=0.06742682908787966 acc=0.9666666666666667\n",
      "EPOCH=880\n",
      "train: loss=0.07537518168334718 acc=0.9710144927536232\n",
      "test: loss=0.0700223577101675 acc=0.9695652173913043\n",
      "EPOCH=881\n",
      "train: loss=0.06950809320942566 acc=0.9637681159420289\n",
      "test: loss=0.07249874494417272 acc=0.9695652173913043\n",
      "EPOCH=882\n",
      "train: loss=0.044936510876010416 acc=0.9855072463768116\n",
      "test: loss=0.06808564604969364 acc=0.9739130434782609\n",
      "EPOCH=883\n",
      "train: loss=0.0511581010756119 acc=0.9768115942028985\n",
      "test: loss=0.06569936635846572 acc=0.9739130434782609\n",
      "EPOCH=884\n",
      "train: loss=0.061047803004545 acc=0.9753623188405797\n",
      "test: loss=0.06742085235838356 acc=0.972463768115942\n",
      "EPOCH=885\n",
      "train: loss=0.02716794520799506 acc=0.9898550724637681\n",
      "test: loss=0.07045808953964508 acc=0.972463768115942\n",
      "EPOCH=886\n",
      "train: loss=0.061271085716892625 acc=0.9710144927536232\n",
      "test: loss=0.06876044227187848 acc=0.972463768115942\n",
      "EPOCH=887\n",
      "train: loss=0.0329500376763935 acc=0.991304347826087\n",
      "test: loss=0.06745400661352333 acc=0.972463768115942\n",
      "EPOCH=888\n",
      "train: loss=0.0843318992173562 acc=0.9623188405797102\n",
      "test: loss=0.06832461914081654 acc=0.9681159420289855\n",
      "EPOCH=889\n",
      "train: loss=0.08590200692804545 acc=0.9608695652173913\n",
      "test: loss=0.068606860349198 acc=0.972463768115942\n",
      "EPOCH=890\n",
      "train: loss=0.034107447018842635 acc=0.9855072463768116\n",
      "test: loss=0.06510424686184813 acc=0.9739130434782609\n",
      "EPOCH=891\n",
      "train: loss=0.04099574126781941 acc=0.981159420289855\n",
      "test: loss=0.0699515047489796 acc=0.9666666666666667\n",
      "EPOCH=892\n",
      "train: loss=0.05490476538087784 acc=0.9782608695652174\n",
      "test: loss=0.07255551533925134 acc=0.9710144927536232\n",
      "EPOCH=893\n",
      "train: loss=0.0649022786081317 acc=0.9681159420289855\n",
      "test: loss=0.0672446105722046 acc=0.972463768115942\n",
      "EPOCH=894\n",
      "train: loss=0.051844982767362956 acc=0.9739130434782609\n",
      "test: loss=0.06982186964265617 acc=0.9710144927536232\n",
      "EPOCH=895\n",
      "train: loss=0.059815305919298276 acc=0.972463768115942\n",
      "test: loss=0.06567433476554466 acc=0.9681159420289855\n",
      "EPOCH=896\n",
      "train: loss=0.0473018855782326 acc=0.9768115942028985\n",
      "test: loss=0.06837012647197804 acc=0.9768115942028985\n",
      "EPOCH=897\n",
      "train: loss=0.05826288284512773 acc=0.9695652173913043\n",
      "test: loss=0.06650786901186934 acc=0.9753623188405797\n",
      "EPOCH=898\n",
      "train: loss=0.051990977241977245 acc=0.9710144927536232\n",
      "test: loss=0.06463472039686764 acc=0.9710144927536232\n",
      "EPOCH=899\n",
      "train: loss=0.04963050019492919 acc=0.9768115942028985\n",
      "test: loss=0.06892758157887854 acc=0.9695652173913043\n",
      "EPOCH=900\n",
      "train: loss=0.04616106799295049 acc=0.9768115942028985\n",
      "test: loss=0.06802514465385691 acc=0.9768115942028985\n",
      "EPOCH=901\n",
      "train: loss=0.05201848951699478 acc=0.972463768115942\n",
      "test: loss=0.07137462594437269 acc=0.9695652173913043\n",
      "EPOCH=902\n",
      "train: loss=0.06003160335086617 acc=0.9681159420289855\n",
      "test: loss=0.0665827210110516 acc=0.9753623188405797\n",
      "EPOCH=903\n",
      "train: loss=0.042987498977882614 acc=0.981159420289855\n",
      "test: loss=0.06670223837022375 acc=0.9739130434782609\n",
      "EPOCH=904\n",
      "train: loss=0.06135764293647124 acc=0.9695652173913043\n",
      "test: loss=0.06674657976010018 acc=0.9753623188405797\n",
      "EPOCH=905\n",
      "train: loss=0.05872646564190333 acc=0.9739130434782609\n",
      "test: loss=0.0714066396680666 acc=0.9695652173913043\n",
      "EPOCH=906\n",
      "train: loss=0.0580539622953088 acc=0.9753623188405797\n",
      "test: loss=0.07266596028036171 acc=0.9695652173913043\n",
      "EPOCH=907\n",
      "train: loss=0.06361934960299163 acc=0.9681159420289855\n",
      "test: loss=0.06532437159727159 acc=0.9753623188405797\n",
      "EPOCH=908\n",
      "train: loss=0.04448773029841065 acc=0.9782608695652174\n",
      "test: loss=0.06927823379695598 acc=0.9739130434782609\n",
      "EPOCH=909\n",
      "train: loss=0.07033507723588121 acc=0.9739130434782609\n",
      "test: loss=0.06775515554387007 acc=0.9753623188405797\n",
      "EPOCH=910\n",
      "train: loss=0.05319173310815075 acc=0.9782608695652174\n",
      "test: loss=0.06725841469837236 acc=0.972463768115942\n",
      "EPOCH=911\n",
      "train: loss=0.05489986243039647 acc=0.9782608695652174\n",
      "test: loss=0.06944184800450848 acc=0.972463768115942\n",
      "EPOCH=912\n",
      "train: loss=0.039195247072132136 acc=0.9826086956521739\n",
      "test: loss=0.06606713312441116 acc=0.9768115942028985\n",
      "EPOCH=913\n",
      "train: loss=0.07188888172782353 acc=0.9666666666666667\n",
      "test: loss=0.0650133014700837 acc=0.9739130434782609\n",
      "EPOCH=914\n",
      "train: loss=0.0500476150151424 acc=0.9797101449275363\n",
      "test: loss=0.06515823830895814 acc=0.9753623188405797\n",
      "EPOCH=915\n",
      "train: loss=0.05814643202420931 acc=0.9710144927536232\n",
      "test: loss=0.0657715515243401 acc=0.9652173913043478\n",
      "EPOCH=916\n",
      "train: loss=0.06756365935144773 acc=0.9681159420289855\n",
      "test: loss=0.06805825212254933 acc=0.9695652173913043\n",
      "EPOCH=917\n",
      "train: loss=0.03720244237650403 acc=0.9826086956521739\n",
      "test: loss=0.06200203442316236 acc=0.972463768115942\n",
      "EPOCH=918\n",
      "train: loss=0.06092171238942992 acc=0.972463768115942\n",
      "test: loss=0.06696000947282235 acc=0.9681159420289855\n",
      "EPOCH=919\n",
      "train: loss=0.06774526155844689 acc=0.9666666666666667\n",
      "test: loss=0.06597661088731877 acc=0.9739130434782609\n",
      "EPOCH=920\n",
      "train: loss=0.04875462552571406 acc=0.9753623188405797\n",
      "test: loss=0.0679837024273217 acc=0.972463768115942\n",
      "EPOCH=921\n",
      "train: loss=0.07755552344156647 acc=0.9695652173913043\n",
      "test: loss=0.06407361150068573 acc=0.972463768115942\n",
      "EPOCH=922\n",
      "train: loss=0.04617440491230924 acc=0.9797101449275363\n",
      "test: loss=0.07156391482201431 acc=0.9681159420289855\n",
      "EPOCH=923\n",
      "train: loss=0.05865359511974335 acc=0.9710144927536232\n",
      "test: loss=0.0675785683639034 acc=0.9739130434782609\n",
      "EPOCH=924\n",
      "train: loss=0.043968144616836034 acc=0.981159420289855\n",
      "test: loss=0.06681110400538452 acc=0.9753623188405797\n",
      "EPOCH=925\n",
      "train: loss=0.06860875862441601 acc=0.9695652173913043\n",
      "test: loss=0.0658976842848215 acc=0.9739130434782609\n",
      "EPOCH=926\n",
      "train: loss=0.03676713213036052 acc=0.9855072463768116\n",
      "test: loss=0.06302516073264793 acc=0.972463768115942\n",
      "EPOCH=927\n",
      "train: loss=0.059215555648306405 acc=0.9695652173913043\n",
      "test: loss=0.06958813815907168 acc=0.9753623188405797\n",
      "EPOCH=928\n",
      "train: loss=0.04869243260565832 acc=0.972463768115942\n",
      "test: loss=0.06398266248820633 acc=0.972463768115942\n",
      "EPOCH=929\n",
      "train: loss=0.06105187333470886 acc=0.972463768115942\n",
      "test: loss=0.06569974284757775 acc=0.972463768115942\n",
      "EPOCH=930\n",
      "train: loss=0.06244341715485701 acc=0.9710144927536232\n",
      "test: loss=0.06666223503852435 acc=0.9710144927536232\n",
      "EPOCH=931\n",
      "train: loss=0.07003609415051194 acc=0.9608695652173913\n",
      "test: loss=0.06551191702601572 acc=0.972463768115942\n",
      "EPOCH=932\n",
      "train: loss=0.03958558575393009 acc=0.981159420289855\n",
      "test: loss=0.06480424657680542 acc=0.9739130434782609\n",
      "EPOCH=933\n",
      "train: loss=0.08454543763888803 acc=0.9579710144927536\n",
      "test: loss=0.06309360703790101 acc=0.972463768115942\n",
      "EPOCH=934\n",
      "train: loss=0.06918811558021883 acc=0.9695652173913043\n",
      "test: loss=0.064535673598254 acc=0.9666666666666667\n",
      "EPOCH=935\n",
      "train: loss=0.06951920513520461 acc=0.9710144927536232\n",
      "test: loss=0.06481149814581347 acc=0.9710144927536232\n",
      "EPOCH=936\n",
      "train: loss=0.04308019432478469 acc=0.981159420289855\n",
      "test: loss=0.06929704066306798 acc=0.9739130434782609\n",
      "EPOCH=937\n",
      "train: loss=0.0738152040756918 acc=0.9637681159420289\n",
      "test: loss=0.06401394647873804 acc=0.972463768115942\n",
      "EPOCH=938\n",
      "train: loss=0.0627862253844215 acc=0.972463768115942\n",
      "test: loss=0.06236420178173617 acc=0.9768115942028985\n",
      "EPOCH=939\n",
      "train: loss=0.06411920084855652 acc=0.9652173913043478\n",
      "test: loss=0.06691817338421883 acc=0.9739130434782609\n",
      "EPOCH=940\n",
      "train: loss=0.04138605489760669 acc=0.9855072463768116\n",
      "test: loss=0.062976669879734 acc=0.9768115942028985\n",
      "EPOCH=941\n",
      "train: loss=0.05211555932413146 acc=0.9797101449275363\n",
      "test: loss=0.06273037591921889 acc=0.9739130434782609\n",
      "EPOCH=942\n",
      "train: loss=0.03767505431261465 acc=0.9855072463768116\n",
      "test: loss=0.0646244345192749 acc=0.9753623188405797\n",
      "EPOCH=943\n",
      "train: loss=0.06403569425291065 acc=0.9710144927536232\n",
      "test: loss=0.06214375624435888 acc=0.9753623188405797\n",
      "EPOCH=944\n",
      "train: loss=0.05833177793249174 acc=0.9637681159420289\n",
      "test: loss=0.06403154240394947 acc=0.9753623188405797\n",
      "EPOCH=945\n",
      "train: loss=0.06331871222106808 acc=0.9666666666666667\n",
      "test: loss=0.06509463933060422 acc=0.9710144927536232\n",
      "EPOCH=946\n",
      "train: loss=0.041585751203515535 acc=0.9840579710144928\n",
      "test: loss=0.06097690228448144 acc=0.9739130434782609\n",
      "EPOCH=947\n",
      "train: loss=0.06310423764971523 acc=0.9666666666666667\n",
      "test: loss=0.06299596056849316 acc=0.972463768115942\n",
      "EPOCH=948\n",
      "train: loss=0.05329117019886883 acc=0.972463768115942\n",
      "test: loss=0.06867822714088484 acc=0.972463768115942\n",
      "EPOCH=949\n",
      "train: loss=0.044949530443115275 acc=0.9768115942028985\n",
      "test: loss=0.06526538796166372 acc=0.9739130434782609\n",
      "EPOCH=950\n",
      "train: loss=0.042440536962519176 acc=0.9826086956521739\n",
      "test: loss=0.06168602326844248 acc=0.9710144927536232\n",
      "EPOCH=951\n",
      "train: loss=0.07540365036906863 acc=0.9666666666666667\n",
      "test: loss=0.06130946191094112 acc=0.972463768115942\n",
      "EPOCH=952\n",
      "train: loss=0.03205514469897933 acc=0.9927536231884058\n",
      "test: loss=0.061903076961952866 acc=0.9739130434782609\n",
      "EPOCH=953\n",
      "train: loss=0.05939289544662319 acc=0.972463768115942\n",
      "test: loss=0.061362133076256764 acc=0.9768115942028985\n",
      "EPOCH=954\n",
      "train: loss=0.06431143008689702 acc=0.9797101449275363\n",
      "test: loss=0.06291853492358801 acc=0.9753623188405797\n",
      "EPOCH=955\n",
      "train: loss=0.04782210723037757 acc=0.9768115942028985\n",
      "test: loss=0.06224763359499202 acc=0.9768115942028985\n",
      "EPOCH=956\n",
      "train: loss=0.036060443191967 acc=0.9840579710144928\n",
      "test: loss=0.05917940664962518 acc=0.9782608695652174\n",
      "EPOCH=957\n",
      "train: loss=0.05954347162102581 acc=0.9695652173913043\n",
      "test: loss=0.06446905162251042 acc=0.9768115942028985\n",
      "EPOCH=958\n",
      "train: loss=0.05503451087852519 acc=0.9753623188405797\n",
      "test: loss=0.06320044132370674 acc=0.9753623188405797\n",
      "EPOCH=959\n",
      "train: loss=0.046451620646822976 acc=0.981159420289855\n",
      "test: loss=0.05982125634979871 acc=0.9768115942028985\n",
      "EPOCH=960\n",
      "train: loss=0.04280667283690782 acc=0.9840579710144928\n",
      "test: loss=0.06344673846183727 acc=0.9739130434782609\n",
      "EPOCH=961\n",
      "train: loss=0.04378623499482942 acc=0.9840579710144928\n",
      "test: loss=0.06459383412491645 acc=0.972463768115942\n",
      "EPOCH=962\n",
      "train: loss=0.04346357768752574 acc=0.981159420289855\n",
      "test: loss=0.06423732398055407 acc=0.972463768115942\n",
      "EPOCH=963\n",
      "train: loss=0.041046794659830174 acc=0.9782608695652174\n",
      "test: loss=0.06353085219393478 acc=0.9739130434782609\n",
      "EPOCH=964\n",
      "train: loss=0.06057234823934383 acc=0.9695652173913043\n",
      "test: loss=0.060803428086911485 acc=0.972463768115942\n",
      "EPOCH=965\n",
      "train: loss=0.04630717537777118 acc=0.9768115942028985\n",
      "test: loss=0.06450204330163596 acc=0.9753623188405797\n",
      "EPOCH=966\n",
      "train: loss=0.040457872432550245 acc=0.9840579710144928\n",
      "test: loss=0.06482457158133517 acc=0.972463768115942\n",
      "EPOCH=967\n",
      "train: loss=0.06772937820823163 acc=0.9695652173913043\n",
      "test: loss=0.062182524092085856 acc=0.9782608695652174\n",
      "EPOCH=968\n",
      "train: loss=0.06415849734334937 acc=0.9739130434782609\n",
      "test: loss=0.06109942039204542 acc=0.9739130434782609\n",
      "EPOCH=969\n",
      "train: loss=0.06380793974982338 acc=0.9710144927536232\n",
      "test: loss=0.06471264560516593 acc=0.972463768115942\n",
      "EPOCH=970\n",
      "train: loss=0.050208320869129394 acc=0.981159420289855\n",
      "test: loss=0.06105911200128464 acc=0.972463768115942\n",
      "EPOCH=971\n",
      "train: loss=0.03983254041897673 acc=0.9840579710144928\n",
      "test: loss=0.06458276248310575 acc=0.972463768115942\n",
      "EPOCH=972\n",
      "train: loss=0.0300327410210261 acc=0.991304347826087\n",
      "test: loss=0.06043834603217765 acc=0.9710144927536232\n",
      "EPOCH=973\n",
      "train: loss=0.04274572275127333 acc=0.9855072463768116\n",
      "test: loss=0.06097210400343818 acc=0.9739130434782609\n",
      "EPOCH=974\n",
      "train: loss=0.061219390494656356 acc=0.9753623188405797\n",
      "test: loss=0.05973312728957242 acc=0.9753623188405797\n",
      "EPOCH=975\n",
      "train: loss=0.069584498396026 acc=0.9739130434782609\n",
      "test: loss=0.05910314861392743 acc=0.9710144927536232\n",
      "EPOCH=976\n",
      "train: loss=0.07122173201386467 acc=0.9594202898550724\n",
      "test: loss=0.0636735104957422 acc=0.972463768115942\n",
      "EPOCH=977\n",
      "train: loss=0.05255486198144287 acc=0.9782608695652174\n",
      "test: loss=0.06112701530425888 acc=0.9710144927536232\n",
      "EPOCH=978\n",
      "train: loss=0.06655104062245644 acc=0.9739130434782609\n",
      "test: loss=0.061917670159342894 acc=0.9768115942028985\n",
      "EPOCH=979\n",
      "train: loss=0.0368787209842362 acc=0.9782608695652174\n",
      "test: loss=0.05876190553749286 acc=0.9753623188405797\n",
      "EPOCH=980\n",
      "train: loss=0.053207847469075406 acc=0.9782608695652174\n",
      "test: loss=0.06026247411838953 acc=0.9753623188405797\n",
      "EPOCH=981\n",
      "train: loss=0.041948074028333934 acc=0.981159420289855\n",
      "test: loss=0.06152342076600082 acc=0.9739130434782609\n",
      "EPOCH=982\n",
      "train: loss=0.0538364402656103 acc=0.9739130434782609\n",
      "test: loss=0.06329762975633443 acc=0.972463768115942\n",
      "EPOCH=983\n",
      "train: loss=0.0509632392989551 acc=0.9739130434782609\n",
      "test: loss=0.06337884179656127 acc=0.9695652173913043\n",
      "EPOCH=984\n",
      "train: loss=0.05140404117743777 acc=0.9768115942028985\n",
      "test: loss=0.06079150159329138 acc=0.972463768115942\n",
      "EPOCH=985\n",
      "train: loss=0.07053918507562375 acc=0.9637681159420289\n",
      "test: loss=0.06114752424015556 acc=0.9739130434782609\n",
      "EPOCH=986\n",
      "train: loss=0.05634681258387777 acc=0.9782608695652174\n",
      "test: loss=0.06677697244815094 acc=0.9710144927536232\n",
      "EPOCH=987\n",
      "train: loss=0.06853679159869712 acc=0.9666666666666667\n",
      "test: loss=0.059218584087988774 acc=0.9768115942028985\n",
      "EPOCH=988\n",
      "train: loss=0.05234840953635291 acc=0.9739130434782609\n",
      "test: loss=0.0646498047561889 acc=0.9695652173913043\n",
      "EPOCH=989\n",
      "train: loss=0.06312993407440834 acc=0.9753623188405797\n",
      "test: loss=0.062291735754737745 acc=0.9695652173913043\n",
      "EPOCH=990\n",
      "train: loss=0.0484254101005388 acc=0.9797101449275363\n",
      "test: loss=0.0593966075578998 acc=0.972463768115942\n",
      "EPOCH=991\n",
      "train: loss=0.026456462008372258 acc=0.9898550724637681\n",
      "test: loss=0.059704361547479864 acc=0.972463768115942\n",
      "EPOCH=992\n",
      "train: loss=0.03519438057191312 acc=0.9797101449275363\n",
      "test: loss=0.06274244012984544 acc=0.9681159420289855\n",
      "EPOCH=993\n",
      "train: loss=0.06828972748506136 acc=0.9681159420289855\n",
      "test: loss=0.059071633410319034 acc=0.972463768115942\n",
      "EPOCH=994\n",
      "train: loss=0.04853559059687365 acc=0.9782608695652174\n",
      "test: loss=0.06077502129694005 acc=0.9739130434782609\n",
      "EPOCH=995\n",
      "train: loss=0.0751362284291814 acc=0.9666666666666667\n",
      "test: loss=0.06353888939549593 acc=0.972463768115942\n",
      "EPOCH=996\n",
      "train: loss=0.05395200465127318 acc=0.972463768115942\n",
      "test: loss=0.05833055099040981 acc=0.9739130434782609\n",
      "EPOCH=997\n",
      "train: loss=0.054685423505373774 acc=0.9710144927536232\n",
      "test: loss=0.06143060778807146 acc=0.9681159420289855\n",
      "EPOCH=998\n",
      "train: loss=0.05099546595010304 acc=0.9782608695652174\n",
      "test: loss=0.062426776113293864 acc=0.9753623188405797\n",
      "EPOCH=999\n",
      "train: loss=0.03920060654050778 acc=0.9797101449275363\n",
      "test: loss=0.060526122266375985 acc=0.972463768115942\n",
      "EPOCH=1000\n",
      "train: loss=0.08457829418040463 acc=0.9594202898550724\n",
      "test: loss=0.06260458798482683 acc=0.972463768115942\n",
      "EPOCH=1001\n",
      "train: loss=0.03895476289545115 acc=0.9782608695652174\n",
      "test: loss=0.05833174872593582 acc=0.9695652173913043\n",
      "EPOCH=1002\n",
      "train: loss=0.057633666693529234 acc=0.972463768115942\n",
      "test: loss=0.05884020399123242 acc=0.9753623188405797\n",
      "EPOCH=1003\n",
      "train: loss=0.028150478617820576 acc=0.9855072463768116\n",
      "test: loss=0.0614004758639433 acc=0.972463768115942\n",
      "EPOCH=1004\n",
      "train: loss=0.04967108644847233 acc=0.981159420289855\n",
      "test: loss=0.059394554891774566 acc=0.9710144927536232\n",
      "EPOCH=1005\n",
      "train: loss=0.05342638777718012 acc=0.9782608695652174\n",
      "test: loss=0.06074070137342089 acc=0.9710144927536232\n",
      "EPOCH=1006\n",
      "train: loss=0.07575580940693782 acc=0.9608695652173913\n",
      "test: loss=0.05901192029544127 acc=0.9753623188405797\n",
      "EPOCH=1007\n",
      "train: loss=0.048818852590358036 acc=0.9855072463768116\n",
      "test: loss=0.058694812133102986 acc=0.9753623188405797\n",
      "EPOCH=1008\n",
      "train: loss=0.045860818813725175 acc=0.9768115942028985\n",
      "test: loss=0.06214060661860016 acc=0.972463768115942\n",
      "EPOCH=1009\n",
      "train: loss=0.055947037658852034 acc=0.9710144927536232\n",
      "test: loss=0.05902373362596532 acc=0.9768115942028985\n",
      "EPOCH=1010\n",
      "train: loss=0.024103529422844504 acc=0.9898550724637681\n",
      "test: loss=0.05894213951540873 acc=0.9753623188405797\n",
      "EPOCH=1011\n",
      "train: loss=0.04903162006843053 acc=0.9739130434782609\n",
      "test: loss=0.058830658523235234 acc=0.9710144927536232\n",
      "EPOCH=1012\n",
      "train: loss=0.03286874388340325 acc=0.9840579710144928\n",
      "test: loss=0.05738764344405153 acc=0.9753623188405797\n",
      "EPOCH=1013\n",
      "train: loss=0.04196199726825883 acc=0.981159420289855\n",
      "test: loss=0.058955897491266326 acc=0.9739130434782609\n",
      "EPOCH=1014\n",
      "train: loss=0.039507854075053535 acc=0.981159420289855\n",
      "test: loss=0.05849535412393815 acc=0.9768115942028985\n",
      "EPOCH=1015\n",
      "train: loss=0.040253699399110915 acc=0.9826086956521739\n",
      "test: loss=0.0571983294643866 acc=0.9782608695652174\n",
      "EPOCH=1016\n",
      "train: loss=0.05990150199979773 acc=0.9710144927536232\n",
      "test: loss=0.05869336011249472 acc=0.972463768115942\n",
      "EPOCH=1017\n",
      "train: loss=0.06117024092643775 acc=0.9695652173913043\n",
      "test: loss=0.057520185747241936 acc=0.9739130434782609\n",
      "EPOCH=1018\n",
      "train: loss=0.046531319804085015 acc=0.9753623188405797\n",
      "test: loss=0.060447728138006196 acc=0.9710144927536232\n",
      "EPOCH=1019\n",
      "train: loss=0.04608711816056084 acc=0.9753623188405797\n",
      "test: loss=0.0588854243245387 acc=0.9739130434782609\n",
      "EPOCH=1020\n",
      "train: loss=0.045947961223648755 acc=0.9840579710144928\n",
      "test: loss=0.06086844401515834 acc=0.9710144927536232\n",
      "EPOCH=1021\n",
      "train: loss=0.07519747232565233 acc=0.9623188405797102\n",
      "test: loss=0.059356371923393755 acc=0.972463768115942\n",
      "EPOCH=1022\n",
      "train: loss=0.040783890987150004 acc=0.9797101449275363\n",
      "test: loss=0.058621652931712794 acc=0.9710144927536232\n",
      "EPOCH=1023\n",
      "train: loss=0.0482812718244455 acc=0.9739130434782609\n",
      "test: loss=0.059183094027857625 acc=0.972463768115942\n",
      "EPOCH=1024\n",
      "train: loss=0.032897392738353765 acc=0.9898550724637681\n",
      "test: loss=0.06019862579800864 acc=0.9753623188405797\n",
      "EPOCH=1025\n",
      "train: loss=0.02480245240227319 acc=0.991304347826087\n",
      "test: loss=0.057190451535878956 acc=0.9739130434782609\n",
      "EPOCH=1026\n",
      "train: loss=0.05515677340862713 acc=0.9768115942028985\n",
      "test: loss=0.05611848254350089 acc=0.9753623188405797\n",
      "EPOCH=1027\n",
      "train: loss=0.03204260430550549 acc=0.9884057971014493\n",
      "test: loss=0.0564726701405836 acc=0.9753623188405797\n",
      "EPOCH=1028\n",
      "train: loss=0.0742524965231333 acc=0.9608695652173913\n",
      "test: loss=0.06118212488500638 acc=0.972463768115942\n",
      "EPOCH=1029\n",
      "train: loss=0.03763139676485877 acc=0.9840579710144928\n",
      "test: loss=0.058903260378529575 acc=0.9753623188405797\n",
      "EPOCH=1030\n",
      "train: loss=0.03576550311467116 acc=0.9826086956521739\n",
      "test: loss=0.05837663195315809 acc=0.9739130434782609\n",
      "EPOCH=1031\n",
      "train: loss=0.06772526760146212 acc=0.9652173913043478\n",
      "test: loss=0.056569194678791984 acc=0.9753623188405797\n",
      "EPOCH=1032\n",
      "train: loss=0.05356214512497717 acc=0.9739130434782609\n",
      "test: loss=0.05670533083845217 acc=0.972463768115942\n",
      "EPOCH=1033\n",
      "train: loss=0.04234857423096926 acc=0.9782608695652174\n",
      "test: loss=0.059145229761403885 acc=0.9739130434782609\n",
      "EPOCH=1034\n",
      "train: loss=0.035814859579075486 acc=0.9855072463768116\n",
      "test: loss=0.05795619766803913 acc=0.9753623188405797\n",
      "EPOCH=1035\n",
      "train: loss=0.060574448329502394 acc=0.9710144927536232\n",
      "test: loss=0.05604529782328637 acc=0.9739130434782609\n",
      "EPOCH=1036\n",
      "train: loss=0.07000145833909928 acc=0.9753623188405797\n",
      "test: loss=0.05515433081141871 acc=0.9753623188405797\n",
      "EPOCH=1037\n",
      "train: loss=0.0771138850324389 acc=0.9637681159420289\n",
      "test: loss=0.05899913388286612 acc=0.9681159420289855\n",
      "EPOCH=1038\n",
      "train: loss=0.02988795881160414 acc=0.9826086956521739\n",
      "test: loss=0.05820770168443217 acc=0.972463768115942\n",
      "EPOCH=1039\n",
      "train: loss=0.050586610009134975 acc=0.9782608695652174\n",
      "test: loss=0.056791648492637334 acc=0.9768115942028985\n",
      "EPOCH=1040\n",
      "train: loss=0.03155019763030507 acc=0.9855072463768116\n",
      "test: loss=0.06087136064748325 acc=0.9753623188405797\n",
      "EPOCH=1041\n",
      "train: loss=0.04384913563153935 acc=0.9768115942028985\n",
      "test: loss=0.05692489994981259 acc=0.9695652173913043\n",
      "EPOCH=1042\n",
      "train: loss=0.04152101243447014 acc=0.9782608695652174\n",
      "test: loss=0.058155347466393385 acc=0.9782608695652174\n",
      "EPOCH=1043\n",
      "train: loss=0.07945996558662113 acc=0.9623188405797102\n",
      "test: loss=0.05748332957600438 acc=0.972463768115942\n",
      "EPOCH=1044\n",
      "train: loss=0.04431442558252001 acc=0.9739130434782609\n",
      "test: loss=0.0583900292201126 acc=0.9710144927536232\n",
      "EPOCH=1045\n",
      "train: loss=0.039618869489803726 acc=0.981159420289855\n",
      "test: loss=0.05655209796246801 acc=0.9710144927536232\n",
      "EPOCH=1046\n",
      "train: loss=0.05566035783813419 acc=0.9753623188405797\n",
      "test: loss=0.056040053902733714 acc=0.9710144927536232\n",
      "EPOCH=1047\n",
      "train: loss=0.05757235018723541 acc=0.9710144927536232\n",
      "test: loss=0.058474369480327526 acc=0.9753623188405797\n",
      "EPOCH=1048\n",
      "train: loss=0.047006438203198594 acc=0.9753623188405797\n",
      "test: loss=0.0596510037544157 acc=0.9637681159420289\n",
      "EPOCH=1049\n",
      "train: loss=0.06382375069754773 acc=0.9666666666666667\n",
      "test: loss=0.05851880596633425 acc=0.9753623188405797\n",
      "EPOCH=1050\n",
      "train: loss=0.0758677739371228 acc=0.9550724637681159\n",
      "test: loss=0.05770807963253296 acc=0.9753623188405797\n",
      "EPOCH=1051\n",
      "train: loss=0.03487438165306641 acc=0.9855072463768116\n",
      "test: loss=0.058622492508084115 acc=0.9710144927536232\n",
      "EPOCH=1052\n",
      "train: loss=0.05334223386871182 acc=0.9768115942028985\n",
      "test: loss=0.055901218859832194 acc=0.972463768115942\n",
      "EPOCH=1053\n",
      "train: loss=0.043182998689306785 acc=0.9768115942028985\n",
      "test: loss=0.05733806210548824 acc=0.9739130434782609\n",
      "EPOCH=1054\n",
      "train: loss=0.056027134105464685 acc=0.9710144927536232\n",
      "test: loss=0.055455110326894895 acc=0.9782608695652174\n",
      "EPOCH=1055\n",
      "train: loss=0.04715053777305354 acc=0.9753623188405797\n",
      "test: loss=0.05459313629843146 acc=0.9739130434782609\n",
      "EPOCH=1056\n",
      "train: loss=0.0455215139428145 acc=0.9753623188405797\n",
      "test: loss=0.05648857269901008 acc=0.9710144927536232\n",
      "EPOCH=1057\n",
      "train: loss=0.03248749327848316 acc=0.9855072463768116\n",
      "test: loss=0.06039548247949847 acc=0.9739130434782609\n",
      "EPOCH=1058\n",
      "train: loss=0.1252712399997163 acc=0.9521739130434783\n",
      "test: loss=0.20385630528504256 acc=0.9304347826086956\n",
      "EPOCH=1059\n",
      "train: loss=0.3153516249881679 acc=0.8985507246376812\n",
      "test: loss=0.4760152251526579 acc=0.8797101449275362\n",
      "EPOCH=1060\n",
      "train: loss=0.26071080719553813 acc=0.9347826086956522\n",
      "test: loss=0.2723070922349816 acc=0.9101449275362319\n",
      "EPOCH=1061\n",
      "train: loss=0.07529401425583897 acc=0.9637681159420289\n",
      "test: loss=0.09233352421826187 acc=0.9681159420289855\n",
      "EPOCH=1062\n",
      "train: loss=0.11413183125721657 acc=0.9579710144927536\n",
      "test: loss=0.0742726060306344 acc=0.9652173913043478\n",
      "EPOCH=1063\n",
      "train: loss=0.12133183479133802 acc=0.9565217391304348\n",
      "test: loss=0.16040379431371907 acc=0.9391304347826087\n",
      "EPOCH=1064\n",
      "train: loss=0.10766367897554571 acc=0.9565217391304348\n",
      "test: loss=0.08615179931581537 acc=0.9637681159420289\n",
      "EPOCH=1065\n",
      "train: loss=0.08304353225894068 acc=0.9608695652173913\n",
      "test: loss=0.12259935698501341 acc=0.9550724637681159\n",
      "EPOCH=1066\n",
      "train: loss=0.11270303375265842 acc=0.946376811594203\n",
      "test: loss=0.12167132570788226 acc=0.9492753623188406\n",
      "EPOCH=1067\n",
      "train: loss=0.09461913363749037 acc=0.9623188405797102\n",
      "test: loss=0.1423679646165964 acc=0.9565217391304348\n",
      "EPOCH=1068\n",
      "train: loss=0.06866170469943095 acc=0.9666666666666667\n",
      "test: loss=0.06596537397885464 acc=0.972463768115942\n",
      "EPOCH=1069\n",
      "train: loss=0.08466833537869395 acc=0.9710144927536232\n",
      "test: loss=0.11462116562661025 acc=0.9536231884057971\n",
      "EPOCH=1070\n",
      "train: loss=0.05063533712090758 acc=0.9782608695652174\n",
      "test: loss=0.06375443042814533 acc=0.9695652173913043\n",
      "EPOCH=1071\n",
      "train: loss=0.08380700894526485 acc=0.9623188405797102\n",
      "test: loss=0.06712502983956727 acc=0.9695652173913043\n",
      "EPOCH=1072\n",
      "train: loss=0.06870834454456336 acc=0.9782608695652174\n",
      "test: loss=0.07026599768294399 acc=0.9695652173913043\n",
      "EPOCH=1073\n",
      "train: loss=0.05852524394894651 acc=0.9782608695652174\n",
      "test: loss=0.06291674123604923 acc=0.9753623188405797\n",
      "EPOCH=1074\n",
      "train: loss=0.06592286317919134 acc=0.9753623188405797\n",
      "test: loss=0.07933558740781492 acc=0.9681159420289855\n",
      "EPOCH=1075\n",
      "train: loss=0.04967807549860841 acc=0.9768115942028985\n",
      "test: loss=0.059730507634465205 acc=0.9739130434782609\n",
      "EPOCH=1076\n",
      "train: loss=0.03325104324018664 acc=0.9826086956521739\n",
      "test: loss=0.07206380698859723 acc=0.9710144927536232\n",
      "EPOCH=1077\n",
      "train: loss=0.07896230909388043 acc=0.9623188405797102\n",
      "test: loss=0.06942553996237531 acc=0.9739130434782609\n",
      "EPOCH=1078\n",
      "train: loss=0.06415995903057951 acc=0.9695652173913043\n",
      "test: loss=0.07986670357345066 acc=0.9666666666666667\n",
      "EPOCH=1079\n",
      "train: loss=0.061756553201148055 acc=0.9753623188405797\n",
      "test: loss=0.058902627472266225 acc=0.9768115942028985\n",
      "EPOCH=1080\n",
      "train: loss=0.0572439234239268 acc=0.9768115942028985\n",
      "test: loss=0.06361644098647772 acc=0.9739130434782609\n",
      "EPOCH=1081\n",
      "train: loss=0.04382749356906639 acc=0.9768115942028985\n",
      "test: loss=0.07130738898337367 acc=0.9637681159420289\n",
      "EPOCH=1082\n",
      "train: loss=0.0740557441171903 acc=0.9623188405797102\n",
      "test: loss=0.06537067767290519 acc=0.9710144927536232\n",
      "EPOCH=1083\n",
      "train: loss=0.06694556383642952 acc=0.9739130434782609\n",
      "test: loss=0.06307617579761343 acc=0.9768115942028985\n",
      "EPOCH=1084\n",
      "train: loss=0.060974564858377885 acc=0.972463768115942\n",
      "test: loss=0.06001658231236914 acc=0.9768115942028985\n",
      "EPOCH=1085\n",
      "train: loss=0.07077652924252474 acc=0.9695652173913043\n",
      "test: loss=0.06804171950729812 acc=0.9681159420289855\n",
      "EPOCH=1086\n",
      "train: loss=0.03429571674868447 acc=0.9884057971014493\n",
      "test: loss=0.07007364473824877 acc=0.9695652173913043\n",
      "EPOCH=1087\n",
      "train: loss=0.05035935113710261 acc=0.972463768115942\n",
      "test: loss=0.06725359773762504 acc=0.972463768115942\n",
      "EPOCH=1088\n",
      "train: loss=0.040696216153945974 acc=0.9855072463768116\n",
      "test: loss=0.06693069401187088 acc=0.9695652173913043\n",
      "EPOCH=1089\n",
      "train: loss=0.04461983425936673 acc=0.9840579710144928\n",
      "test: loss=0.058863843792325 acc=0.9782608695652174\n",
      "EPOCH=1090\n",
      "train: loss=0.04858413572574474 acc=0.9782608695652174\n",
      "test: loss=0.060654413711193664 acc=0.9739130434782609\n",
      "EPOCH=1091\n",
      "train: loss=0.020591073552314206 acc=0.991304347826087\n",
      "test: loss=0.06181329139827998 acc=0.9695652173913043\n",
      "EPOCH=1092\n",
      "train: loss=0.05075608644484649 acc=0.9753623188405797\n",
      "test: loss=0.06648307284654473 acc=0.9710144927536232\n",
      "EPOCH=1093\n",
      "train: loss=0.07507250025712767 acc=0.9666666666666667\n",
      "test: loss=0.05969465055038796 acc=0.972463768115942\n",
      "EPOCH=1094\n",
      "train: loss=0.06368240396719659 acc=0.9695652173913043\n",
      "test: loss=0.05869883919222065 acc=0.9753623188405797\n",
      "EPOCH=1095\n",
      "train: loss=0.052044988904905984 acc=0.9782608695652174\n",
      "test: loss=0.06828465681490205 acc=0.972463768115942\n",
      "EPOCH=1096\n",
      "train: loss=0.060740481236792994 acc=0.9666666666666667\n",
      "test: loss=0.05727821490160523 acc=0.9753623188405797\n",
      "EPOCH=1097\n",
      "train: loss=0.046936004566787155 acc=0.9797101449275363\n",
      "test: loss=0.06233617501866268 acc=0.9666666666666667\n",
      "EPOCH=1098\n",
      "train: loss=0.04777133700253233 acc=0.9739130434782609\n",
      "test: loss=0.06084427863316569 acc=0.9695652173913043\n",
      "EPOCH=1099\n",
      "train: loss=0.07027447548486004 acc=0.9637681159420289\n",
      "test: loss=0.060343726448223556 acc=0.9695652173913043\n",
      "EPOCH=1100\n",
      "train: loss=0.04686686477266556 acc=0.981159420289855\n",
      "test: loss=0.05957304584231985 acc=0.9710144927536232\n",
      "EPOCH=1101\n",
      "train: loss=0.049487174307885885 acc=0.9768115942028985\n",
      "test: loss=0.0737307403028755 acc=0.972463768115942\n",
      "EPOCH=1102\n",
      "train: loss=0.06333379625069392 acc=0.9695652173913043\n",
      "test: loss=0.06577766493745946 acc=0.9695652173913043\n",
      "EPOCH=1103\n",
      "train: loss=0.03514866934879879 acc=0.9826086956521739\n",
      "test: loss=0.06490599182041876 acc=0.972463768115942\n",
      "EPOCH=1104\n",
      "train: loss=0.045077194875580096 acc=0.9782608695652174\n",
      "test: loss=0.0661243950051205 acc=0.9739130434782609\n",
      "EPOCH=1105\n",
      "train: loss=0.0643972447726855 acc=0.9710144927536232\n",
      "test: loss=0.0643760652676796 acc=0.9695652173913043\n",
      "EPOCH=1106\n",
      "train: loss=0.047416024045442436 acc=0.981159420289855\n",
      "test: loss=0.06404636840123369 acc=0.9710144927536232\n",
      "EPOCH=1107\n",
      "train: loss=0.05061132458347649 acc=0.9768115942028985\n",
      "test: loss=0.059433654662149754 acc=0.972463768115942\n",
      "EPOCH=1108\n",
      "train: loss=0.06741833008712135 acc=0.9695652173913043\n",
      "test: loss=0.0614611235722172 acc=0.972463768115942\n",
      "EPOCH=1109\n",
      "train: loss=0.03160697638223349 acc=0.9869565217391304\n",
      "test: loss=0.06062301876805753 acc=0.9739130434782609\n",
      "EPOCH=1110\n",
      "train: loss=0.06168796369229399 acc=0.9695652173913043\n",
      "test: loss=0.06010625007274828 acc=0.972463768115942\n",
      "EPOCH=1111\n",
      "train: loss=0.05641487192325941 acc=0.9768115942028985\n",
      "test: loss=0.06453068128975238 acc=0.972463768115942\n",
      "EPOCH=1112\n",
      "train: loss=0.047999328705169875 acc=0.9797101449275363\n",
      "test: loss=0.06696917963455844 acc=0.9695652173913043\n",
      "EPOCH=1113\n",
      "train: loss=0.04996533320482157 acc=0.9782608695652174\n",
      "test: loss=0.05949861823793814 acc=0.9710144927536232\n",
      "EPOCH=1114\n",
      "train: loss=0.03437820145203856 acc=0.9869565217391304\n",
      "test: loss=0.06652808503777652 acc=0.9695652173913043\n",
      "EPOCH=1115\n",
      "train: loss=0.05799786984554077 acc=0.9681159420289855\n",
      "test: loss=0.06763444001800953 acc=0.9652173913043478\n",
      "EPOCH=1116\n",
      "train: loss=0.05461257727856668 acc=0.9739130434782609\n",
      "test: loss=0.06165882205246325 acc=0.972463768115942\n",
      "EPOCH=1117\n",
      "train: loss=0.054582268363285766 acc=0.9710144927536232\n",
      "test: loss=0.061159082664642664 acc=0.972463768115942\n",
      "EPOCH=1118\n",
      "train: loss=0.052479660798173434 acc=0.9710144927536232\n",
      "test: loss=0.06337132436698928 acc=0.972463768115942\n",
      "EPOCH=1119\n",
      "train: loss=0.05185897448709457 acc=0.9797101449275363\n",
      "test: loss=0.06333254956281335 acc=0.9695652173913043\n",
      "EPOCH=1120\n",
      "train: loss=0.07380251033126449 acc=0.9637681159420289\n",
      "test: loss=0.05678162772059745 acc=0.9739130434782609\n",
      "EPOCH=1121\n",
      "train: loss=0.051280219152238446 acc=0.9753623188405797\n",
      "test: loss=0.06266619473017357 acc=0.9710144927536232\n",
      "EPOCH=1122\n",
      "train: loss=0.031617531316105243 acc=0.9884057971014493\n",
      "test: loss=0.060212540558878924 acc=0.9739130434782609\n",
      "EPOCH=1123\n",
      "train: loss=0.046617225029561817 acc=0.9753623188405797\n",
      "test: loss=0.0634570312040177 acc=0.972463768115942\n",
      "EPOCH=1124\n",
      "train: loss=0.040793197167184055 acc=0.981159420289855\n",
      "test: loss=0.06427673466878249 acc=0.9695652173913043\n",
      "EPOCH=1125\n",
      "train: loss=0.06391643174369974 acc=0.9666666666666667\n",
      "test: loss=0.061225544861093446 acc=0.9710144927536232\n",
      "EPOCH=1126\n",
      "train: loss=0.059748540786560556 acc=0.9753623188405797\n",
      "test: loss=0.0657139038545569 acc=0.972463768115942\n",
      "EPOCH=1127\n",
      "train: loss=0.049173017363670274 acc=0.9782608695652174\n",
      "test: loss=0.06128189257521564 acc=0.9753623188405797\n",
      "EPOCH=1128\n",
      "train: loss=0.062147421325306575 acc=0.9666666666666667\n",
      "test: loss=0.05884762443675248 acc=0.9782608695652174\n",
      "EPOCH=1129\n",
      "train: loss=0.05274905425983117 acc=0.9797101449275363\n",
      "test: loss=0.05955059709707725 acc=0.9768115942028985\n",
      "EPOCH=1130\n",
      "train: loss=0.04105955113077033 acc=0.981159420289855\n",
      "test: loss=0.059847653982996296 acc=0.9782608695652174\n",
      "EPOCH=1131\n",
      "train: loss=0.04647800862208571 acc=0.9753623188405797\n",
      "test: loss=0.06255686893577607 acc=0.9695652173913043\n",
      "EPOCH=1132\n",
      "train: loss=0.055405358827851185 acc=0.9681159420289855\n",
      "test: loss=0.06218756636537376 acc=0.9753623188405797\n",
      "EPOCH=1133\n",
      "train: loss=0.04254801245190641 acc=0.981159420289855\n",
      "test: loss=0.06034086937143227 acc=0.972463768115942\n",
      "EPOCH=1134\n",
      "train: loss=0.05912622970337447 acc=0.9753623188405797\n",
      "test: loss=0.07211199032649762 acc=0.9695652173913043\n",
      "EPOCH=1135\n",
      "train: loss=0.047922594430655455 acc=0.9840579710144928\n",
      "test: loss=0.06283174625734425 acc=0.972463768115942\n",
      "EPOCH=1136\n",
      "train: loss=0.06524063879342452 acc=0.9681159420289855\n",
      "test: loss=0.061914641651840865 acc=0.9753623188405797\n",
      "EPOCH=1137\n",
      "train: loss=0.031558826938344664 acc=0.9826086956521739\n",
      "test: loss=0.06565786395653932 acc=0.9710144927536232\n",
      "EPOCH=1138\n",
      "train: loss=0.060637711186847874 acc=0.9710144927536232\n",
      "test: loss=0.05959663491433419 acc=0.9753623188405797\n",
      "EPOCH=1139\n",
      "train: loss=0.05731567208699542 acc=0.9695652173913043\n",
      "test: loss=0.05783804691712115 acc=0.9739130434782609\n",
      "EPOCH=1140\n",
      "train: loss=0.04574003908742047 acc=0.981159420289855\n",
      "test: loss=0.06393380772887239 acc=0.9710144927536232\n",
      "EPOCH=1141\n",
      "train: loss=0.05634858587265308 acc=0.9710144927536232\n",
      "test: loss=0.06135751808735958 acc=0.9739130434782609\n",
      "EPOCH=1142\n",
      "train: loss=0.055675563552015195 acc=0.9768115942028985\n",
      "test: loss=0.05862909418418476 acc=0.9753623188405797\n",
      "EPOCH=1143\n",
      "train: loss=0.05799357539304921 acc=0.9710144927536232\n",
      "test: loss=0.06050013637541794 acc=0.9768115942028985\n",
      "EPOCH=1144\n",
      "train: loss=0.03686521090022282 acc=0.9884057971014493\n",
      "test: loss=0.062114809807843584 acc=0.972463768115942\n",
      "EPOCH=1145\n",
      "train: loss=0.045593911385499335 acc=0.981159420289855\n",
      "test: loss=0.05964582412099264 acc=0.9710144927536232\n",
      "EPOCH=1146\n",
      "train: loss=0.04880639480819657 acc=0.9753623188405797\n",
      "test: loss=0.06250296079804073 acc=0.9739130434782609\n",
      "EPOCH=1147\n",
      "train: loss=0.040480846967106125 acc=0.981159420289855\n",
      "test: loss=0.06346929220025352 acc=0.9666666666666667\n",
      "EPOCH=1148\n",
      "train: loss=0.05621051315365227 acc=0.9681159420289855\n",
      "test: loss=0.06271021071701342 acc=0.972463768115942\n",
      "EPOCH=1149\n",
      "train: loss=0.033376557770278904 acc=0.9826086956521739\n",
      "test: loss=0.06053781922555502 acc=0.9710144927536232\n",
      "EPOCH=1150\n",
      "train: loss=0.05508794060890799 acc=0.9739130434782609\n",
      "test: loss=0.06552583158156414 acc=0.9739130434782609\n",
      "EPOCH=1151\n",
      "train: loss=0.046050768244746146 acc=0.9855072463768116\n",
      "test: loss=0.057917688023913304 acc=0.9768115942028985\n",
      "EPOCH=1152\n",
      "train: loss=0.03779062898790156 acc=0.981159420289855\n",
      "test: loss=0.05677206065890102 acc=0.9768115942028985\n",
      "EPOCH=1153\n",
      "train: loss=0.04057031733424157 acc=0.9840579710144928\n",
      "test: loss=0.062239110716001085 acc=0.972463768115942\n",
      "EPOCH=1154\n",
      "train: loss=0.0605023420893088 acc=0.9695652173913043\n",
      "test: loss=0.06173357524032954 acc=0.9681159420289855\n",
      "EPOCH=1155\n",
      "train: loss=0.0396728691341264 acc=0.9855072463768116\n",
      "test: loss=0.058945779816126906 acc=0.9710144927536232\n",
      "EPOCH=1156\n",
      "train: loss=0.03137419118771175 acc=0.9855072463768116\n",
      "test: loss=0.05935652035243624 acc=0.9768115942028985\n",
      "EPOCH=1157\n",
      "train: loss=0.06330986361085306 acc=0.9797101449275363\n",
      "test: loss=0.06221459574052246 acc=0.972463768115942\n",
      "EPOCH=1158\n",
      "train: loss=0.035686602325197374 acc=0.981159420289855\n",
      "test: loss=0.06116565272818985 acc=0.9739130434782609\n",
      "EPOCH=1159\n",
      "train: loss=0.0627190541560804 acc=0.9666666666666667\n",
      "test: loss=0.06327793321294573 acc=0.9753623188405797\n",
      "EPOCH=1160\n",
      "train: loss=0.04953493822031373 acc=0.9768115942028985\n",
      "test: loss=0.06562050041961134 acc=0.9710144927536232\n",
      "EPOCH=1161\n",
      "train: loss=0.05061875474155193 acc=0.9739130434782609\n",
      "test: loss=0.06236720276416093 acc=0.972463768115942\n",
      "EPOCH=1162\n",
      "train: loss=0.04887894527489727 acc=0.9768115942028985\n",
      "test: loss=0.06231681087786986 acc=0.9782608695652174\n",
      "EPOCH=1163\n",
      "train: loss=0.07939810305961668 acc=0.9666666666666667\n",
      "test: loss=0.059616017090152275 acc=0.9739130434782609\n",
      "EPOCH=1164\n",
      "train: loss=0.048453703483039505 acc=0.9797101449275363\n",
      "test: loss=0.06866185109969777 acc=0.972463768115942\n",
      "EPOCH=1165\n",
      "train: loss=0.05939493335143734 acc=0.9710144927536232\n",
      "test: loss=0.06161061154810241 acc=0.972463768115942\n",
      "EPOCH=1166\n",
      "train: loss=0.056850496073492636 acc=0.9695652173913043\n",
      "test: loss=0.05702635806865281 acc=0.9768115942028985\n",
      "EPOCH=1167\n",
      "train: loss=0.04749903792166049 acc=0.9840579710144928\n",
      "test: loss=0.06440036174780614 acc=0.9666666666666667\n",
      "EPOCH=1168\n",
      "train: loss=0.05990162633554732 acc=0.9681159420289855\n",
      "test: loss=0.06054690629865534 acc=0.9768115942028985\n",
      "EPOCH=1169\n",
      "train: loss=0.06890935499174741 acc=0.9681159420289855\n",
      "test: loss=0.06117555713102892 acc=0.972463768115942\n",
      "EPOCH=1170\n",
      "train: loss=0.052310214611414746 acc=0.9753623188405797\n",
      "test: loss=0.05802772564448514 acc=0.9782608695652174\n",
      "EPOCH=1171\n",
      "train: loss=0.07124897271153041 acc=0.9666666666666667\n",
      "test: loss=0.06310461483288222 acc=0.9739130434782609\n",
      "EPOCH=1172\n",
      "train: loss=0.07226174050218956 acc=0.9666666666666667\n",
      "test: loss=0.062203997011325 acc=0.9768115942028985\n",
      "EPOCH=1173\n",
      "train: loss=0.04151080331764706 acc=0.9826086956521739\n",
      "test: loss=0.07007065696308191 acc=0.972463768115942\n",
      "EPOCH=1174\n",
      "train: loss=0.044335286575734316 acc=0.981159420289855\n",
      "test: loss=0.07006280215939531 acc=0.9666666666666667\n",
      "EPOCH=1175\n",
      "train: loss=0.05901965968378426 acc=0.9681159420289855\n",
      "test: loss=0.05846204529307936 acc=0.9666666666666667\n",
      "EPOCH=1176\n",
      "train: loss=0.044339922857215976 acc=0.9768115942028985\n",
      "test: loss=0.061740653978536744 acc=0.9739130434782609\n",
      "EPOCH=1177\n",
      "train: loss=0.03583317870629177 acc=0.9855072463768116\n",
      "test: loss=0.06085287646307036 acc=0.9739130434782609\n",
      "EPOCH=1178\n",
      "train: loss=0.05217191409445011 acc=0.9797101449275363\n",
      "test: loss=0.068694819890842 acc=0.9710144927536232\n",
      "EPOCH=1179\n",
      "train: loss=0.05401050990115878 acc=0.972463768115942\n",
      "test: loss=0.06278622808178488 acc=0.9753623188405797\n",
      "EPOCH=1180\n",
      "train: loss=0.05388824366181024 acc=0.9753623188405797\n",
      "test: loss=0.05941674420761637 acc=0.9739130434782609\n",
      "EPOCH=1181\n",
      "train: loss=0.041660400913530876 acc=0.9826086956521739\n",
      "test: loss=0.06068168491284303 acc=0.9681159420289855\n",
      "EPOCH=1182\n",
      "train: loss=0.025829243443385115 acc=0.9869565217391304\n",
      "test: loss=0.06157368032029758 acc=0.9753623188405797\n",
      "EPOCH=1183\n",
      "train: loss=0.03513020151690901 acc=0.9869565217391304\n",
      "test: loss=0.06664708723809577 acc=0.972463768115942\n",
      "EPOCH=1184\n",
      "train: loss=0.05556814224707632 acc=0.972463768115942\n",
      "test: loss=0.06154067826350976 acc=0.9753623188405797\n",
      "EPOCH=1185\n",
      "train: loss=0.053608818779510244 acc=0.9768115942028985\n",
      "test: loss=0.06177804962638906 acc=0.9710144927536232\n",
      "EPOCH=1186\n",
      "train: loss=0.06524022840044102 acc=0.9695652173913043\n",
      "test: loss=0.05839450504799523 acc=0.9753623188405797\n",
      "EPOCH=1187\n",
      "train: loss=0.05001094241196885 acc=0.9710144927536232\n",
      "test: loss=0.06098427444427333 acc=0.9753623188405797\n",
      "EPOCH=1188\n",
      "train: loss=0.050928356205559015 acc=0.981159420289855\n",
      "test: loss=0.060300061617495536 acc=0.9782608695652174\n",
      "EPOCH=1189\n",
      "train: loss=0.0613611427872721 acc=0.9710144927536232\n",
      "test: loss=0.06163076103869564 acc=0.972463768115942\n",
      "EPOCH=1190\n",
      "train: loss=0.050464122181738846 acc=0.981159420289855\n",
      "test: loss=0.06165168982021105 acc=0.972463768115942\n",
      "EPOCH=1191\n",
      "train: loss=0.03812834310802843 acc=0.981159420289855\n",
      "test: loss=0.056371727618163316 acc=0.9739130434782609\n",
      "EPOCH=1192\n",
      "train: loss=0.08225430189790754 acc=0.9579710144927536\n",
      "test: loss=0.05642823515964369 acc=0.9753623188405797\n",
      "EPOCH=1193\n",
      "train: loss=0.03811187458506697 acc=0.9826086956521739\n",
      "test: loss=0.05922891962558631 acc=0.9695652173913043\n",
      "EPOCH=1194\n",
      "train: loss=0.045826031190954054 acc=0.9797101449275363\n",
      "test: loss=0.058566161138066064 acc=0.972463768115942\n",
      "EPOCH=1195\n",
      "train: loss=0.8547687775690415 acc=0.7217391304347827\n",
      "test: loss=1.1915996861910108 acc=0.6536231884057971\n",
      "EPOCH=1196\n",
      "train: loss=0.09253333536442535 acc=0.9710144927536232\n",
      "test: loss=0.11326981004687128 acc=0.9623188405797102\n",
      "EPOCH=1197\n",
      "train: loss=0.14116537638279533 acc=0.9478260869565217\n",
      "test: loss=0.14311135759775878 acc=0.9434782608695652\n",
      "EPOCH=1198\n",
      "train: loss=0.08840311720559868 acc=0.9652173913043478\n",
      "test: loss=0.10989734322046185 acc=0.9579710144927536\n",
      "EPOCH=1199\n",
      "train: loss=0.10353407428764486 acc=0.9608695652173913\n",
      "test: loss=0.1654065897878879 acc=0.9318840579710145\n",
      "EPOCH=1200\n",
      "train: loss=0.054363339932604705 acc=0.9753623188405797\n",
      "test: loss=0.06487304015989082 acc=0.9753623188405797\n",
      "EPOCH=1201\n",
      "train: loss=0.08087869095200197 acc=0.9608695652173913\n",
      "test: loss=0.06278629075816708 acc=0.972463768115942\n",
      "EPOCH=1202\n",
      "train: loss=0.07295411860753187 acc=0.9666666666666667\n",
      "test: loss=0.08550000840587339 acc=0.9681159420289855\n",
      "EPOCH=1203\n",
      "train: loss=0.08631607905292278 acc=0.9666666666666667\n",
      "test: loss=0.08015444161081282 acc=0.9695652173913043\n",
      "EPOCH=1204\n",
      "train: loss=0.058394882390534066 acc=0.9710144927536232\n",
      "test: loss=0.07748366322332327 acc=0.9565217391304348\n",
      "EPOCH=1205\n",
      "train: loss=0.07336180863189515 acc=0.972463768115942\n",
      "test: loss=0.07605991842923744 acc=0.9666666666666667\n",
      "EPOCH=1206\n",
      "train: loss=0.06220254648087674 acc=0.9739130434782609\n",
      "test: loss=0.07680665051825551 acc=0.9623188405797102\n",
      "EPOCH=1207\n",
      "train: loss=0.05412925628031107 acc=0.9753623188405797\n",
      "test: loss=0.08501914526566669 acc=0.9637681159420289\n",
      "EPOCH=1208\n",
      "train: loss=0.042019254684529266 acc=0.9840579710144928\n",
      "test: loss=0.0680376960440429 acc=0.9652173913043478\n",
      "EPOCH=1209\n",
      "train: loss=0.04418302932348207 acc=0.9782608695652174\n",
      "test: loss=0.06459520088091066 acc=0.9681159420289855\n",
      "EPOCH=1210\n",
      "train: loss=0.04733931324651968 acc=0.9797101449275363\n",
      "test: loss=0.06699900507167127 acc=0.9666666666666667\n",
      "EPOCH=1211\n",
      "train: loss=0.08347528582918017 acc=0.9637681159420289\n",
      "test: loss=0.06201474494154166 acc=0.9666666666666667\n",
      "EPOCH=1212\n",
      "train: loss=0.0768288807607989 acc=0.9652173913043478\n",
      "test: loss=0.06484707906771384 acc=0.9652173913043478\n",
      "EPOCH=1213\n",
      "train: loss=0.09733669989311505 acc=0.9594202898550724\n",
      "test: loss=0.07075414791258289 acc=0.972463768115942\n",
      "EPOCH=1214\n",
      "train: loss=0.06923751829579645 acc=0.9666666666666667\n",
      "test: loss=0.07041012373074466 acc=0.9695652173913043\n",
      "EPOCH=1215\n",
      "train: loss=0.052701001531452775 acc=0.9739130434782609\n",
      "test: loss=0.07006297974115057 acc=0.9695652173913043\n",
      "EPOCH=1216\n",
      "train: loss=0.04310945890509163 acc=0.9826086956521739\n",
      "test: loss=0.06752770147289226 acc=0.9666666666666667\n",
      "EPOCH=1217\n",
      "train: loss=0.049294511522463016 acc=0.9797101449275363\n",
      "test: loss=0.07163132968897731 acc=0.9666666666666667\n",
      "EPOCH=1218\n",
      "train: loss=0.04519841539093285 acc=0.9797101449275363\n",
      "test: loss=0.06697466115665984 acc=0.972463768115942\n",
      "EPOCH=1219\n",
      "train: loss=0.05056139362941174 acc=0.9753623188405797\n",
      "test: loss=0.0690112002685059 acc=0.9710144927536232\n",
      "EPOCH=1220\n",
      "train: loss=0.04962806222153074 acc=0.9782608695652174\n",
      "test: loss=0.06495489443479563 acc=0.972463768115942\n",
      "EPOCH=1221\n",
      "train: loss=0.06466302580335914 acc=0.9637681159420289\n",
      "test: loss=0.06771838232230143 acc=0.9710144927536232\n",
      "EPOCH=1222\n",
      "train: loss=0.0738720362708142 acc=0.9652173913043478\n",
      "test: loss=0.0661142107097055 acc=0.9695652173913043\n",
      "EPOCH=1223\n",
      "train: loss=0.05756358670090265 acc=0.9753623188405797\n",
      "test: loss=0.06847921281574453 acc=0.9666666666666667\n",
      "EPOCH=1224\n",
      "train: loss=0.06675786786180413 acc=0.9739130434782609\n",
      "test: loss=0.06616750905895732 acc=0.9681159420289855\n",
      "EPOCH=1225\n",
      "train: loss=0.07141117767325238 acc=0.9681159420289855\n",
      "test: loss=0.0672076690922308 acc=0.972463768115942\n",
      "EPOCH=1226\n",
      "train: loss=0.06393613453724278 acc=0.9681159420289855\n",
      "test: loss=0.06984334233013927 acc=0.9710144927536232\n",
      "EPOCH=1227\n",
      "train: loss=0.06652039782983715 acc=0.972463768115942\n",
      "test: loss=0.06487081661126926 acc=0.9739130434782609\n",
      "EPOCH=1228\n",
      "train: loss=0.05860202649724364 acc=0.9681159420289855\n",
      "test: loss=0.06715012394503352 acc=0.9652173913043478\n",
      "EPOCH=1229\n",
      "train: loss=0.07362935599448354 acc=0.9652173913043478\n",
      "test: loss=0.07242515877758791 acc=0.9666666666666667\n",
      "EPOCH=1230\n",
      "train: loss=0.04300699161036499 acc=0.9855072463768116\n",
      "test: loss=0.0670517948323979 acc=0.9710144927536232\n",
      "EPOCH=1231\n",
      "train: loss=0.05691820290446178 acc=0.9666666666666667\n",
      "test: loss=0.06692474738570069 acc=0.9739130434782609\n",
      "EPOCH=1232\n",
      "train: loss=0.05251047510933636 acc=0.9739130434782609\n",
      "test: loss=0.06506239858917345 acc=0.9652173913043478\n",
      "EPOCH=1233\n",
      "train: loss=0.054949529857495794 acc=0.9739130434782609\n",
      "test: loss=0.06905208673479672 acc=0.9710144927536232\n",
      "EPOCH=1234\n",
      "train: loss=0.05159682298304864 acc=0.9753623188405797\n",
      "test: loss=0.06281148560543759 acc=0.9695652173913043\n",
      "EPOCH=1235\n",
      "train: loss=0.07501490743970489 acc=0.9753623188405797\n",
      "test: loss=0.06942339739215715 acc=0.9652173913043478\n",
      "EPOCH=1236\n",
      "train: loss=0.06100490587985904 acc=0.9753623188405797\n",
      "test: loss=0.064535316156682 acc=0.9652173913043478\n",
      "EPOCH=1237\n",
      "train: loss=0.048750302088850846 acc=0.9855072463768116\n",
      "test: loss=0.07366010169010268 acc=0.9710144927536232\n",
      "EPOCH=1238\n",
      "train: loss=0.05207913220957111 acc=0.9753623188405797\n",
      "test: loss=0.06509624485824689 acc=0.9695652173913043\n",
      "EPOCH=1239\n",
      "train: loss=0.07637366636041588 acc=0.9666666666666667\n",
      "test: loss=0.0691111997276254 acc=0.9681159420289855\n",
      "EPOCH=1240\n",
      "train: loss=0.06922317133202789 acc=0.9637681159420289\n",
      "test: loss=0.06349531301693954 acc=0.9681159420289855\n",
      "EPOCH=1241\n",
      "train: loss=0.04462523329100795 acc=0.9797101449275363\n",
      "test: loss=0.06554671580040189 acc=0.9710144927536232\n",
      "EPOCH=1242\n",
      "train: loss=0.07741730442669419 acc=0.9579710144927536\n",
      "test: loss=0.059936466772237074 acc=0.9739130434782609\n",
      "EPOCH=1243\n",
      "train: loss=0.055171393057559 acc=0.9768115942028985\n",
      "test: loss=0.0634186645747711 acc=0.972463768115942\n",
      "EPOCH=1244\n",
      "train: loss=0.08350166442739429 acc=0.9666666666666667\n",
      "test: loss=0.06336469057508472 acc=0.9637681159420289\n",
      "EPOCH=1245\n",
      "train: loss=0.056488022729026074 acc=0.9753623188405797\n",
      "test: loss=0.06333516533333644 acc=0.9768115942028985\n",
      "EPOCH=1246\n",
      "train: loss=0.05559725831912081 acc=0.9753623188405797\n",
      "test: loss=0.064876798039311 acc=0.9666666666666667\n",
      "EPOCH=1247\n",
      "train: loss=0.037858685719434786 acc=0.9869565217391304\n",
      "test: loss=0.06612992472428117 acc=0.9695652173913043\n",
      "EPOCH=1248\n",
      "train: loss=0.07400264465152787 acc=0.9710144927536232\n",
      "test: loss=0.06852326328995545 acc=0.9695652173913043\n",
      "EPOCH=1249\n",
      "train: loss=0.06389665838742219 acc=0.9681159420289855\n",
      "test: loss=0.06768095265019748 acc=0.9666666666666667\n",
      "EPOCH=1250\n",
      "train: loss=0.06836730780601928 acc=0.9637681159420289\n",
      "test: loss=0.0626581690633471 acc=0.9739130434782609\n",
      "EPOCH=1251\n",
      "train: loss=0.07397105120443141 acc=0.9710144927536232\n",
      "test: loss=0.06454325342710694 acc=0.9681159420289855\n",
      "EPOCH=1252\n",
      "train: loss=0.06525996955169813 acc=0.9710144927536232\n",
      "test: loss=0.0664102857524554 acc=0.9710144927536232\n",
      "EPOCH=1253\n",
      "train: loss=0.045680755759541905 acc=0.9840579710144928\n",
      "test: loss=0.06864020516080924 acc=0.9753623188405797\n",
      "EPOCH=1254\n",
      "train: loss=0.056356945528799815 acc=0.9710144927536232\n",
      "test: loss=0.06509348607692164 acc=0.9739130434782609\n",
      "EPOCH=1255\n",
      "train: loss=0.07652518901096031 acc=0.9623188405797102\n",
      "test: loss=0.0688642146242208 acc=0.9695652173913043\n",
      "EPOCH=1256\n",
      "train: loss=0.03511213278628746 acc=0.9869565217391304\n",
      "test: loss=0.06452963196282115 acc=0.9782608695652174\n",
      "EPOCH=1257\n",
      "train: loss=0.07120953224447853 acc=0.9652173913043478\n",
      "test: loss=0.06689816376628825 acc=0.9710144927536232\n",
      "EPOCH=1258\n",
      "train: loss=0.05598779252223441 acc=0.9710144927536232\n",
      "test: loss=0.06431163714172049 acc=0.972463768115942\n",
      "EPOCH=1259\n",
      "train: loss=0.04008394600534788 acc=0.9840579710144928\n",
      "test: loss=0.06443954431722637 acc=0.9753623188405797\n",
      "EPOCH=1260\n",
      "train: loss=0.038487304264606906 acc=0.9826086956521739\n",
      "test: loss=0.0687068855203479 acc=0.9695652173913043\n",
      "EPOCH=1261\n",
      "train: loss=0.04426247195905535 acc=0.9840579710144928\n",
      "test: loss=0.06536783202515095 acc=0.972463768115942\n",
      "EPOCH=1262\n",
      "train: loss=0.057800498474973115 acc=0.9739130434782609\n",
      "test: loss=0.0645500293091112 acc=0.9695652173913043\n",
      "EPOCH=1263\n",
      "train: loss=0.05704902200872557 acc=0.9753623188405797\n",
      "test: loss=0.06670265228844789 acc=0.9768115942028985\n",
      "EPOCH=1264\n",
      "train: loss=0.048386496345817515 acc=0.9753623188405797\n",
      "test: loss=0.06647168146087545 acc=0.9710144927536232\n",
      "EPOCH=1265\n",
      "train: loss=0.03973722455133529 acc=0.9855072463768116\n",
      "test: loss=0.0692345036832754 acc=0.9681159420289855\n",
      "EPOCH=1266\n",
      "train: loss=0.05894749444829341 acc=0.9710144927536232\n",
      "test: loss=0.06363298965440112 acc=0.9681159420289855\n",
      "EPOCH=1267\n",
      "train: loss=0.04761518140392468 acc=0.981159420289855\n",
      "test: loss=0.07124626469501587 acc=0.9608695652173913\n",
      "EPOCH=1268\n",
      "train: loss=0.04167315782708973 acc=0.981159420289855\n",
      "test: loss=0.066576546560981 acc=0.9666666666666667\n",
      "EPOCH=1269\n",
      "train: loss=0.03831214139807406 acc=0.9869565217391304\n",
      "test: loss=0.0651750124063606 acc=0.9710144927536232\n",
      "EPOCH=1270\n",
      "train: loss=0.05747337725347917 acc=0.9710144927536232\n",
      "test: loss=0.06203307378469645 acc=0.9753623188405797\n",
      "EPOCH=1271\n",
      "train: loss=0.05953605764932581 acc=0.9710144927536232\n",
      "test: loss=0.06438241327633698 acc=0.9695652173913043\n",
      "EPOCH=1272\n",
      "train: loss=0.05540631148933086 acc=0.9768115942028985\n",
      "test: loss=0.06888727069215173 acc=0.9666666666666667\n",
      "EPOCH=1273\n",
      "train: loss=0.04133162829707479 acc=0.9782608695652174\n",
      "test: loss=0.062117536072092 acc=0.9753623188405797\n",
      "EPOCH=1274\n",
      "train: loss=0.060865650996219184 acc=0.972463768115942\n",
      "test: loss=0.06625035956036675 acc=0.9695652173913043\n",
      "EPOCH=1275\n",
      "train: loss=0.05357587714461225 acc=0.9695652173913043\n",
      "test: loss=0.06897651032557435 acc=0.9681159420289855\n",
      "EPOCH=1276\n",
      "train: loss=0.059099372284351526 acc=0.9710144927536232\n",
      "test: loss=0.06655495814613249 acc=0.972463768115942\n",
      "EPOCH=1277\n",
      "train: loss=0.04731020179806335 acc=0.9797101449275363\n",
      "test: loss=0.06445663629759585 acc=0.9695652173913043\n",
      "EPOCH=1278\n",
      "train: loss=0.0592429197372099 acc=0.972463768115942\n",
      "test: loss=0.06779280102817285 acc=0.9710144927536232\n",
      "EPOCH=1279\n",
      "train: loss=0.05187955542414864 acc=0.9768115942028985\n",
      "test: loss=0.06220866335592143 acc=0.9710144927536232\n",
      "EPOCH=1280\n",
      "train: loss=0.059533863618579416 acc=0.9782608695652174\n",
      "test: loss=0.06157165049746174 acc=0.9768115942028985\n",
      "EPOCH=1281\n",
      "train: loss=0.0357460966411764 acc=0.9869565217391304\n",
      "test: loss=0.06400032676645437 acc=0.9753623188405797\n",
      "EPOCH=1282\n",
      "train: loss=0.042167495111699226 acc=0.9855072463768116\n",
      "test: loss=0.06276896865614193 acc=0.9695652173913043\n",
      "EPOCH=1283\n",
      "train: loss=0.04714421824359739 acc=0.981159420289855\n",
      "test: loss=0.06203017884592014 acc=0.9753623188405797\n",
      "EPOCH=1284\n",
      "train: loss=0.06113983498699956 acc=0.972463768115942\n",
      "test: loss=0.061027540888121984 acc=0.9681159420289855\n",
      "EPOCH=1285\n",
      "train: loss=0.061605773664228834 acc=0.9753623188405797\n",
      "test: loss=0.06262140486772788 acc=0.9681159420289855\n",
      "EPOCH=1286\n",
      "train: loss=0.05575827985398739 acc=0.9753623188405797\n",
      "test: loss=0.06352102007010475 acc=0.9637681159420289\n",
      "EPOCH=1287\n",
      "train: loss=0.05864148664233005 acc=0.9710144927536232\n",
      "test: loss=0.06308881237117464 acc=0.9753623188405797\n",
      "EPOCH=1288\n",
      "train: loss=0.06586217028888508 acc=0.9681159420289855\n",
      "test: loss=0.06422571680436653 acc=0.9695652173913043\n",
      "EPOCH=1289\n",
      "train: loss=0.054791893378490816 acc=0.9768115942028985\n",
      "test: loss=0.06671134874729523 acc=0.9666666666666667\n",
      "EPOCH=1290\n",
      "train: loss=0.04979561899765945 acc=0.9782608695652174\n",
      "test: loss=0.06372155894235432 acc=0.972463768115942\n",
      "EPOCH=1291\n",
      "train: loss=0.052203932480295305 acc=0.9768115942028985\n",
      "test: loss=0.060209679040885575 acc=0.9797101449275363\n",
      "EPOCH=1292\n",
      "train: loss=0.0347235531640991 acc=0.9884057971014493\n",
      "test: loss=0.06344419884402172 acc=0.972463768115942\n",
      "EPOCH=1293\n",
      "train: loss=0.06247628636677105 acc=0.9710144927536232\n",
      "test: loss=0.0641988215293918 acc=0.9753623188405797\n",
      "EPOCH=1294\n",
      "train: loss=0.0824021944838349 acc=0.9637681159420289\n",
      "test: loss=0.0606745657300993 acc=0.9739130434782609\n",
      "EPOCH=1295\n",
      "train: loss=0.05385141783115671 acc=0.9797101449275363\n",
      "test: loss=0.06359922403725951 acc=0.9695652173913043\n",
      "EPOCH=1296\n",
      "train: loss=0.056332229954460716 acc=0.9681159420289855\n",
      "test: loss=0.06194400289475172 acc=0.972463768115942\n",
      "EPOCH=1297\n",
      "train: loss=0.056529915454659706 acc=0.9753623188405797\n",
      "test: loss=0.06047344125016612 acc=0.9710144927536232\n",
      "EPOCH=1298\n",
      "train: loss=0.04933886962249239 acc=0.9797101449275363\n",
      "test: loss=0.06165765246470007 acc=0.972463768115942\n",
      "EPOCH=1299\n",
      "train: loss=0.05817634765273508 acc=0.9753623188405797\n",
      "test: loss=0.06306608734278053 acc=0.9768115942028985\n",
      "EPOCH=1300\n",
      "train: loss=0.05337553102113032 acc=0.972463768115942\n",
      "test: loss=0.061919439286469494 acc=0.9637681159420289\n",
      "EPOCH=1301\n",
      "train: loss=0.0639873845784063 acc=0.9652173913043478\n",
      "test: loss=0.06227095124414659 acc=0.9710144927536232\n",
      "EPOCH=1302\n",
      "train: loss=0.06740750998753593 acc=0.9710144927536232\n",
      "test: loss=0.06214172203564651 acc=0.9710144927536232\n",
      "EPOCH=1303\n",
      "train: loss=0.04380212594322499 acc=0.9826086956521739\n",
      "test: loss=0.059244753237860506 acc=0.9739130434782609\n",
      "EPOCH=1304\n",
      "train: loss=0.05565132323660003 acc=0.9739130434782609\n",
      "test: loss=0.06507429022326233 acc=0.972463768115942\n",
      "EPOCH=1305\n",
      "train: loss=0.0890990157641333 acc=0.9565217391304348\n",
      "test: loss=0.06206286564266608 acc=0.972463768115942\n",
      "EPOCH=1306\n",
      "train: loss=0.03875017673922051 acc=0.981159420289855\n",
      "test: loss=0.06315270198526153 acc=0.972463768115942\n",
      "EPOCH=1307\n",
      "train: loss=0.042480138632169644 acc=0.9782608695652174\n",
      "test: loss=0.06260674324248715 acc=0.9739130434782609\n",
      "EPOCH=1308\n",
      "train: loss=0.04442089088326865 acc=0.9768115942028985\n",
      "test: loss=0.0634216668632663 acc=0.9768115942028985\n",
      "EPOCH=1309\n",
      "train: loss=0.043461571988619274 acc=0.9826086956521739\n",
      "test: loss=0.061627656942447796 acc=0.9753623188405797\n",
      "EPOCH=1310\n",
      "train: loss=0.035062620117360985 acc=0.9884057971014493\n",
      "test: loss=0.0627327866399447 acc=0.972463768115942\n",
      "EPOCH=1311\n",
      "train: loss=0.06885054920278558 acc=0.9753623188405797\n",
      "test: loss=0.06331192703051393 acc=0.9710144927536232\n",
      "EPOCH=1312\n",
      "train: loss=0.032784490302093676 acc=0.9855072463768116\n",
      "test: loss=0.06275557430341484 acc=0.9710144927536232\n",
      "EPOCH=1313\n",
      "train: loss=0.06586599156845736 acc=0.9681159420289855\n",
      "test: loss=0.06507855517188879 acc=0.9681159420289855\n",
      "EPOCH=1314\n",
      "train: loss=0.06801353149493301 acc=0.9681159420289855\n",
      "test: loss=0.05979948572769799 acc=0.972463768115942\n",
      "EPOCH=1315\n",
      "train: loss=0.02982016591667879 acc=0.9840579710144928\n",
      "test: loss=0.0636527797117311 acc=0.9695652173913043\n",
      "EPOCH=1316\n",
      "train: loss=0.060002735668039646 acc=0.972463768115942\n",
      "test: loss=0.06452326225242852 acc=0.9681159420289855\n",
      "EPOCH=1317\n",
      "train: loss=0.05705423800446269 acc=0.9710144927536232\n",
      "test: loss=0.06279060560646357 acc=0.9695652173913043\n",
      "EPOCH=1318\n",
      "train: loss=0.05611453385005037 acc=0.9768115942028985\n",
      "test: loss=0.06359752219923864 acc=0.9681159420289855\n",
      "EPOCH=1319\n",
      "train: loss=0.04978498122272915 acc=0.981159420289855\n",
      "test: loss=0.061230920069933116 acc=0.9739130434782609\n",
      "EPOCH=1320\n",
      "train: loss=0.04064804960646443 acc=0.9826086956521739\n",
      "test: loss=0.0656040530863592 acc=0.9681159420289855\n",
      "EPOCH=1321\n",
      "train: loss=0.07087394190616261 acc=0.9681159420289855\n",
      "test: loss=0.06096129269080343 acc=0.9753623188405797\n",
      "EPOCH=1322\n",
      "train: loss=0.050998067193770674 acc=0.9782608695652174\n",
      "test: loss=0.061693589723256455 acc=0.9710144927536232\n",
      "EPOCH=1323\n",
      "train: loss=0.054263015116758885 acc=0.9753623188405797\n",
      "test: loss=0.06464460938029955 acc=0.9710144927536232\n",
      "EPOCH=1324\n",
      "train: loss=0.0568362751160575 acc=0.9623188405797102\n",
      "test: loss=0.06276804405931276 acc=0.9695652173913043\n",
      "EPOCH=1325\n",
      "train: loss=0.04289973034801012 acc=0.9869565217391304\n",
      "test: loss=0.06143402098912161 acc=0.9753623188405797\n",
      "EPOCH=1326\n",
      "train: loss=0.06440814903489475 acc=0.9739130434782609\n",
      "test: loss=0.06495179988841836 acc=0.972463768115942\n",
      "EPOCH=1327\n",
      "train: loss=0.04966392805115061 acc=0.9753623188405797\n",
      "test: loss=0.06418787681732706 acc=0.9666666666666667\n",
      "EPOCH=1328\n",
      "train: loss=0.0330848805315953 acc=0.9840579710144928\n",
      "test: loss=0.05945632730617826 acc=0.9768115942028985\n",
      "EPOCH=1329\n",
      "train: loss=0.05540000808017322 acc=0.9681159420289855\n",
      "test: loss=0.06277979651858148 acc=0.972463768115942\n",
      "EPOCH=1330\n",
      "train: loss=0.05122539494846132 acc=0.9768115942028985\n",
      "test: loss=0.061959299282554794 acc=0.972463768115942\n",
      "EPOCH=1331\n",
      "train: loss=0.0419982226381818 acc=0.981159420289855\n",
      "test: loss=0.06299846730382891 acc=0.972463768115942\n",
      "EPOCH=1332\n",
      "train: loss=0.03680930766402779 acc=0.9797101449275363\n",
      "test: loss=0.05904229774862983 acc=0.972463768115942\n",
      "EPOCH=1333\n",
      "train: loss=0.0709320052259171 acc=0.9652173913043478\n",
      "test: loss=0.06096184602939958 acc=0.972463768115942\n",
      "EPOCH=1334\n",
      "train: loss=0.027079365191364543 acc=0.991304347826087\n",
      "test: loss=0.060510454908512736 acc=0.9768115942028985\n",
      "EPOCH=1335\n",
      "train: loss=0.06668982484937457 acc=0.9695652173913043\n",
      "test: loss=0.06089713738164436 acc=0.972463768115942\n",
      "EPOCH=1336\n",
      "train: loss=0.061567550878107315 acc=0.9695652173913043\n",
      "test: loss=0.05995558704627787 acc=0.972463768115942\n",
      "EPOCH=1337\n",
      "train: loss=0.046072920563748725 acc=0.9782608695652174\n",
      "test: loss=0.06282064845893315 acc=0.972463768115942\n",
      "EPOCH=1338\n",
      "train: loss=0.06126628025024162 acc=0.9695652173913043\n",
      "test: loss=0.0633909072161807 acc=0.9666666666666667\n",
      "EPOCH=1339\n",
      "train: loss=0.06633064558278938 acc=0.9753623188405797\n",
      "test: loss=0.062105962340523645 acc=0.9710144927536232\n",
      "EPOCH=1340\n",
      "train: loss=0.033109250659336086 acc=0.9855072463768116\n",
      "test: loss=0.06190379478911652 acc=0.9753623188405797\n",
      "EPOCH=1341\n",
      "train: loss=0.05930688266586143 acc=0.9695652173913043\n",
      "test: loss=0.06244236593329017 acc=0.9666666666666667\n",
      "EPOCH=1342\n",
      "train: loss=0.047505482120484686 acc=0.9840579710144928\n",
      "test: loss=0.0596866512801749 acc=0.9768115942028985\n",
      "EPOCH=1343\n",
      "train: loss=0.04467205735272739 acc=0.981159420289855\n",
      "test: loss=0.06075252311203117 acc=0.9768115942028985\n",
      "EPOCH=1344\n",
      "train: loss=0.056390602302012566 acc=0.9666666666666667\n",
      "test: loss=0.06177306251491151 acc=0.9739130434782609\n",
      "EPOCH=1345\n",
      "train: loss=0.047216192167284496 acc=0.9869565217391304\n",
      "test: loss=0.06277549325883805 acc=0.9695652173913043\n",
      "EPOCH=1346\n",
      "train: loss=0.06462285671569715 acc=0.9739130434782609\n",
      "test: loss=0.06220659006014234 acc=0.972463768115942\n",
      "EPOCH=1347\n",
      "train: loss=0.04191269934814889 acc=0.9782608695652174\n",
      "test: loss=0.06486860117601952 acc=0.9710144927536232\n",
      "EPOCH=1348\n",
      "train: loss=0.04922948383187505 acc=0.9768115942028985\n",
      "test: loss=0.06114491832790517 acc=0.9782608695652174\n",
      "EPOCH=1349\n",
      "train: loss=0.06086110617048395 acc=0.9695652173913043\n",
      "test: loss=0.059624414297585755 acc=0.9753623188405797\n",
      "EPOCH=1350\n",
      "train: loss=0.047736943071952254 acc=0.9782608695652174\n",
      "test: loss=0.06103139944314757 acc=0.9739130434782609\n",
      "EPOCH=1351\n",
      "train: loss=0.05526866345218667 acc=0.972463768115942\n",
      "test: loss=0.0656271196216167 acc=0.9710144927536232\n",
      "EPOCH=1352\n",
      "train: loss=0.03718115724892182 acc=0.9869565217391304\n",
      "test: loss=0.06202503459371447 acc=0.9652173913043478\n",
      "EPOCH=1353\n",
      "train: loss=0.04895625517849163 acc=0.9826086956521739\n",
      "test: loss=0.06235545938105635 acc=0.972463768115942\n",
      "EPOCH=1354\n",
      "train: loss=0.07761287555518435 acc=0.9637681159420289\n",
      "test: loss=0.0633850065095614 acc=0.9695652173913043\n",
      "EPOCH=1355\n",
      "train: loss=0.07200835101565764 acc=0.9695652173913043\n",
      "test: loss=0.06087715713810597 acc=0.9753623188405797\n",
      "EPOCH=1356\n",
      "train: loss=0.04511541119724289 acc=0.9753623188405797\n",
      "test: loss=0.06022874042320057 acc=0.9710144927536232\n",
      "EPOCH=1357\n",
      "train: loss=0.046999312301917465 acc=0.9753623188405797\n",
      "test: loss=0.059196705071696026 acc=0.9739130434782609\n",
      "EPOCH=1358\n",
      "train: loss=0.04185912168367365 acc=0.981159420289855\n",
      "test: loss=0.05876426848972214 acc=0.9753623188405797\n",
      "EPOCH=1359\n",
      "train: loss=0.04247478243396056 acc=0.9768115942028985\n",
      "test: loss=0.05760636201956974 acc=0.9753623188405797\n",
      "EPOCH=1360\n",
      "train: loss=0.06373300585490421 acc=0.9695652173913043\n",
      "test: loss=0.06265545922483169 acc=0.972463768115942\n",
      "EPOCH=1361\n",
      "train: loss=0.06690842754371246 acc=0.9710144927536232\n",
      "test: loss=0.06413793446703235 acc=0.9695652173913043\n",
      "EPOCH=1362\n",
      "train: loss=0.034307519642756606 acc=0.9884057971014493\n",
      "test: loss=0.05772862516207775 acc=0.9753623188405797\n",
      "EPOCH=1363\n",
      "train: loss=0.04923578459346075 acc=0.9753623188405797\n",
      "test: loss=0.06093058803760486 acc=0.9739130434782609\n",
      "EPOCH=1364\n",
      "train: loss=0.033176649532391016 acc=0.9826086956521739\n",
      "test: loss=0.05837636361332822 acc=0.9739130434782609\n",
      "EPOCH=1365\n",
      "train: loss=0.06197722149385863 acc=0.9695652173913043\n",
      "test: loss=0.06174393058022448 acc=0.9666666666666667\n",
      "EPOCH=1366\n",
      "train: loss=0.05071002323453663 acc=0.9768115942028985\n",
      "test: loss=0.05848696857472951 acc=0.9739130434782609\n",
      "EPOCH=1367\n",
      "train: loss=0.029527579790168894 acc=0.9898550724637681\n",
      "test: loss=0.06403671066029559 acc=0.9681159420289855\n",
      "EPOCH=1368\n",
      "train: loss=0.06942225122093089 acc=0.9681159420289855\n",
      "test: loss=0.060110213442226765 acc=0.972463768115942\n",
      "EPOCH=1369\n",
      "train: loss=0.061417402422099907 acc=0.9797101449275363\n",
      "test: loss=0.05887868958640682 acc=0.9753623188405797\n",
      "EPOCH=1370\n",
      "train: loss=0.054949127560765886 acc=0.9710144927536232\n",
      "test: loss=0.06006511444166749 acc=0.9710144927536232\n",
      "EPOCH=1371\n",
      "train: loss=0.04288012809775408 acc=0.9768115942028985\n",
      "test: loss=0.06021543102353753 acc=0.9768115942028985\n",
      "EPOCH=1372\n",
      "train: loss=0.06281539780083496 acc=0.9623188405797102\n",
      "test: loss=0.05818976831332036 acc=0.9739130434782609\n",
      "EPOCH=1373\n",
      "train: loss=0.05166850916893771 acc=0.972463768115942\n",
      "test: loss=0.061613760796847744 acc=0.9710144927536232\n",
      "EPOCH=1374\n",
      "train: loss=0.061834024301475735 acc=0.972463768115942\n",
      "test: loss=0.06372046093524265 acc=0.972463768115942\n",
      "EPOCH=1375\n",
      "train: loss=0.07144616198478276 acc=0.9623188405797102\n",
      "test: loss=0.05983621763561746 acc=0.9710144927536232\n",
      "EPOCH=1376\n",
      "train: loss=0.03983660654002704 acc=0.9840579710144928\n",
      "test: loss=0.05983542894088897 acc=0.9739130434782609\n",
      "EPOCH=1377\n",
      "train: loss=0.04783120005776596 acc=0.9782608695652174\n",
      "test: loss=0.05907975783326884 acc=0.9710144927536232\n",
      "EPOCH=1378\n",
      "train: loss=0.04956567013825698 acc=0.972463768115942\n",
      "test: loss=0.05986749190871114 acc=0.9753623188405797\n",
      "EPOCH=1379\n",
      "train: loss=0.0731786520175057 acc=0.9623188405797102\n",
      "test: loss=0.06139995956274566 acc=0.9710144927536232\n",
      "EPOCH=1380\n",
      "train: loss=0.05030771694124901 acc=0.9753623188405797\n",
      "test: loss=0.06038886770938092 acc=0.9681159420289855\n",
      "EPOCH=1381\n",
      "train: loss=0.048488762129419244 acc=0.9782608695652174\n",
      "test: loss=0.06147818575977237 acc=0.9681159420289855\n",
      "EPOCH=1382\n",
      "train: loss=0.05682277740828501 acc=0.9753623188405797\n",
      "test: loss=0.060508140756653235 acc=0.9753623188405797\n",
      "EPOCH=1383\n",
      "train: loss=0.06759446606277196 acc=0.9666666666666667\n",
      "test: loss=0.05805724936413249 acc=0.9753623188405797\n",
      "EPOCH=1384\n",
      "train: loss=0.04059802125417597 acc=0.9826086956521739\n",
      "test: loss=0.05849231019212359 acc=0.972463768115942\n",
      "EPOCH=1385\n",
      "train: loss=0.041717410486125546 acc=0.981159420289855\n",
      "test: loss=0.05979097701228049 acc=0.9710144927536232\n",
      "EPOCH=1386\n",
      "train: loss=0.04296411343814278 acc=0.9782608695652174\n",
      "test: loss=0.05924578956723145 acc=0.9753623188405797\n",
      "EPOCH=1387\n",
      "train: loss=0.06693907518167821 acc=0.9739130434782609\n",
      "test: loss=0.05847789086425302 acc=0.9681159420289855\n",
      "EPOCH=1388\n",
      "train: loss=0.03912934144304758 acc=0.9898550724637681\n",
      "test: loss=0.06074299709887641 acc=0.9768115942028985\n",
      "EPOCH=1389\n",
      "train: loss=0.03605008159703285 acc=0.981159420289855\n",
      "test: loss=0.05907286915809925 acc=0.9753623188405797\n",
      "EPOCH=1390\n",
      "train: loss=0.043950379627781484 acc=0.9782608695652174\n",
      "test: loss=0.061614135916617296 acc=0.972463768115942\n",
      "EPOCH=1391\n",
      "train: loss=0.029422089326497376 acc=0.9869565217391304\n",
      "test: loss=0.0630986041884893 acc=0.9768115942028985\n",
      "EPOCH=1392\n",
      "train: loss=0.05219968907769431 acc=0.9826086956521739\n",
      "test: loss=0.0662849144967875 acc=0.9681159420289855\n",
      "EPOCH=1393\n",
      "train: loss=0.070112944913731 acc=0.9710144927536232\n",
      "test: loss=0.06035574895026913 acc=0.9710144927536232\n",
      "EPOCH=1394\n",
      "train: loss=0.08327798712248836 acc=0.9623188405797102\n",
      "test: loss=0.05883286025179306 acc=0.9695652173913043\n",
      "EPOCH=1395\n",
      "train: loss=0.034598214875750495 acc=0.9797101449275363\n",
      "test: loss=0.057079399986646955 acc=0.9739130434782609\n",
      "EPOCH=1396\n",
      "train: loss=0.046707295847894556 acc=0.9782608695652174\n",
      "test: loss=0.0597742208267787 acc=0.9753623188405797\n",
      "EPOCH=1397\n",
      "train: loss=0.03403636133879107 acc=0.9826086956521739\n",
      "test: loss=0.056536317720711084 acc=0.9768115942028985\n",
      "EPOCH=1398\n",
      "train: loss=0.043475936576970776 acc=0.981159420289855\n",
      "test: loss=0.05863701357533331 acc=0.9739130434782609\n",
      "EPOCH=1399\n",
      "train: loss=0.07795215136050733 acc=0.9637681159420289\n",
      "test: loss=0.0614354133206345 acc=0.9739130434782609\n",
      "EPOCH=1400\n",
      "train: loss=0.04368541902140438 acc=0.9797101449275363\n",
      "test: loss=0.05963925871341561 acc=0.9710144927536232\n",
      "EPOCH=1401\n",
      "train: loss=0.05100847050054246 acc=0.9695652173913043\n",
      "test: loss=0.059465110026848525 acc=0.972463768115942\n",
      "EPOCH=1402\n",
      "train: loss=0.038110690412008714 acc=0.9797101449275363\n",
      "test: loss=0.05939905618517225 acc=0.9768115942028985\n",
      "EPOCH=1403\n",
      "train: loss=0.04106784192514023 acc=0.9826086956521739\n",
      "test: loss=0.05889705063890365 acc=0.972463768115942\n",
      "EPOCH=1404\n",
      "train: loss=0.04896993680667077 acc=0.972463768115942\n",
      "test: loss=0.05985686481977437 acc=0.9710144927536232\n",
      "EPOCH=1405\n",
      "train: loss=0.043530696886926734 acc=0.9826086956521739\n",
      "test: loss=0.05688240939185328 acc=0.972463768115942\n",
      "EPOCH=1406\n",
      "train: loss=0.03679909252852994 acc=0.9797101449275363\n",
      "test: loss=0.05633212969874428 acc=0.972463768115942\n",
      "EPOCH=1407\n",
      "train: loss=0.04065036658719304 acc=0.981159420289855\n",
      "test: loss=0.06363388579612828 acc=0.9695652173913043\n",
      "EPOCH=1408\n",
      "train: loss=0.042893995090496656 acc=0.9826086956521739\n",
      "test: loss=0.05705152132283675 acc=0.972463768115942\n",
      "EPOCH=1409\n",
      "train: loss=0.0381670521798698 acc=0.9855072463768116\n",
      "test: loss=0.05822673669456788 acc=0.9710144927536232\n",
      "EPOCH=1410\n",
      "train: loss=0.04977195768066856 acc=0.9739130434782609\n",
      "test: loss=0.06091366638554276 acc=0.9710144927536232\n",
      "EPOCH=1411\n",
      "train: loss=0.04732326871405798 acc=0.9782608695652174\n",
      "test: loss=0.05947435050864722 acc=0.9782608695652174\n",
      "EPOCH=1412\n",
      "train: loss=0.05760131101002976 acc=0.9739130434782609\n",
      "test: loss=0.06002028341473854 acc=0.9739130434782609\n",
      "EPOCH=1413\n",
      "train: loss=0.053247523902246595 acc=0.9710144927536232\n",
      "test: loss=0.05874868238307442 acc=0.9710144927536232\n",
      "EPOCH=1414\n",
      "train: loss=0.044438853309877 acc=0.9797101449275363\n",
      "test: loss=0.05822073969512349 acc=0.9695652173913043\n",
      "EPOCH=1415\n",
      "train: loss=0.04375416700961863 acc=0.9797101449275363\n",
      "test: loss=0.059711906258057064 acc=0.9695652173913043\n",
      "EPOCH=1416\n",
      "train: loss=0.047035170954292965 acc=0.9782608695652174\n",
      "test: loss=0.059197755063980104 acc=0.9753623188405797\n",
      "EPOCH=1417\n",
      "train: loss=0.059069915090947056 acc=0.9710144927536232\n",
      "test: loss=0.0613643206451359 acc=0.972463768115942\n",
      "EPOCH=1418\n",
      "train: loss=0.053099975036865796 acc=0.9753623188405797\n",
      "test: loss=0.06080897181443018 acc=0.9753623188405797\n",
      "EPOCH=1419\n",
      "train: loss=0.04647163510045418 acc=0.9797101449275363\n",
      "test: loss=0.060699518682137736 acc=0.972463768115942\n",
      "EPOCH=1420\n",
      "train: loss=0.02845665872288425 acc=0.9855072463768116\n",
      "test: loss=0.06276194703360483 acc=0.972463768115942\n",
      "EPOCH=1421\n",
      "train: loss=0.032865893969282085 acc=0.9826086956521739\n",
      "test: loss=0.06022550701889093 acc=0.9753623188405797\n",
      "EPOCH=1422\n",
      "train: loss=0.021717233404402278 acc=0.9869565217391304\n",
      "test: loss=0.057478073804232505 acc=0.972463768115942\n",
      "EPOCH=1423\n",
      "train: loss=0.04865198535762192 acc=0.9797101449275363\n",
      "test: loss=0.05780910550470769 acc=0.9768115942028985\n",
      "EPOCH=1424\n",
      "train: loss=0.0350412573867174 acc=0.9826086956521739\n",
      "test: loss=0.057609664353487265 acc=0.9768115942028985\n",
      "EPOCH=1425\n",
      "train: loss=0.03470820390225784 acc=0.9826086956521739\n",
      "test: loss=0.059153938796390096 acc=0.9782608695652174\n",
      "EPOCH=1426\n",
      "train: loss=0.04194371762820296 acc=0.9840579710144928\n",
      "test: loss=0.05923599274108641 acc=0.9739130434782609\n",
      "EPOCH=1427\n",
      "train: loss=0.040737745179831315 acc=0.981159420289855\n",
      "test: loss=0.05672397097958886 acc=0.9768115942028985\n",
      "EPOCH=1428\n",
      "train: loss=0.04431561364942491 acc=0.981159420289855\n",
      "test: loss=0.060625202942103634 acc=0.9695652173913043\n",
      "EPOCH=1429\n",
      "train: loss=0.06111833319451604 acc=0.9666666666666667\n",
      "test: loss=0.060726524320135825 acc=0.9695652173913043\n",
      "EPOCH=1430\n",
      "train: loss=0.02556438846249504 acc=0.9884057971014493\n",
      "test: loss=0.0604240141112978 acc=0.9753623188405797\n",
      "EPOCH=1431\n",
      "train: loss=0.04890135837620153 acc=0.972463768115942\n",
      "test: loss=0.058434677960249835 acc=0.972463768115942\n",
      "EPOCH=1432\n",
      "train: loss=0.06334224997348731 acc=0.9681159420289855\n",
      "test: loss=0.057802503810009884 acc=0.9768115942028985\n",
      "EPOCH=1433\n",
      "train: loss=0.03893633836333057 acc=0.9782608695652174\n",
      "test: loss=0.05936964718565822 acc=0.9710144927536232\n",
      "EPOCH=1434\n",
      "train: loss=0.044610863174551374 acc=0.9797101449275363\n",
      "test: loss=0.05875000060650876 acc=0.9681159420289855\n",
      "EPOCH=1435\n",
      "train: loss=0.05327813081094569 acc=0.972463768115942\n",
      "test: loss=0.06003953658731939 acc=0.972463768115942\n",
      "EPOCH=1436\n",
      "train: loss=0.048674154184818755 acc=0.9753623188405797\n",
      "test: loss=0.058747633619405826 acc=0.9768115942028985\n",
      "EPOCH=1437\n",
      "train: loss=0.06193557566560743 acc=0.9666666666666667\n",
      "test: loss=0.057465991755606474 acc=0.9710144927536232\n",
      "EPOCH=1438\n",
      "train: loss=0.039257909822949744 acc=0.9782608695652174\n",
      "test: loss=0.05654670628331045 acc=0.9797101449275363\n",
      "EPOCH=1439\n",
      "train: loss=0.039969957895846876 acc=0.981159420289855\n",
      "test: loss=0.05863988505150524 acc=0.972463768115942\n",
      "EPOCH=1440\n",
      "train: loss=0.05842883461109985 acc=0.9710144927536232\n",
      "test: loss=0.057790624729054614 acc=0.9695652173913043\n",
      "EPOCH=1441\n",
      "train: loss=0.0548985556177554 acc=0.972463768115942\n",
      "test: loss=0.05727304624825931 acc=0.9739130434782609\n",
      "EPOCH=1442\n",
      "train: loss=0.05055970905458636 acc=0.9768115942028985\n",
      "test: loss=0.05612880985010495 acc=0.9768115942028985\n",
      "EPOCH=1443\n",
      "train: loss=0.02718622992688353 acc=0.9869565217391304\n",
      "test: loss=0.059463126438623434 acc=0.972463768115942\n",
      "EPOCH=1444\n",
      "train: loss=0.046765317684195076 acc=0.981159420289855\n",
      "test: loss=0.05593409707218991 acc=0.9768115942028985\n",
      "EPOCH=1445\n",
      "train: loss=0.04938176307229086 acc=0.972463768115942\n",
      "test: loss=0.05842668937731531 acc=0.9768115942028985\n",
      "EPOCH=1446\n",
      "train: loss=0.05680331020438087 acc=0.9681159420289855\n",
      "test: loss=0.054908385163927986 acc=0.9753623188405797\n",
      "EPOCH=1447\n",
      "train: loss=0.04968356599771429 acc=0.9768115942028985\n",
      "test: loss=0.05798997616606376 acc=0.9782608695652174\n",
      "EPOCH=1448\n",
      "train: loss=0.03150757525347162 acc=0.9869565217391304\n",
      "test: loss=0.06012078116470146 acc=0.9782608695652174\n",
      "EPOCH=1449\n",
      "train: loss=0.04166671476134549 acc=0.981159420289855\n",
      "test: loss=0.05650934831154172 acc=0.972463768115942\n",
      "EPOCH=1450\n",
      "train: loss=0.05820030795089088 acc=0.972463768115942\n",
      "test: loss=0.05671127135697886 acc=0.972463768115942\n",
      "EPOCH=1451\n",
      "train: loss=0.037724840090842454 acc=0.9840579710144928\n",
      "test: loss=0.0561999216496439 acc=0.9739130434782609\n",
      "EPOCH=1452\n",
      "train: loss=0.06425480760441792 acc=0.9695652173913043\n",
      "test: loss=0.05763617687783142 acc=0.9768115942028985\n",
      "EPOCH=1453\n",
      "train: loss=0.058145328525294936 acc=0.9681159420289855\n",
      "test: loss=0.05714442972885098 acc=0.9753623188405797\n",
      "EPOCH=1454\n",
      "train: loss=0.04199449787292408 acc=0.981159420289855\n",
      "test: loss=0.059452085965670275 acc=0.972463768115942\n",
      "EPOCH=1455\n",
      "train: loss=0.07103283202297894 acc=0.9637681159420289\n",
      "test: loss=0.05805922809828326 acc=0.972463768115942\n",
      "EPOCH=1456\n",
      "train: loss=0.04506020326455572 acc=0.981159420289855\n",
      "test: loss=0.05612525255378548 acc=0.972463768115942\n",
      "EPOCH=1457\n",
      "train: loss=0.02669077308710363 acc=0.991304347826087\n",
      "test: loss=0.05473532494837743 acc=0.9797101449275363\n",
      "EPOCH=1458\n",
      "train: loss=0.061162023313419314 acc=0.9710144927536232\n",
      "test: loss=0.05778878709394302 acc=0.9695652173913043\n",
      "EPOCH=1459\n",
      "train: loss=0.04960049764209392 acc=0.9782608695652174\n",
      "test: loss=0.05773997130781282 acc=0.9710144927536232\n",
      "EPOCH=1460\n",
      "train: loss=0.04685489926748123 acc=0.9710144927536232\n",
      "test: loss=0.0555918360131134 acc=0.9768115942028985\n",
      "EPOCH=1461\n",
      "train: loss=0.044569298405505955 acc=0.9826086956521739\n",
      "test: loss=0.055950688943010474 acc=0.9768115942028985\n",
      "EPOCH=1462\n",
      "train: loss=0.056847489705300555 acc=0.9695652173913043\n",
      "test: loss=0.06059674512216231 acc=0.972463768115942\n",
      "EPOCH=1463\n",
      "train: loss=0.07605312470285962 acc=0.9623188405797102\n",
      "test: loss=0.05732976740889564 acc=0.972463768115942\n",
      "EPOCH=1464\n",
      "train: loss=0.01993914997691268 acc=0.9898550724637681\n",
      "test: loss=0.057927549484994775 acc=0.9710144927536232\n",
      "EPOCH=1465\n",
      "train: loss=0.042149085262139147 acc=0.9782608695652174\n",
      "test: loss=0.055705782715974005 acc=0.9768115942028985\n",
      "EPOCH=1466\n",
      "train: loss=0.05151111909183085 acc=0.972463768115942\n",
      "test: loss=0.05472404010230887 acc=0.9753623188405797\n",
      "EPOCH=1467\n",
      "train: loss=0.05703180256767913 acc=0.9739130434782609\n",
      "test: loss=0.0558835016333324 acc=0.9753623188405797\n",
      "EPOCH=1468\n",
      "train: loss=0.03749254916478742 acc=0.9855072463768116\n",
      "test: loss=0.05615641536905243 acc=0.972463768115942\n",
      "EPOCH=1469\n",
      "train: loss=0.055893370636194456 acc=0.9710144927536232\n",
      "test: loss=0.05673047832258252 acc=0.9768115942028985\n",
      "EPOCH=1470\n",
      "train: loss=0.044072920017440655 acc=0.9797101449275363\n",
      "test: loss=0.057564934380812414 acc=0.9695652173913043\n",
      "EPOCH=1471\n",
      "train: loss=0.04854571857738443 acc=0.9768115942028985\n",
      "test: loss=0.056502861834531376 acc=0.9753623188405797\n",
      "EPOCH=1472\n",
      "train: loss=0.06596755967865897 acc=0.9681159420289855\n",
      "test: loss=0.0567872498877955 acc=0.9753623188405797\n",
      "EPOCH=1473\n",
      "train: loss=0.05894446553153862 acc=0.9681159420289855\n",
      "test: loss=0.05784237972176896 acc=0.9753623188405797\n",
      "EPOCH=1474\n",
      "train: loss=0.04430460952530303 acc=0.9768115942028985\n",
      "test: loss=0.05900014098902785 acc=0.9782608695652174\n",
      "EPOCH=1475\n",
      "train: loss=0.042363718500496764 acc=0.9768115942028985\n",
      "test: loss=0.05462148908870188 acc=0.9768115942028985\n",
      "EPOCH=1476\n",
      "train: loss=0.05183649305416627 acc=0.972463768115942\n",
      "test: loss=0.056361786453303905 acc=0.972463768115942\n",
      "EPOCH=1477\n",
      "train: loss=0.04698585560014181 acc=0.9768115942028985\n",
      "test: loss=0.056622849241017195 acc=0.9768115942028985\n",
      "EPOCH=1478\n",
      "train: loss=0.04625067312718699 acc=0.9797101449275363\n",
      "test: loss=0.058079228924712525 acc=0.9753623188405797\n",
      "EPOCH=1479\n",
      "train: loss=0.054112551698282094 acc=0.9739130434782609\n",
      "test: loss=0.05719236988548778 acc=0.9782608695652174\n",
      "EPOCH=1480\n",
      "train: loss=0.06287180201341667 acc=0.9710144927536232\n",
      "test: loss=0.05700806165634548 acc=0.972463768115942\n",
      "EPOCH=1481\n",
      "train: loss=0.042867119296813735 acc=0.9782608695652174\n",
      "test: loss=0.05743657094570088 acc=0.9753623188405797\n",
      "EPOCH=1482\n",
      "train: loss=0.05726406129239044 acc=0.9710144927536232\n",
      "test: loss=0.05771297756299295 acc=0.9782608695652174\n",
      "EPOCH=1483\n",
      "train: loss=0.04720629401000209 acc=0.9753623188405797\n",
      "test: loss=0.05744663600951055 acc=0.9739130434782609\n",
      "EPOCH=1484\n",
      "train: loss=0.0808680756597754 acc=0.9565217391304348\n",
      "test: loss=0.05619891143377837 acc=0.972463768115942\n",
      "EPOCH=1485\n",
      "train: loss=0.03777323146256703 acc=0.9797101449275363\n",
      "test: loss=0.056486290907208636 acc=0.972463768115942\n",
      "EPOCH=1486\n",
      "train: loss=0.06187881717362876 acc=0.9695652173913043\n",
      "test: loss=0.0582856137249123 acc=0.9739130434782609\n",
      "EPOCH=1487\n",
      "train: loss=0.027372788578304764 acc=0.9797101449275363\n",
      "test: loss=0.053505703713017626 acc=0.9768115942028985\n",
      "EPOCH=1488\n",
      "train: loss=0.059144470782304885 acc=0.9710144927536232\n",
      "test: loss=0.056157051334246914 acc=0.972463768115942\n",
      "EPOCH=1489\n",
      "train: loss=0.07399740992347979 acc=0.9608695652173913\n",
      "test: loss=0.055749276241774906 acc=0.972463768115942\n",
      "EPOCH=1490\n",
      "train: loss=0.04632907156300688 acc=0.9782608695652174\n",
      "test: loss=0.05697445712410407 acc=0.9710144927536232\n",
      "EPOCH=1491\n",
      "train: loss=0.07454007913142797 acc=0.9623188405797102\n",
      "test: loss=0.057773570106152826 acc=0.9710144927536232\n",
      "EPOCH=1492\n",
      "train: loss=0.02295517337114322 acc=0.9898550724637681\n",
      "test: loss=0.057541190055873284 acc=0.9681159420289855\n",
      "EPOCH=1493\n",
      "train: loss=0.051237079427641226 acc=0.9710144927536232\n",
      "test: loss=0.05609490753090156 acc=0.972463768115942\n",
      "EPOCH=1494\n",
      "train: loss=0.03393446132001602 acc=0.9826086956521739\n",
      "test: loss=0.05637030876157824 acc=0.9753623188405797\n",
      "EPOCH=1495\n",
      "train: loss=0.04631586681979621 acc=0.9739130434782609\n",
      "test: loss=0.05728883399564387 acc=0.9768115942028985\n",
      "EPOCH=1496\n",
      "train: loss=0.05272219690316343 acc=0.9739130434782609\n",
      "test: loss=0.05692798293528916 acc=0.9739130434782609\n",
      "EPOCH=1497\n",
      "train: loss=0.07142632684913358 acc=0.9695652173913043\n",
      "test: loss=0.056292714804461824 acc=0.9753623188405797\n",
      "EPOCH=1498\n",
      "train: loss=0.04809002323761138 acc=0.9797101449275363\n",
      "test: loss=0.05666479470143649 acc=0.972463768115942\n",
      "EPOCH=1499\n",
      "train: loss=0.07086739222210273 acc=0.9637681159420289\n",
      "test: loss=0.05538795544123978 acc=0.9739130434782609\n",
      "EPOCH=1500\n",
      "train: loss=0.044642059519271686 acc=0.981159420289855\n",
      "test: loss=0.06112722000997318 acc=0.9739130434782609\n",
      "EPOCH=1501\n",
      "train: loss=0.044071466114670485 acc=0.9782608695652174\n",
      "test: loss=0.054771640091169124 acc=0.9768115942028985\n",
      "EPOCH=1502\n",
      "train: loss=0.03923790714110539 acc=0.981159420289855\n",
      "test: loss=0.060232310472173305 acc=0.9753623188405797\n",
      "EPOCH=1503\n",
      "train: loss=0.06995712914511112 acc=0.9666666666666667\n",
      "test: loss=0.054389386853302045 acc=0.9739130434782609\n",
      "EPOCH=1504\n",
      "train: loss=0.04529000445454482 acc=0.9739130434782609\n",
      "test: loss=0.056940604002498126 acc=0.9739130434782609\n",
      "EPOCH=1505\n",
      "train: loss=0.04146666425089954 acc=0.9797101449275363\n",
      "test: loss=0.05611895775208806 acc=0.9782608695652174\n",
      "EPOCH=1506\n",
      "train: loss=0.04155785933554058 acc=0.9768115942028985\n",
      "test: loss=0.05693877041308008 acc=0.9710144927536232\n",
      "EPOCH=1507\n",
      "train: loss=0.04156331167350287 acc=0.9826086956521739\n",
      "test: loss=0.05538595049427299 acc=0.9710144927536232\n",
      "EPOCH=1508\n",
      "train: loss=0.053485787855765056 acc=0.9782608695652174\n",
      "test: loss=0.05702350073134799 acc=0.9739130434782609\n",
      "EPOCH=1509\n",
      "train: loss=0.044170675198908216 acc=0.9782608695652174\n",
      "test: loss=0.05516406517093312 acc=0.9739130434782609\n",
      "EPOCH=1510\n",
      "train: loss=0.0554114237297756 acc=0.9695652173913043\n",
      "test: loss=0.05843521565812863 acc=0.9739130434782609\n",
      "EPOCH=1511\n",
      "train: loss=0.03447822930124601 acc=0.9840579710144928\n",
      "test: loss=0.05730555361857257 acc=0.9768115942028985\n",
      "EPOCH=1512\n",
      "train: loss=0.04114415029453017 acc=0.9753623188405797\n",
      "test: loss=0.056501305563582474 acc=0.9710144927536232\n",
      "EPOCH=1513\n",
      "train: loss=0.021281389940981305 acc=0.991304347826087\n",
      "test: loss=0.05412453179172271 acc=0.9739130434782609\n",
      "EPOCH=1514\n",
      "train: loss=0.03607343978926633 acc=0.981159420289855\n",
      "test: loss=0.05566075548901943 acc=0.9739130434782609\n",
      "EPOCH=1515\n",
      "train: loss=0.030727435841790794 acc=0.9855072463768116\n",
      "test: loss=0.05707810784095934 acc=0.972463768115942\n",
      "EPOCH=1516\n",
      "train: loss=0.0580436373231863 acc=0.9753623188405797\n",
      "test: loss=0.05594066938891322 acc=0.9739130434782609\n",
      "EPOCH=1517\n",
      "train: loss=0.028854939277533156 acc=0.9826086956521739\n",
      "test: loss=0.05850811672961228 acc=0.9768115942028985\n",
      "EPOCH=1518\n",
      "train: loss=0.05361531959955489 acc=0.9739130434782609\n",
      "test: loss=0.056380876145502704 acc=0.9739130434782609\n",
      "EPOCH=1519\n",
      "train: loss=0.052448889550894835 acc=0.9695652173913043\n",
      "test: loss=0.05502474219979965 acc=0.9739130434782609\n",
      "EPOCH=1520\n",
      "train: loss=0.03444472822079337 acc=0.9753623188405797\n",
      "test: loss=0.053261929073688485 acc=0.9739130434782609\n",
      "EPOCH=1521\n",
      "train: loss=0.036134977645706515 acc=0.9797101449275363\n",
      "test: loss=0.05686125714091249 acc=0.9753623188405797\n",
      "EPOCH=1522\n",
      "train: loss=0.05280159464262647 acc=0.9782608695652174\n",
      "test: loss=0.05330934893884393 acc=0.9768115942028985\n",
      "EPOCH=1523\n",
      "train: loss=0.03261776284208659 acc=0.991304347826087\n",
      "test: loss=0.05360245602582853 acc=0.9753623188405797\n",
      "EPOCH=1524\n",
      "train: loss=0.03200150490337725 acc=0.9826086956521739\n",
      "test: loss=0.05405296209043923 acc=0.9753623188405797\n",
      "EPOCH=1525\n",
      "train: loss=0.04177203299628538 acc=0.981159420289855\n",
      "test: loss=0.054805242923381306 acc=0.972463768115942\n",
      "EPOCH=1526\n",
      "train: loss=0.02734651358762003 acc=0.9855072463768116\n",
      "test: loss=0.05565399845922824 acc=0.9739130434782609\n",
      "EPOCH=1527\n",
      "train: loss=0.06470896003296256 acc=0.9666666666666667\n",
      "test: loss=0.053407343952776076 acc=0.972463768115942\n",
      "EPOCH=1528\n",
      "train: loss=0.04542294478439867 acc=0.972463768115942\n",
      "test: loss=0.057792676923173046 acc=0.9768115942028985\n",
      "EPOCH=1529\n",
      "train: loss=0.04843757353801196 acc=0.9768115942028985\n",
      "test: loss=0.055195355768162785 acc=0.9782608695652174\n",
      "EPOCH=1530\n",
      "train: loss=0.055029818278378564 acc=0.9695652173913043\n",
      "test: loss=0.05238361432722768 acc=0.9739130434782609\n",
      "EPOCH=1531\n",
      "train: loss=0.0400240894091298 acc=0.9797101449275363\n",
      "test: loss=0.054109697722147874 acc=0.972463768115942\n",
      "EPOCH=1532\n",
      "train: loss=0.041324961315541454 acc=0.9753623188405797\n",
      "test: loss=0.05233417967962716 acc=0.9710144927536232\n",
      "EPOCH=1533\n",
      "train: loss=0.05338782342508276 acc=0.9710144927536232\n",
      "test: loss=0.05531620063090417 acc=0.9710144927536232\n",
      "EPOCH=1534\n",
      "train: loss=0.05045813528687046 acc=0.9768115942028985\n",
      "test: loss=0.05749645585798742 acc=0.9710144927536232\n",
      "EPOCH=1535\n",
      "train: loss=0.026599586192246907 acc=0.9869565217391304\n",
      "test: loss=0.055201303658171136 acc=0.9710144927536232\n",
      "EPOCH=1536\n",
      "train: loss=0.05884596176003994 acc=0.9710144927536232\n",
      "test: loss=0.05312180523401883 acc=0.9782608695652174\n",
      "EPOCH=1537\n",
      "train: loss=0.0221217037411502 acc=0.9898550724637681\n",
      "test: loss=0.054518021705551825 acc=0.9753623188405797\n",
      "EPOCH=1538\n",
      "train: loss=0.03378991007966229 acc=0.9840579710144928\n",
      "test: loss=0.054837917752938094 acc=0.9768115942028985\n",
      "EPOCH=1539\n",
      "train: loss=0.024925178580818417 acc=0.9884057971014493\n",
      "test: loss=0.05599872609635942 acc=0.9710144927536232\n",
      "EPOCH=1540\n",
      "train: loss=0.03654018709203557 acc=0.9782608695652174\n",
      "test: loss=0.05342799761934205 acc=0.9739130434782609\n",
      "EPOCH=1541\n",
      "train: loss=0.052593745236557014 acc=0.9768115942028985\n",
      "test: loss=0.0566578421405564 acc=0.9753623188405797\n",
      "EPOCH=1542\n",
      "train: loss=0.05579566248596867 acc=0.9753623188405797\n",
      "test: loss=0.055120169530783365 acc=0.972463768115942\n",
      "EPOCH=1543\n",
      "train: loss=0.05174298502599069 acc=0.9768115942028985\n",
      "test: loss=0.05476288130721879 acc=0.9753623188405797\n",
      "EPOCH=1544\n",
      "train: loss=0.05811251577330099 acc=0.9695652173913043\n",
      "test: loss=0.052361697752827556 acc=0.9753623188405797\n",
      "EPOCH=1545\n",
      "train: loss=0.04615239652035315 acc=0.9768115942028985\n",
      "test: loss=0.05483113646694613 acc=0.9710144927536232\n",
      "EPOCH=1546\n",
      "train: loss=0.028645269123746514 acc=0.9855072463768116\n",
      "test: loss=0.05804783836998351 acc=0.9768115942028985\n",
      "EPOCH=1547\n",
      "train: loss=0.038043764409545336 acc=0.9797101449275363\n",
      "test: loss=0.05476464585127571 acc=0.972463768115942\n",
      "EPOCH=1548\n",
      "train: loss=0.04211208766541336 acc=0.9768115942028985\n",
      "test: loss=0.05483811030159079 acc=0.9782608695652174\n",
      "EPOCH=1549\n",
      "train: loss=0.03857077749560557 acc=0.981159420289855\n",
      "test: loss=0.055737939968856565 acc=0.972463768115942\n",
      "EPOCH=1550\n",
      "train: loss=0.03923738648022904 acc=0.9898550724637681\n",
      "test: loss=0.0557024642212234 acc=0.972463768115942\n",
      "EPOCH=1551\n",
      "train: loss=0.05424700695985979 acc=0.9695652173913043\n",
      "test: loss=0.05418485680703017 acc=0.9710144927536232\n",
      "EPOCH=1552\n",
      "train: loss=0.059753851460636276 acc=0.9710144927536232\n",
      "test: loss=0.053770901841869265 acc=0.9753623188405797\n",
      "EPOCH=1553\n",
      "train: loss=0.03623364501108047 acc=0.9826086956521739\n",
      "test: loss=0.05295229259969263 acc=0.9739130434782609\n",
      "EPOCH=1554\n",
      "train: loss=0.06537177043623224 acc=0.9695652173913043\n",
      "test: loss=0.053367439920234266 acc=0.972463768115942\n",
      "EPOCH=1555\n",
      "train: loss=0.029903258017800624 acc=0.9869565217391304\n",
      "test: loss=0.053002158108858444 acc=0.9739130434782609\n",
      "EPOCH=1556\n",
      "train: loss=0.04375814135666103 acc=0.9826086956521739\n",
      "test: loss=0.05676004456432063 acc=0.9710144927536232\n",
      "EPOCH=1557\n",
      "train: loss=0.050959515446439924 acc=0.9681159420289855\n",
      "test: loss=0.054946828004267895 acc=0.9739130434782609\n",
      "EPOCH=1558\n",
      "train: loss=0.06556158093544494 acc=0.9623188405797102\n",
      "test: loss=0.05355616983545482 acc=0.9768115942028985\n",
      "EPOCH=1559\n",
      "train: loss=0.04852497820189579 acc=0.9782608695652174\n",
      "test: loss=0.054296701002270575 acc=0.9710144927536232\n",
      "EPOCH=1560\n",
      "train: loss=0.04366272097384374 acc=0.9797101449275363\n",
      "test: loss=0.0554955352188264 acc=0.9739130434782609\n",
      "EPOCH=1561\n",
      "train: loss=0.050377722698824866 acc=0.9753623188405797\n",
      "test: loss=0.05616971812447964 acc=0.972463768115942\n",
      "EPOCH=1562\n",
      "train: loss=0.042799544662349275 acc=0.981159420289855\n",
      "test: loss=0.056646712130431806 acc=0.9710144927536232\n",
      "EPOCH=1563\n",
      "train: loss=0.03515678742380935 acc=0.9855072463768116\n",
      "test: loss=0.05364381421057185 acc=0.9739130434782609\n",
      "EPOCH=1564\n",
      "train: loss=0.06774626808192105 acc=0.9594202898550724\n",
      "test: loss=0.05413909081146505 acc=0.9739130434782609\n",
      "EPOCH=1565\n",
      "train: loss=0.041993104635030594 acc=0.9782608695652174\n",
      "test: loss=0.053718747229937455 acc=0.9739130434782609\n",
      "EPOCH=1566\n",
      "train: loss=0.036643153711057544 acc=0.9826086956521739\n",
      "test: loss=0.05483287162590242 acc=0.9710144927536232\n",
      "EPOCH=1567\n",
      "train: loss=0.0628483682082037 acc=0.9681159420289855\n",
      "test: loss=0.054661479979959224 acc=0.9753623188405797\n",
      "EPOCH=1568\n",
      "train: loss=0.05030327625600116 acc=0.9782608695652174\n",
      "test: loss=0.05237251748580525 acc=0.9782608695652174\n",
      "EPOCH=1569\n",
      "train: loss=0.05393801645295006 acc=0.9739130434782609\n",
      "test: loss=0.054939432951904295 acc=0.9768115942028985\n",
      "EPOCH=1570\n",
      "train: loss=0.03979870137564844 acc=0.9826086956521739\n",
      "test: loss=0.05321368770180613 acc=0.9753623188405797\n",
      "EPOCH=1571\n",
      "train: loss=0.04215570104557 acc=0.9797101449275363\n",
      "test: loss=0.054369157750821476 acc=0.9710144927536232\n",
      "EPOCH=1572\n",
      "train: loss=0.04887649085698644 acc=0.9739130434782609\n",
      "test: loss=0.051599339176172335 acc=0.9753623188405797\n",
      "EPOCH=1573\n",
      "train: loss=0.06218388243538146 acc=0.9681159420289855\n",
      "test: loss=0.054087373165527496 acc=0.9739130434782609\n",
      "EPOCH=1574\n",
      "train: loss=0.047982496848329774 acc=0.9739130434782609\n",
      "test: loss=0.053410270738979304 acc=0.9753623188405797\n",
      "EPOCH=1575\n",
      "train: loss=0.049099180182722205 acc=0.9826086956521739\n",
      "test: loss=0.05325752370584411 acc=0.9768115942028985\n",
      "EPOCH=1576\n",
      "train: loss=0.046809100460285816 acc=0.9782608695652174\n",
      "test: loss=0.05294099444185541 acc=0.972463768115942\n",
      "EPOCH=1577\n",
      "train: loss=0.04306778016810249 acc=0.981159420289855\n",
      "test: loss=0.05303336156106468 acc=0.9753623188405797\n",
      "EPOCH=1578\n",
      "train: loss=0.05056543115043264 acc=0.9782608695652174\n",
      "test: loss=0.05426148167593011 acc=0.9739130434782609\n",
      "EPOCH=1579\n",
      "train: loss=0.04015488177899318 acc=0.9753623188405797\n",
      "test: loss=0.05370968166770397 acc=0.9739130434782609\n",
      "EPOCH=1580\n",
      "train: loss=0.04073853431895666 acc=0.9782608695652174\n",
      "test: loss=0.0528559619101359 acc=0.9739130434782609\n",
      "EPOCH=1581\n",
      "train: loss=0.07731265344428653 acc=0.9594202898550724\n",
      "test: loss=0.05652036990632546 acc=0.972463768115942\n",
      "EPOCH=1582\n",
      "train: loss=0.034879837263140825 acc=0.981159420289855\n",
      "test: loss=0.04974484000883872 acc=0.972463768115942\n",
      "EPOCH=1583\n",
      "train: loss=0.050791891559442824 acc=0.9768115942028985\n",
      "test: loss=0.05370666846070383 acc=0.9710144927536232\n",
      "EPOCH=1584\n",
      "train: loss=0.04491826567316622 acc=0.9753623188405797\n",
      "test: loss=0.05316269547121632 acc=0.9695652173913043\n",
      "EPOCH=1585\n",
      "train: loss=0.032775426659834184 acc=0.9826086956521739\n",
      "test: loss=0.053804002125103646 acc=0.9753623188405797\n",
      "EPOCH=1586\n",
      "train: loss=0.05993520866540644 acc=0.9739130434782609\n",
      "test: loss=0.05395697752813155 acc=0.9753623188405797\n",
      "EPOCH=1587\n",
      "train: loss=0.0702717040414241 acc=0.9695652173913043\n",
      "test: loss=0.05401300815310864 acc=0.9782608695652174\n",
      "EPOCH=1588\n",
      "train: loss=0.02494259702912904 acc=0.9869565217391304\n",
      "test: loss=0.051836174176993925 acc=0.9739130434782609\n",
      "EPOCH=1589\n",
      "train: loss=0.035970626340386495 acc=0.9840579710144928\n",
      "test: loss=0.053061963846053786 acc=0.9753623188405797\n",
      "EPOCH=1590\n",
      "train: loss=0.04753661378600092 acc=0.9768115942028985\n",
      "test: loss=0.05376387748471947 acc=0.9739130434782609\n",
      "EPOCH=1591\n",
      "train: loss=0.04687533455226207 acc=0.972463768115942\n",
      "test: loss=0.05448729104191219 acc=0.9753623188405797\n",
      "EPOCH=1592\n",
      "train: loss=0.054218911444088635 acc=0.9695652173913043\n",
      "test: loss=0.05458661886592636 acc=0.9753623188405797\n",
      "EPOCH=1593\n",
      "train: loss=0.05081719762876711 acc=0.9695652173913043\n",
      "test: loss=0.05128326122949781 acc=0.9739130434782609\n",
      "EPOCH=1594\n",
      "train: loss=0.06087167159534531 acc=0.9666666666666667\n",
      "test: loss=0.05282280807211212 acc=0.9768115942028985\n",
      "EPOCH=1595\n",
      "train: loss=0.030292417393754954 acc=0.9855072463768116\n",
      "test: loss=0.05341013046886178 acc=0.972463768115942\n",
      "EPOCH=1596\n",
      "train: loss=0.040662177567483436 acc=0.9782608695652174\n",
      "test: loss=0.05496483628876983 acc=0.9695652173913043\n",
      "EPOCH=1597\n",
      "train: loss=0.055108140732097514 acc=0.9666666666666667\n",
      "test: loss=0.05208781479230942 acc=0.9753623188405797\n",
      "EPOCH=1598\n",
      "train: loss=0.05390321451960582 acc=0.9739130434782609\n",
      "test: loss=0.05434950297799489 acc=0.9739130434782609\n",
      "EPOCH=1599\n",
      "train: loss=0.04095800492056624 acc=0.9826086956521739\n",
      "test: loss=0.05264110076091642 acc=0.9710144927536232\n",
      "EPOCH=1600\n",
      "train: loss=0.058133410690028864 acc=0.9681159420289855\n",
      "test: loss=0.05337968734029728 acc=0.972463768115942\n",
      "EPOCH=1601\n",
      "train: loss=0.047632625594128994 acc=0.9753623188405797\n",
      "test: loss=0.053425570259477584 acc=0.9710144927536232\n",
      "EPOCH=1602\n",
      "train: loss=0.03843283877650075 acc=0.981159420289855\n",
      "test: loss=0.05173211677096517 acc=0.9768115942028985\n",
      "EPOCH=1603\n",
      "train: loss=0.055501702842298856 acc=0.9753623188405797\n",
      "test: loss=0.05266753079820978 acc=0.9753623188405797\n",
      "EPOCH=1604\n",
      "train: loss=0.022020675598959958 acc=0.9884057971014493\n",
      "test: loss=0.05141036578020098 acc=0.9681159420289855\n",
      "EPOCH=1605\n",
      "train: loss=0.058764571553449704 acc=0.9695652173913043\n",
      "test: loss=0.053282220173563356 acc=0.9782608695652174\n",
      "EPOCH=1606\n",
      "train: loss=0.06247390119142197 acc=0.9681159420289855\n",
      "test: loss=0.054738945508911646 acc=0.9710144927536232\n",
      "EPOCH=1607\n",
      "train: loss=0.06216506029096355 acc=0.9666666666666667\n",
      "test: loss=0.04939265054271341 acc=0.9739130434782609\n",
      "EPOCH=1608\n",
      "train: loss=0.037179658727338624 acc=0.9826086956521739\n",
      "test: loss=0.05203660165848515 acc=0.9782608695652174\n",
      "EPOCH=1609\n",
      "train: loss=0.03492604654659227 acc=0.9884057971014493\n",
      "test: loss=0.052883816837591496 acc=0.9753623188405797\n",
      "EPOCH=1610\n",
      "train: loss=0.03850954633253459 acc=0.9826086956521739\n",
      "test: loss=0.05392429627567531 acc=0.9768115942028985\n",
      "EPOCH=1611\n",
      "train: loss=0.05233489506472821 acc=0.972463768115942\n",
      "test: loss=0.05534365263941378 acc=0.9739130434782609\n",
      "EPOCH=1612\n",
      "train: loss=0.04020987863168835 acc=0.9826086956521739\n",
      "test: loss=0.05445239291018116 acc=0.9782608695652174\n",
      "EPOCH=1613\n",
      "train: loss=0.04708053047112961 acc=0.9739130434782609\n",
      "test: loss=0.05376699157447794 acc=0.9753623188405797\n",
      "EPOCH=1614\n",
      "train: loss=0.05318200969129101 acc=0.9739130434782609\n",
      "test: loss=0.052301086409183864 acc=0.9768115942028985\n",
      "EPOCH=1615\n",
      "train: loss=0.031162732599208535 acc=0.9884057971014493\n",
      "test: loss=0.05131604345852081 acc=0.9768115942028985\n",
      "EPOCH=1616\n",
      "train: loss=0.052334941512935385 acc=0.9681159420289855\n",
      "test: loss=0.05182351711040177 acc=0.9768115942028985\n",
      "EPOCH=1617\n",
      "train: loss=0.049037339407398045 acc=0.972463768115942\n",
      "test: loss=0.05028018407150014 acc=0.9768115942028985\n",
      "EPOCH=1618\n",
      "train: loss=0.038418769623984945 acc=0.9797101449275363\n",
      "test: loss=0.05140428983119937 acc=0.9768115942028985\n",
      "EPOCH=1619\n",
      "train: loss=0.041744962000607885 acc=0.9797101449275363\n",
      "test: loss=0.05222523399677917 acc=0.9753623188405797\n",
      "EPOCH=1620\n",
      "train: loss=0.060724125172734615 acc=0.972463768115942\n",
      "test: loss=0.05192888253597266 acc=0.9739130434782609\n",
      "EPOCH=1621\n",
      "train: loss=0.04753030620527516 acc=0.9797101449275363\n",
      "test: loss=0.05306973930295626 acc=0.9753623188405797\n",
      "EPOCH=1622\n",
      "train: loss=0.03870978515793043 acc=0.9826086956521739\n",
      "test: loss=0.050408245256344804 acc=0.9768115942028985\n",
      "EPOCH=1623\n",
      "train: loss=0.049393286496345265 acc=0.9753623188405797\n",
      "test: loss=0.05131432311595819 acc=0.972463768115942\n",
      "EPOCH=1624\n",
      "train: loss=0.058427793256684964 acc=0.9637681159420289\n",
      "test: loss=0.04965321322226249 acc=0.972463768115942\n",
      "EPOCH=1625\n",
      "train: loss=0.05026430332084306 acc=0.9753623188405797\n",
      "test: loss=0.05128437451399382 acc=0.9739130434782609\n",
      "EPOCH=1626\n",
      "train: loss=0.0633598430298134 acc=0.9681159420289855\n",
      "test: loss=0.052267528362584144 acc=0.9739130434782609\n",
      "EPOCH=1627\n",
      "train: loss=0.05199683640692899 acc=0.9768115942028985\n",
      "test: loss=0.052937033403808544 acc=0.9739130434782609\n",
      "EPOCH=1628\n",
      "train: loss=0.03494347657549072 acc=0.9782608695652174\n",
      "test: loss=0.0522719012529659 acc=0.9710144927536232\n",
      "EPOCH=1629\n",
      "train: loss=0.04755221189684131 acc=0.9768115942028985\n",
      "test: loss=0.05239017152722546 acc=0.9739130434782609\n",
      "EPOCH=1630\n",
      "train: loss=0.046712808477586035 acc=0.9753623188405797\n",
      "test: loss=0.053594782515893225 acc=0.9710144927536232\n",
      "EPOCH=1631\n",
      "train: loss=0.05640503595612238 acc=0.972463768115942\n",
      "test: loss=0.053217530804838625 acc=0.9782608695652174\n",
      "EPOCH=1632\n",
      "train: loss=0.06263771014952932 acc=0.9681159420289855\n",
      "test: loss=0.05450087388268314 acc=0.972463768115942\n",
      "EPOCH=1633\n",
      "train: loss=0.04469691210733469 acc=0.9739130434782609\n",
      "test: loss=0.05189216045072462 acc=0.972463768115942\n",
      "EPOCH=1634\n",
      "train: loss=0.04565662248561236 acc=0.9739130434782609\n",
      "test: loss=0.052311645682973265 acc=0.9753623188405797\n",
      "EPOCH=1635\n",
      "train: loss=0.05583929841240304 acc=0.972463768115942\n",
      "test: loss=0.05136835611617963 acc=0.9710144927536232\n",
      "EPOCH=1636\n",
      "train: loss=0.041126030748672424 acc=0.9797101449275363\n",
      "test: loss=0.05138714524120747 acc=0.9753623188405797\n",
      "EPOCH=1637\n",
      "train: loss=0.05558806935336627 acc=0.9637681159420289\n",
      "test: loss=0.050320029088647175 acc=0.9797101449275363\n",
      "EPOCH=1638\n",
      "train: loss=0.03156026124975948 acc=0.9826086956521739\n",
      "test: loss=0.05314230054550185 acc=0.9710144927536232\n",
      "EPOCH=1639\n",
      "train: loss=0.04810578981667069 acc=0.9826086956521739\n",
      "test: loss=0.0505320876065906 acc=0.9753623188405797\n",
      "EPOCH=1640\n",
      "train: loss=0.07443238000152624 acc=0.9594202898550724\n",
      "test: loss=0.05282980797178236 acc=0.9739130434782609\n",
      "EPOCH=1641\n",
      "train: loss=0.06196936864256573 acc=0.9681159420289855\n",
      "test: loss=0.05114909053897393 acc=0.9710144927536232\n",
      "EPOCH=1642\n",
      "train: loss=0.026056334361893127 acc=0.9855072463768116\n",
      "test: loss=0.05015551046220123 acc=0.9739130434782609\n",
      "EPOCH=1643\n",
      "train: loss=0.03696498270247498 acc=0.9797101449275363\n",
      "test: loss=0.05210557258539665 acc=0.9753623188405797\n",
      "EPOCH=1644\n",
      "train: loss=0.04705068167783844 acc=0.9768115942028985\n",
      "test: loss=0.05085340224106488 acc=0.9768115942028985\n",
      "EPOCH=1645\n",
      "train: loss=0.04265350207833416 acc=0.9739130434782609\n",
      "test: loss=0.05049797421565481 acc=0.9681159420289855\n",
      "EPOCH=1646\n",
      "train: loss=0.043319902536081235 acc=0.981159420289855\n",
      "test: loss=0.051278947827947306 acc=0.9753623188405797\n",
      "EPOCH=1647\n",
      "train: loss=0.02826730476052692 acc=0.9884057971014493\n",
      "test: loss=0.0527747078930724 acc=0.9768115942028985\n",
      "EPOCH=1648\n",
      "train: loss=0.060815614803121844 acc=0.9710144927536232\n",
      "test: loss=0.05126926381070611 acc=0.9782608695652174\n",
      "EPOCH=1649\n",
      "train: loss=0.03670071691151497 acc=0.9855072463768116\n",
      "test: loss=0.05302648439221024 acc=0.972463768115942\n",
      "EPOCH=1650\n",
      "train: loss=0.04408109208553098 acc=0.9768115942028985\n",
      "test: loss=0.05014288515529307 acc=0.9753623188405797\n",
      "EPOCH=1651\n",
      "train: loss=0.05171587428972183 acc=0.972463768115942\n",
      "test: loss=0.05126227189357365 acc=0.972463768115942\n",
      "EPOCH=1652\n",
      "train: loss=0.03408526874527542 acc=0.9826086956521739\n",
      "test: loss=0.05164089418449325 acc=0.9739130434782609\n",
      "EPOCH=1653\n",
      "train: loss=0.03931022395221774 acc=0.9826086956521739\n",
      "test: loss=0.050885480162280915 acc=0.972463768115942\n",
      "EPOCH=1654\n",
      "train: loss=0.06194203802363459 acc=0.9753623188405797\n",
      "test: loss=0.05166361746850992 acc=0.9782608695652174\n",
      "EPOCH=1655\n",
      "train: loss=0.03600128301724136 acc=0.981159420289855\n",
      "test: loss=0.05077142464337155 acc=0.9739130434782609\n",
      "EPOCH=1656\n",
      "train: loss=0.061712596732037835 acc=0.9681159420289855\n",
      "test: loss=0.05674216196730057 acc=0.9753623188405797\n",
      "EPOCH=1657\n",
      "train: loss=0.0254641691725198 acc=0.9855072463768116\n",
      "test: loss=0.052501404798638615 acc=0.9739130434782609\n",
      "EPOCH=1658\n",
      "train: loss=0.0617203842973551 acc=0.9666666666666667\n",
      "test: loss=0.050708365536100315 acc=0.9768115942028985\n",
      "EPOCH=1659\n",
      "train: loss=0.061446627564532195 acc=0.9666666666666667\n",
      "test: loss=0.04970347422452758 acc=0.9753623188405797\n",
      "EPOCH=1660\n",
      "train: loss=0.038874332391148535 acc=0.981159420289855\n",
      "test: loss=0.051930880949039374 acc=0.9710144927536232\n",
      "EPOCH=1661\n",
      "train: loss=0.05786841653143716 acc=0.9710144927536232\n",
      "test: loss=0.05078301703427877 acc=0.972463768115942\n",
      "EPOCH=1662\n",
      "train: loss=0.0421916785005361 acc=0.9768115942028985\n",
      "test: loss=0.051393676557961745 acc=0.9768115942028985\n",
      "EPOCH=1663\n",
      "train: loss=0.032284113044492094 acc=0.9840579710144928\n",
      "test: loss=0.05082191272351268 acc=0.9753623188405797\n",
      "EPOCH=1664\n",
      "train: loss=0.03762043347971209 acc=0.9782608695652174\n",
      "test: loss=0.051139722247730376 acc=0.9710144927536232\n",
      "EPOCH=1665\n",
      "train: loss=0.038825744905569226 acc=0.9826086956521739\n",
      "test: loss=0.05099548878783733 acc=0.9753623188405797\n",
      "EPOCH=1666\n",
      "train: loss=0.07237246948046666 acc=0.9666666666666667\n",
      "test: loss=0.05093053825695969 acc=0.9739130434782609\n",
      "EPOCH=1667\n",
      "train: loss=0.05324735468645729 acc=0.9768115942028985\n",
      "test: loss=0.051187534668252224 acc=0.9710144927536232\n",
      "EPOCH=1668\n",
      "train: loss=0.032124422785635105 acc=0.9840579710144928\n",
      "test: loss=0.052351903038766356 acc=0.972463768115942\n",
      "EPOCH=1669\n",
      "train: loss=0.04882817395779595 acc=0.9753623188405797\n",
      "test: loss=0.05039751442703714 acc=0.9739130434782609\n",
      "EPOCH=1670\n",
      "train: loss=0.03018374584806018 acc=0.9840579710144928\n",
      "test: loss=0.04954188363429537 acc=0.9753623188405797\n",
      "EPOCH=1671\n",
      "train: loss=0.03734529964552301 acc=0.981159420289855\n",
      "test: loss=0.05107682365659478 acc=0.972463768115942\n",
      "EPOCH=1672\n",
      "train: loss=0.046534097366997956 acc=0.9739130434782609\n",
      "test: loss=0.05120790102513536 acc=0.9753623188405797\n",
      "EPOCH=1673\n",
      "train: loss=0.055512727552156936 acc=0.9666666666666667\n",
      "test: loss=0.04901029848608063 acc=0.9753623188405797\n",
      "EPOCH=1674\n",
      "train: loss=0.03276026797472885 acc=0.9869565217391304\n",
      "test: loss=0.05019845751399883 acc=0.972463768115942\n",
      "EPOCH=1675\n",
      "train: loss=0.042066715171807864 acc=0.9768115942028985\n",
      "test: loss=0.052129648644968496 acc=0.9739130434782609\n",
      "EPOCH=1676\n",
      "train: loss=0.04190960702420069 acc=0.981159420289855\n",
      "test: loss=0.04963927675690369 acc=0.9753623188405797\n",
      "EPOCH=1677\n",
      "train: loss=0.04566411269251339 acc=0.9753623188405797\n",
      "test: loss=0.048932660521993646 acc=0.9753623188405797\n",
      "EPOCH=1678\n",
      "train: loss=0.027699741720351067 acc=0.9869565217391304\n",
      "test: loss=0.04979962183976623 acc=0.9768115942028985\n",
      "EPOCH=1679\n",
      "train: loss=0.05036072058521569 acc=0.972463768115942\n",
      "test: loss=0.04878830825475327 acc=0.9768115942028985\n",
      "EPOCH=1680\n",
      "train: loss=0.05164815300124665 acc=0.9695652173913043\n",
      "test: loss=0.05186998478115125 acc=0.9739130434782609\n",
      "EPOCH=1681\n",
      "train: loss=0.05152486268654257 acc=0.9753623188405797\n",
      "test: loss=0.04933498812136986 acc=0.972463768115942\n",
      "EPOCH=1682\n",
      "train: loss=0.04631333390541906 acc=0.9739130434782609\n",
      "test: loss=0.050734264022489685 acc=0.9797101449275363\n",
      "EPOCH=1683\n",
      "train: loss=0.03503452886349178 acc=0.9840579710144928\n",
      "test: loss=0.049459930961494716 acc=0.9753623188405797\n",
      "EPOCH=1684\n",
      "train: loss=0.04599439129718074 acc=0.9782608695652174\n",
      "test: loss=0.0495126643181261 acc=0.9782608695652174\n",
      "EPOCH=1685\n",
      "train: loss=0.05907679927539048 acc=0.9681159420289855\n",
      "test: loss=0.04981162648673897 acc=0.9753623188405797\n",
      "EPOCH=1686\n",
      "train: loss=0.0308042695342192 acc=0.9855072463768116\n",
      "test: loss=0.05017662130054016 acc=0.9739130434782609\n",
      "EPOCH=1687\n",
      "train: loss=0.02927622165297634 acc=0.981159420289855\n",
      "test: loss=0.04965984516045773 acc=0.9782608695652174\n",
      "EPOCH=1688\n",
      "train: loss=0.04479030866285871 acc=0.972463768115942\n",
      "test: loss=0.04833038167591122 acc=0.9739130434782609\n",
      "EPOCH=1689\n",
      "train: loss=0.01823243758092186 acc=0.9927536231884058\n",
      "test: loss=0.05007889243122568 acc=0.9768115942028985\n",
      "EPOCH=1690\n",
      "train: loss=0.04242717337625241 acc=0.9840579710144928\n",
      "test: loss=0.051229904357690516 acc=0.9710144927536232\n",
      "EPOCH=1691\n",
      "train: loss=0.03625203662959851 acc=0.9797101449275363\n",
      "test: loss=0.04919040470560591 acc=0.972463768115942\n",
      "EPOCH=1692\n",
      "train: loss=0.034955336315034675 acc=0.9840579710144928\n",
      "test: loss=0.049545294414393225 acc=0.9753623188405797\n",
      "EPOCH=1693\n",
      "train: loss=0.025000268304218094 acc=0.9927536231884058\n",
      "test: loss=0.04982216396294225 acc=0.9695652173913043\n",
      "EPOCH=1694\n",
      "train: loss=0.05492460842192836 acc=0.9666666666666667\n",
      "test: loss=0.049717340279350875 acc=0.9739130434782609\n",
      "EPOCH=1695\n",
      "train: loss=0.0478388584434364 acc=0.9753623188405797\n",
      "test: loss=0.050892348740419396 acc=0.9753623188405797\n",
      "EPOCH=1696\n",
      "train: loss=0.04069130651537619 acc=0.9782608695652174\n",
      "test: loss=0.04907258419223818 acc=0.972463768115942\n",
      "EPOCH=1697\n",
      "train: loss=0.31831994934517904 acc=0.9246376811594202\n",
      "test: loss=0.27139753092929153 acc=0.9202898550724637\n",
      "EPOCH=1698\n",
      "train: loss=0.1777276324187277 acc=0.9449275362318841\n",
      "test: loss=0.22921806638518463 acc=0.9318840579710145\n",
      "EPOCH=1699\n",
      "train: loss=0.4468234820083167 acc=0.8463768115942029\n",
      "test: loss=0.458175364566921 acc=0.8289855072463768\n",
      "EPOCH=1700\n",
      "train: loss=0.12215573623145674 acc=0.9478260869565217\n",
      "test: loss=0.12635596663788784 acc=0.9449275362318841\n",
      "EPOCH=1701\n",
      "train: loss=0.10485649330983388 acc=0.9681159420289855\n",
      "test: loss=0.07510976054192635 acc=0.9652173913043478\n",
      "EPOCH=1702\n",
      "train: loss=0.033908448535312004 acc=0.9826086956521739\n",
      "test: loss=0.05681452299005028 acc=0.9695652173913043\n",
      "EPOCH=1703\n",
      "train: loss=0.11091475413108239 acc=0.9594202898550724\n",
      "test: loss=0.09592092137581282 acc=0.9637681159420289\n",
      "EPOCH=1704\n",
      "train: loss=0.07798634562420882 acc=0.9608695652173913\n",
      "test: loss=0.0801071369333806 acc=0.9637681159420289\n",
      "EPOCH=1705\n",
      "train: loss=0.06691421532955678 acc=0.9739130434782609\n",
      "test: loss=0.08293643370867908 acc=0.9666666666666667\n",
      "EPOCH=1706\n",
      "train: loss=0.06069379110318262 acc=0.9739130434782609\n",
      "test: loss=0.1070981585347853 acc=0.9623188405797102\n",
      "EPOCH=1707\n",
      "train: loss=0.06283310086898185 acc=0.9710144927536232\n",
      "test: loss=0.07735925828098382 acc=0.9637681159420289\n",
      "EPOCH=1708\n",
      "train: loss=0.049115684798481704 acc=0.9710144927536232\n",
      "test: loss=0.0606692639170451 acc=0.9710144927536232\n",
      "EPOCH=1709\n",
      "train: loss=0.06020427447520279 acc=0.9710144927536232\n",
      "test: loss=0.11377590914401367 acc=0.9492753623188406\n",
      "EPOCH=1710\n",
      "train: loss=0.05770090322771867 acc=0.9753623188405797\n",
      "test: loss=0.08409250845907057 acc=0.9594202898550724\n",
      "EPOCH=1711\n",
      "train: loss=0.06404991630536461 acc=0.972463768115942\n",
      "test: loss=0.0688352212461059 acc=0.9666666666666667\n",
      "EPOCH=1712\n",
      "train: loss=0.04772921052223831 acc=0.9768115942028985\n",
      "test: loss=0.06484072336194767 acc=0.9710144927536232\n",
      "EPOCH=1713\n",
      "train: loss=0.059764884633447216 acc=0.9710144927536232\n",
      "test: loss=0.05867792117277421 acc=0.9623188405797102\n",
      "EPOCH=1714\n",
      "train: loss=0.03313089329585506 acc=0.981159420289855\n",
      "test: loss=0.07133869653494418 acc=0.9666666666666667\n",
      "EPOCH=1715\n",
      "train: loss=0.054599175191709974 acc=0.9739130434782609\n",
      "test: loss=0.06225640393364129 acc=0.9695652173913043\n",
      "EPOCH=1716\n",
      "train: loss=0.06859069659351154 acc=0.972463768115942\n",
      "test: loss=0.061542945914966764 acc=0.9739130434782609\n",
      "EPOCH=1717\n",
      "train: loss=0.031085036005421816 acc=0.9869565217391304\n",
      "test: loss=0.06068136348961362 acc=0.972463768115942\n",
      "EPOCH=1718\n",
      "train: loss=0.03192997440426743 acc=0.9869565217391304\n",
      "test: loss=0.05758727629196844 acc=0.9681159420289855\n",
      "EPOCH=1719\n",
      "train: loss=0.03734784834425368 acc=0.981159420289855\n",
      "test: loss=0.054932044204110214 acc=0.9739130434782609\n",
      "EPOCH=1720\n",
      "train: loss=0.055074609191461316 acc=0.9739130434782609\n",
      "test: loss=0.05912148076873936 acc=0.9710144927536232\n",
      "EPOCH=1721\n",
      "train: loss=0.05754520006966437 acc=0.9666666666666667\n",
      "test: loss=0.05819219473736189 acc=0.972463768115942\n",
      "EPOCH=1722\n",
      "train: loss=0.051382677993855125 acc=0.9739130434782609\n",
      "test: loss=0.057106708181634686 acc=0.9739130434782609\n",
      "EPOCH=1723\n",
      "train: loss=0.04474443758996105 acc=0.9782608695652174\n",
      "test: loss=0.05829999147622283 acc=0.9739130434782609\n",
      "EPOCH=1724\n",
      "train: loss=0.0531759815259354 acc=0.972463768115942\n",
      "test: loss=0.06152972297392083 acc=0.9695652173913043\n",
      "EPOCH=1725\n",
      "train: loss=0.05312065216300334 acc=0.9753623188405797\n",
      "test: loss=0.06364179389301439 acc=0.9710144927536232\n",
      "EPOCH=1726\n",
      "train: loss=0.041416545711868584 acc=0.981159420289855\n",
      "test: loss=0.059136252067878536 acc=0.9695652173913043\n",
      "EPOCH=1727\n",
      "train: loss=0.03906891520658111 acc=0.9840579710144928\n",
      "test: loss=0.057887957704518246 acc=0.9666666666666667\n",
      "EPOCH=1728\n",
      "train: loss=0.04748598451804077 acc=0.9739130434782609\n",
      "test: loss=0.06562047744545489 acc=0.9695652173913043\n",
      "EPOCH=1729\n",
      "train: loss=0.06344337031711014 acc=0.9681159420289855\n",
      "test: loss=0.05522108241246094 acc=0.9710144927536232\n",
      "EPOCH=1730\n",
      "train: loss=0.0426999565701953 acc=0.981159420289855\n",
      "test: loss=0.05545843637857472 acc=0.9739130434782609\n",
      "EPOCH=1731\n",
      "train: loss=0.04389497294836928 acc=0.9768115942028985\n",
      "test: loss=0.058673503933041576 acc=0.9681159420289855\n",
      "EPOCH=1732\n",
      "train: loss=0.06366665351224798 acc=0.9710144927536232\n",
      "test: loss=0.06246526560769811 acc=0.9695652173913043\n",
      "EPOCH=1733\n",
      "train: loss=0.03582406077734898 acc=0.9797101449275363\n",
      "test: loss=0.05771613985297802 acc=0.9695652173913043\n",
      "EPOCH=1734\n",
      "train: loss=0.048065486654215984 acc=0.9782608695652174\n",
      "test: loss=0.06105333206298954 acc=0.9710144927536232\n",
      "EPOCH=1735\n",
      "train: loss=0.061590792925253275 acc=0.9623188405797102\n",
      "test: loss=0.05745273370659478 acc=0.972463768115942\n",
      "EPOCH=1736\n",
      "train: loss=0.03811072982131187 acc=0.9797101449275363\n",
      "test: loss=0.06508881795511992 acc=0.9681159420289855\n",
      "EPOCH=1737\n",
      "train: loss=0.05023172347313098 acc=0.9753623188405797\n",
      "test: loss=0.0622512402461487 acc=0.9710144927536232\n",
      "EPOCH=1738\n",
      "train: loss=0.04810653682598881 acc=0.9782608695652174\n",
      "test: loss=0.06537418911378587 acc=0.9681159420289855\n",
      "EPOCH=1739\n",
      "train: loss=0.05420728112917246 acc=0.9710144927536232\n",
      "test: loss=0.05915298461751594 acc=0.9739130434782609\n",
      "EPOCH=1740\n",
      "train: loss=0.04944940370497887 acc=0.9739130434782609\n",
      "test: loss=0.056421406586245136 acc=0.972463768115942\n",
      "EPOCH=1741\n",
      "train: loss=0.04781208123340083 acc=0.9710144927536232\n",
      "test: loss=0.0540888796846753 acc=0.9710144927536232\n",
      "EPOCH=1742\n",
      "train: loss=0.05360666365118147 acc=0.9710144927536232\n",
      "test: loss=0.05777888749713688 acc=0.9710144927536232\n",
      "EPOCH=1743\n",
      "train: loss=0.03662033564665831 acc=0.9826086956521739\n",
      "test: loss=0.05422486218346391 acc=0.972463768115942\n",
      "EPOCH=1744\n",
      "train: loss=0.03144945678258452 acc=0.9797101449275363\n",
      "test: loss=0.05482327409013121 acc=0.9739130434782609\n",
      "EPOCH=1745\n",
      "train: loss=0.052505310292192076 acc=0.9753623188405797\n",
      "test: loss=0.05795708513665572 acc=0.9681159420289855\n",
      "EPOCH=1746\n",
      "train: loss=0.06033460734796467 acc=0.9753623188405797\n",
      "test: loss=0.056061331485472564 acc=0.9681159420289855\n",
      "EPOCH=1747\n",
      "train: loss=0.05502883843841348 acc=0.9710144927536232\n",
      "test: loss=0.056421376033707116 acc=0.9710144927536232\n",
      "EPOCH=1748\n",
      "train: loss=0.0535720445739048 acc=0.9666666666666667\n",
      "test: loss=0.06165549185857868 acc=0.9710144927536232\n",
      "EPOCH=1749\n",
      "train: loss=0.03561453881978738 acc=0.9826086956521739\n",
      "test: loss=0.059706714811891486 acc=0.9695652173913043\n",
      "EPOCH=1750\n",
      "train: loss=0.02143284759878576 acc=0.9884057971014493\n",
      "test: loss=0.05146372414118297 acc=0.972463768115942\n",
      "EPOCH=1751\n",
      "train: loss=0.048364913057382844 acc=0.9782608695652174\n",
      "test: loss=0.06295181813010582 acc=0.972463768115942\n",
      "EPOCH=1752\n",
      "train: loss=0.043164614286433624 acc=0.9826086956521739\n",
      "test: loss=0.0645334459384234 acc=0.9695652173913043\n",
      "EPOCH=1753\n",
      "train: loss=0.03709637484203692 acc=0.9782608695652174\n",
      "test: loss=0.060326232656258263 acc=0.9710144927536232\n",
      "EPOCH=1754\n",
      "train: loss=0.0434485368870349 acc=0.9797101449275363\n",
      "test: loss=0.0561567481201447 acc=0.9681159420289855\n",
      "EPOCH=1755\n",
      "train: loss=0.05836467152611627 acc=0.9739130434782609\n",
      "test: loss=0.05847402883051917 acc=0.9637681159420289\n",
      "EPOCH=1756\n",
      "train: loss=0.055113178181536715 acc=0.9666666666666667\n",
      "test: loss=0.055125540367919244 acc=0.9695652173913043\n",
      "EPOCH=1757\n",
      "train: loss=0.05724701469119935 acc=0.972463768115942\n",
      "test: loss=0.05719789472543032 acc=0.9710144927536232\n",
      "EPOCH=1758\n",
      "train: loss=0.023296268226655273 acc=0.9869565217391304\n",
      "test: loss=0.05368353481982192 acc=0.9710144927536232\n",
      "EPOCH=1759\n",
      "train: loss=0.07331379346516625 acc=0.9623188405797102\n",
      "test: loss=0.05471551850790946 acc=0.9681159420289855\n",
      "EPOCH=1760\n",
      "train: loss=0.05429649023389415 acc=0.9739130434782609\n",
      "test: loss=0.05336885100926254 acc=0.972463768115942\n",
      "EPOCH=1761\n",
      "train: loss=0.0367940479778693 acc=0.9826086956521739\n",
      "test: loss=0.05713698793094802 acc=0.9695652173913043\n",
      "EPOCH=1762\n",
      "train: loss=0.05098294680953279 acc=0.9753623188405797\n",
      "test: loss=0.05378252014489551 acc=0.9739130434782609\n",
      "EPOCH=1763\n",
      "train: loss=0.046888176031608615 acc=0.9753623188405797\n",
      "test: loss=0.05148184040232701 acc=0.9710144927536232\n",
      "EPOCH=1764\n",
      "train: loss=0.05705083522784944 acc=0.9768115942028985\n",
      "test: loss=0.052328125457131315 acc=0.9710144927536232\n",
      "EPOCH=1765\n",
      "train: loss=0.04035062252113409 acc=0.9797101449275363\n",
      "test: loss=0.060047748612513746 acc=0.9695652173913043\n",
      "EPOCH=1766\n",
      "train: loss=0.051891594597404535 acc=0.9739130434782609\n",
      "test: loss=0.05723673897588617 acc=0.9739130434782609\n",
      "EPOCH=1767\n",
      "train: loss=0.0400529158055618 acc=0.9855072463768116\n",
      "test: loss=0.05213603171377402 acc=0.9710144927536232\n",
      "EPOCH=1768\n",
      "train: loss=0.06299588965233266 acc=0.9666666666666667\n",
      "test: loss=0.056127069407854134 acc=0.972463768115942\n",
      "EPOCH=1769\n",
      "train: loss=0.05227877959367361 acc=0.9739130434782609\n",
      "test: loss=0.05645440775100094 acc=0.9739130434782609\n",
      "EPOCH=1770\n",
      "train: loss=0.03957448666868967 acc=0.9797101449275363\n",
      "test: loss=0.05844018717645913 acc=0.9681159420289855\n",
      "EPOCH=1771\n",
      "train: loss=0.02312051954500484 acc=0.9855072463768116\n",
      "test: loss=0.06093701227509738 acc=0.9695652173913043\n",
      "EPOCH=1772\n",
      "train: loss=0.054032906147722944 acc=0.9710144927536232\n",
      "test: loss=0.05670404741655717 acc=0.9710144927536232\n",
      "EPOCH=1773\n",
      "train: loss=0.04405008971769565 acc=0.9768115942028985\n",
      "test: loss=0.05691234490346324 acc=0.972463768115942\n",
      "EPOCH=1774\n",
      "train: loss=0.03324331657302969 acc=0.9840579710144928\n",
      "test: loss=0.05394596395908616 acc=0.9652173913043478\n",
      "EPOCH=1775\n",
      "train: loss=0.037634523655746696 acc=0.981159420289855\n",
      "test: loss=0.05690669188988685 acc=0.9710144927536232\n",
      "EPOCH=1776\n",
      "train: loss=0.04336410102504454 acc=0.9797101449275363\n",
      "test: loss=0.05427767318036111 acc=0.9695652173913043\n",
      "EPOCH=1777\n",
      "train: loss=0.04949477558795563 acc=0.9739130434782609\n",
      "test: loss=0.060340030845771025 acc=0.9652173913043478\n",
      "EPOCH=1778\n",
      "train: loss=0.07016018782117193 acc=0.9579710144927536\n",
      "test: loss=0.05294350486981342 acc=0.9695652173913043\n",
      "EPOCH=1779\n",
      "train: loss=0.06033928073721586 acc=0.9637681159420289\n",
      "test: loss=0.0567938059207558 acc=0.9695652173913043\n",
      "EPOCH=1780\n",
      "train: loss=0.03211355581789803 acc=0.981159420289855\n",
      "test: loss=0.05489483499490163 acc=0.9695652173913043\n",
      "EPOCH=1781\n",
      "train: loss=0.06871866989715253 acc=0.9681159420289855\n",
      "test: loss=0.05356570931936182 acc=0.9652173913043478\n",
      "EPOCH=1782\n",
      "train: loss=0.050980939733884824 acc=0.972463768115942\n",
      "test: loss=0.05629356210865597 acc=0.972463768115942\n",
      "EPOCH=1783\n",
      "train: loss=0.03809388998127948 acc=0.9797101449275363\n",
      "test: loss=0.05415725055730717 acc=0.9768115942028985\n",
      "EPOCH=1784\n",
      "train: loss=0.05341654725915635 acc=0.972463768115942\n",
      "test: loss=0.052653305180100474 acc=0.9739130434782609\n",
      "EPOCH=1785\n",
      "train: loss=0.04682177885167932 acc=0.981159420289855\n",
      "test: loss=0.05279365905827762 acc=0.972463768115942\n",
      "EPOCH=1786\n",
      "train: loss=0.06577623133452519 acc=0.9666666666666667\n",
      "test: loss=0.055485482303014436 acc=0.9681159420289855\n",
      "EPOCH=1787\n",
      "train: loss=0.05809758679606345 acc=0.972463768115942\n",
      "test: loss=0.06230648086287062 acc=0.9681159420289855\n",
      "EPOCH=1788\n",
      "train: loss=0.04601265743352695 acc=0.9840579710144928\n",
      "test: loss=0.05266495585338447 acc=0.9739130434782609\n",
      "EPOCH=1789\n",
      "train: loss=0.056126979202447365 acc=0.9666666666666667\n",
      "test: loss=0.053902462613859115 acc=0.9768115942028985\n",
      "EPOCH=1790\n",
      "train: loss=0.03562225733000907 acc=0.9840579710144928\n",
      "test: loss=0.06136269403560088 acc=0.9666666666666667\n",
      "EPOCH=1791\n",
      "train: loss=0.04110296566765 acc=0.9826086956521739\n",
      "test: loss=0.05192219833986226 acc=0.9768115942028985\n",
      "EPOCH=1792\n",
      "train: loss=0.05288662066884493 acc=0.9739130434782609\n",
      "test: loss=0.05356554402225364 acc=0.972463768115942\n",
      "EPOCH=1793\n",
      "train: loss=0.0460268122945693 acc=0.9753623188405797\n",
      "test: loss=0.053832164469551094 acc=0.9753623188405797\n",
      "EPOCH=1794\n",
      "train: loss=0.057475086430901226 acc=0.972463768115942\n",
      "test: loss=0.056650991336942465 acc=0.9739130434782609\n",
      "EPOCH=1795\n",
      "train: loss=0.061765064220524024 acc=0.9753623188405797\n",
      "test: loss=0.055122378190745704 acc=0.9739130434782609\n",
      "EPOCH=1796\n",
      "train: loss=0.057081060354988764 acc=0.9753623188405797\n",
      "test: loss=0.054962380442537 acc=0.9681159420289855\n",
      "EPOCH=1797\n",
      "train: loss=0.04791509928437328 acc=0.972463768115942\n",
      "test: loss=0.055396473838445204 acc=0.9695652173913043\n",
      "EPOCH=1798\n",
      "train: loss=0.052181552268203195 acc=0.9768115942028985\n",
      "test: loss=0.05452870979903256 acc=0.9681159420289855\n",
      "EPOCH=1799\n",
      "train: loss=0.04202729353982548 acc=0.9768115942028985\n",
      "test: loss=0.053088741915675544 acc=0.972463768115942\n",
      "EPOCH=1800\n",
      "train: loss=0.03259997341198216 acc=0.9869565217391304\n",
      "test: loss=0.057546248199951903 acc=0.9681159420289855\n",
      "EPOCH=1801\n",
      "train: loss=0.05787333857571833 acc=0.9710144927536232\n",
      "test: loss=0.0566529113968998 acc=0.9695652173913043\n",
      "EPOCH=1802\n",
      "train: loss=0.04262933511347071 acc=0.9768115942028985\n",
      "test: loss=0.05340559811025874 acc=0.9753623188405797\n",
      "EPOCH=1803\n",
      "train: loss=0.0489209065090241 acc=0.9695652173913043\n",
      "test: loss=0.051627284443250515 acc=0.972463768115942\n",
      "EPOCH=1804\n",
      "train: loss=0.04360333888016146 acc=0.9768115942028985\n",
      "test: loss=0.05077013085284733 acc=0.9739130434782609\n",
      "EPOCH=1805\n",
      "train: loss=0.051429019176304956 acc=0.9753623188405797\n",
      "test: loss=0.05276745176583328 acc=0.972463768115942\n",
      "EPOCH=1806\n",
      "train: loss=0.053063765808398694 acc=0.9753623188405797\n",
      "test: loss=0.051799883854973354 acc=0.9739130434782609\n",
      "EPOCH=1807\n",
      "train: loss=0.054368439902720375 acc=0.972463768115942\n",
      "test: loss=0.05599064085930705 acc=0.9710144927536232\n",
      "EPOCH=1808\n",
      "train: loss=0.05602480071731296 acc=0.9695652173913043\n",
      "test: loss=0.057864796571660444 acc=0.9710144927536232\n",
      "EPOCH=1809\n",
      "train: loss=0.06304735715614311 acc=0.9637681159420289\n",
      "test: loss=0.0531503437377769 acc=0.972463768115942\n",
      "EPOCH=1810\n",
      "train: loss=0.03975440363212563 acc=0.9768115942028985\n",
      "test: loss=0.05271613830317453 acc=0.9695652173913043\n",
      "EPOCH=1811\n",
      "train: loss=0.03153098543358194 acc=0.9884057971014493\n",
      "test: loss=0.055505901191311095 acc=0.972463768115942\n",
      "EPOCH=1812\n",
      "train: loss=0.06175722661566156 acc=0.9695652173913043\n",
      "test: loss=0.058362063858461585 acc=0.9695652173913043\n",
      "EPOCH=1813\n",
      "train: loss=0.048597044468567406 acc=0.9797101449275363\n",
      "test: loss=0.05363757924717617 acc=0.9695652173913043\n",
      "EPOCH=1814\n",
      "train: loss=0.039034338463708275 acc=0.9826086956521739\n",
      "test: loss=0.05246920545357205 acc=0.9753623188405797\n",
      "EPOCH=1815\n",
      "train: loss=0.0346601754642786 acc=0.981159420289855\n",
      "test: loss=0.05911107283764366 acc=0.9681159420289855\n",
      "EPOCH=1816\n",
      "train: loss=0.07716207326145959 acc=0.9623188405797102\n",
      "test: loss=0.054580238833693714 acc=0.9695652173913043\n",
      "EPOCH=1817\n",
      "train: loss=0.033948622666632934 acc=0.9840579710144928\n",
      "test: loss=0.052217477121416334 acc=0.9753623188405797\n",
      "EPOCH=1818\n",
      "train: loss=0.05683970341232783 acc=0.9768115942028985\n",
      "test: loss=0.053212736610668325 acc=0.972463768115942\n",
      "EPOCH=1819\n",
      "train: loss=0.025655112704839955 acc=0.9884057971014493\n",
      "test: loss=0.050036215565828226 acc=0.9753623188405797\n",
      "EPOCH=1820\n",
      "train: loss=0.05141930614009565 acc=0.9826086956521739\n",
      "test: loss=0.05319518136412291 acc=0.9739130434782609\n",
      "EPOCH=1821\n",
      "train: loss=0.04490473800278345 acc=0.9797101449275363\n",
      "test: loss=0.053984202594778805 acc=0.9753623188405797\n",
      "EPOCH=1822\n",
      "train: loss=0.04745256846643785 acc=0.9753623188405797\n",
      "test: loss=0.05203878739703289 acc=0.9739130434782609\n",
      "EPOCH=1823\n",
      "train: loss=0.033934124206715786 acc=0.9797101449275363\n",
      "test: loss=0.055268027558388534 acc=0.972463768115942\n",
      "EPOCH=1824\n",
      "train: loss=0.06560977128757627 acc=0.9623188405797102\n",
      "test: loss=0.05616214564349547 acc=0.9710144927536232\n",
      "EPOCH=1825\n",
      "train: loss=0.056719484248738586 acc=0.9739130434782609\n",
      "test: loss=0.054147397319270235 acc=0.9710144927536232\n",
      "EPOCH=1826\n",
      "train: loss=0.028968143993627515 acc=0.9840579710144928\n",
      "test: loss=0.05459109823396535 acc=0.9695652173913043\n",
      "EPOCH=1827\n",
      "train: loss=0.0712185294386988 acc=0.9579710144927536\n",
      "test: loss=0.054015994155382364 acc=0.9710144927536232\n",
      "EPOCH=1828\n",
      "train: loss=0.03809500184559265 acc=0.9797101449275363\n",
      "test: loss=0.05384169578284774 acc=0.9710144927536232\n",
      "EPOCH=1829\n",
      "train: loss=0.040509879950932214 acc=0.9768115942028985\n",
      "test: loss=0.05384676871821821 acc=0.972463768115942\n",
      "EPOCH=1830\n",
      "train: loss=0.05126617376293427 acc=0.972463768115942\n",
      "test: loss=0.05453072094048029 acc=0.9739130434782609\n",
      "EPOCH=1831\n",
      "train: loss=0.04002859511952861 acc=0.9797101449275363\n",
      "test: loss=0.053535407174370164 acc=0.972463768115942\n",
      "EPOCH=1832\n",
      "train: loss=0.05824146039966938 acc=0.9666666666666667\n",
      "test: loss=0.05597454292092688 acc=0.9710144927536232\n",
      "EPOCH=1833\n",
      "train: loss=0.033073500692287275 acc=0.9768115942028985\n",
      "test: loss=0.05037852806151955 acc=0.9739130434782609\n",
      "EPOCH=1834\n",
      "train: loss=0.06541659688424992 acc=0.9710144927536232\n",
      "test: loss=0.05448291477404649 acc=0.9710144927536232\n",
      "EPOCH=1835\n",
      "train: loss=0.06557039115230441 acc=0.972463768115942\n",
      "test: loss=0.05261257851302927 acc=0.9695652173913043\n",
      "EPOCH=1836\n",
      "train: loss=0.04364213332925332 acc=0.9768115942028985\n",
      "test: loss=0.053931146294227295 acc=0.9739130434782609\n",
      "EPOCH=1837\n",
      "train: loss=0.05536199134615467 acc=0.9739130434782609\n",
      "test: loss=0.05331775856689581 acc=0.9753623188405797\n",
      "EPOCH=1838\n",
      "train: loss=0.06318697088904626 acc=0.972463768115942\n",
      "test: loss=0.058618069363373396 acc=0.9681159420289855\n",
      "EPOCH=1839\n",
      "train: loss=0.03457293655184486 acc=0.9797101449275363\n",
      "test: loss=0.05139775647944325 acc=0.9739130434782609\n",
      "EPOCH=1840\n",
      "train: loss=0.038440071352110666 acc=0.9826086956521739\n",
      "test: loss=0.052080009540474294 acc=0.9739130434782609\n",
      "EPOCH=1841\n",
      "train: loss=0.050971368074476574 acc=0.9695652173913043\n",
      "test: loss=0.051514792761443384 acc=0.9782608695652174\n",
      "EPOCH=1842\n",
      "train: loss=0.06964111765152411 acc=0.9637681159420289\n",
      "test: loss=0.053147789711376854 acc=0.9695652173913043\n",
      "EPOCH=1843\n",
      "train: loss=0.025763079526869286 acc=0.9869565217391304\n",
      "test: loss=0.05496322498774311 acc=0.9753623188405797\n",
      "EPOCH=1844\n",
      "train: loss=0.029062275840058955 acc=0.9855072463768116\n",
      "test: loss=0.05355902707788841 acc=0.9739130434782609\n",
      "EPOCH=1845\n",
      "train: loss=0.050497263543001415 acc=0.9797101449275363\n",
      "test: loss=0.05419188563151721 acc=0.9681159420289855\n",
      "EPOCH=1846\n",
      "train: loss=0.07258624317036377 acc=0.9637681159420289\n",
      "test: loss=0.052910166966180436 acc=0.9739130434782609\n",
      "EPOCH=1847\n",
      "train: loss=0.02426933308644932 acc=0.9855072463768116\n",
      "test: loss=0.056382150403346626 acc=0.9710144927536232\n",
      "EPOCH=1848\n",
      "train: loss=0.05407462343119316 acc=0.9710144927536232\n",
      "test: loss=0.05019081786933833 acc=0.9768115942028985\n",
      "EPOCH=1849\n",
      "train: loss=0.05012538152068264 acc=0.9695652173913043\n",
      "test: loss=0.0534502989070454 acc=0.9710144927536232\n",
      "EPOCH=1850\n",
      "train: loss=0.040184993368731284 acc=0.9782608695652174\n",
      "test: loss=0.05359878846613895 acc=0.9710144927536232\n",
      "EPOCH=1851\n",
      "train: loss=0.0546881438736516 acc=0.9739130434782609\n",
      "test: loss=0.053854734684851 acc=0.9710144927536232\n",
      "EPOCH=1852\n",
      "train: loss=0.04560967374191584 acc=0.9826086956521739\n",
      "test: loss=0.05204928041141237 acc=0.9681159420289855\n",
      "EPOCH=1853\n",
      "train: loss=0.06388575357480682 acc=0.9681159420289855\n",
      "test: loss=0.053769682081776043 acc=0.972463768115942\n",
      "EPOCH=1854\n",
      "train: loss=0.041491603977280495 acc=0.981159420289855\n",
      "test: loss=0.05257244567068606 acc=0.9710144927536232\n",
      "EPOCH=1855\n",
      "train: loss=0.0737183774806719 acc=0.9637681159420289\n",
      "test: loss=0.06159209928198323 acc=0.972463768115942\n",
      "EPOCH=1856\n",
      "train: loss=0.05357239254791701 acc=0.9695652173913043\n",
      "test: loss=0.05482089956543279 acc=0.972463768115942\n",
      "EPOCH=1857\n",
      "train: loss=0.06678092882799838 acc=0.9681159420289855\n",
      "test: loss=0.054602819748726446 acc=0.9739130434782609\n",
      "EPOCH=1858\n",
      "train: loss=0.036449206072301094 acc=0.9855072463768116\n",
      "test: loss=0.05126843428216834 acc=0.972463768115942\n",
      "EPOCH=1859\n",
      "train: loss=0.04602252199417403 acc=0.9782608695652174\n",
      "test: loss=0.04935511662001884 acc=0.972463768115942\n",
      "EPOCH=1860\n",
      "train: loss=0.047307163097390394 acc=0.9753623188405797\n",
      "test: loss=0.05229082313645524 acc=0.972463768115942\n",
      "EPOCH=1861\n",
      "train: loss=0.055056516777613806 acc=0.9710144927536232\n",
      "test: loss=0.05308681469491956 acc=0.9710144927536232\n",
      "EPOCH=1862\n",
      "train: loss=0.03942891909437474 acc=0.981159420289855\n",
      "test: loss=0.05389077467647148 acc=0.9753623188405797\n",
      "EPOCH=1863\n",
      "train: loss=0.05970038121217247 acc=0.9681159420289855\n",
      "test: loss=0.05055606135152293 acc=0.972463768115942\n",
      "EPOCH=1864\n",
      "train: loss=0.052878846934553615 acc=0.9753623188405797\n",
      "test: loss=0.053590418466573135 acc=0.9739130434782609\n",
      "EPOCH=1865\n",
      "train: loss=0.083964897826022 acc=0.9550724637681159\n",
      "test: loss=0.05211059545241908 acc=0.972463768115942\n",
      "EPOCH=1866\n",
      "train: loss=0.05690738801957779 acc=0.9681159420289855\n",
      "test: loss=0.053729996663525105 acc=0.9753623188405797\n",
      "EPOCH=1867\n",
      "train: loss=0.03950831107665065 acc=0.9768115942028985\n",
      "test: loss=0.05213377066637622 acc=0.9753623188405797\n",
      "EPOCH=1868\n",
      "train: loss=0.04281375965174105 acc=0.9826086956521739\n",
      "test: loss=0.050319801602026336 acc=0.9710144927536232\n",
      "EPOCH=1869\n",
      "train: loss=0.04151373721283081 acc=0.981159420289855\n",
      "test: loss=0.054246192467138056 acc=0.9739130434782609\n",
      "EPOCH=1870\n",
      "train: loss=0.06793187746365158 acc=0.9695652173913043\n",
      "test: loss=0.05148279771944682 acc=0.9681159420289855\n",
      "EPOCH=1871\n",
      "train: loss=0.06208098617456998 acc=0.9710144927536232\n",
      "test: loss=0.056863741465359614 acc=0.9768115942028985\n",
      "EPOCH=1872\n",
      "train: loss=0.08552694997152789 acc=0.9565217391304348\n",
      "test: loss=0.05436041594454355 acc=0.9739130434782609\n",
      "EPOCH=1873\n",
      "train: loss=0.06486337184236414 acc=0.9681159420289855\n",
      "test: loss=0.053966329590115075 acc=0.972463768115942\n",
      "EPOCH=1874\n",
      "train: loss=0.06418266446818431 acc=0.9681159420289855\n",
      "test: loss=0.05682369023172776 acc=0.9681159420289855\n",
      "EPOCH=1875\n",
      "train: loss=0.03692484998209148 acc=0.9826086956521739\n",
      "test: loss=0.05132553345362458 acc=0.9695652173913043\n",
      "EPOCH=1876\n",
      "train: loss=0.045272295433125206 acc=0.9768115942028985\n",
      "test: loss=0.05304820237822426 acc=0.9768115942028985\n",
      "EPOCH=1877\n",
      "train: loss=0.04771465281811243 acc=0.9782608695652174\n",
      "test: loss=0.05240502348360254 acc=0.9695652173913043\n",
      "EPOCH=1878\n",
      "train: loss=0.035343949153753995 acc=0.9797101449275363\n",
      "test: loss=0.05085050359541755 acc=0.9695652173913043\n",
      "EPOCH=1879\n",
      "train: loss=0.04291910247462713 acc=0.9826086956521739\n",
      "test: loss=0.05118773466390959 acc=0.9739130434782609\n",
      "EPOCH=1880\n",
      "train: loss=0.02652054734544763 acc=0.9869565217391304\n",
      "test: loss=0.0499959765725903 acc=0.9710144927536232\n",
      "EPOCH=1881\n",
      "train: loss=0.03789927078474807 acc=0.981159420289855\n",
      "test: loss=0.04882520993595035 acc=0.9681159420289855\n",
      "EPOCH=1882\n",
      "train: loss=0.06139055371597599 acc=0.9623188405797102\n",
      "test: loss=0.04986291540015299 acc=0.9753623188405797\n",
      "EPOCH=1883\n",
      "train: loss=0.03999334261867841 acc=0.9826086956521739\n",
      "test: loss=0.0541766173665302 acc=0.9782608695652174\n",
      "EPOCH=1884\n",
      "train: loss=0.029163308637318204 acc=0.9840579710144928\n",
      "test: loss=0.05443493689755091 acc=0.9710144927536232\n",
      "EPOCH=1885\n",
      "train: loss=0.050106165254389726 acc=0.9695652173913043\n",
      "test: loss=0.055805534753407625 acc=0.9768115942028985\n",
      "EPOCH=1886\n",
      "train: loss=0.04110848000190695 acc=0.981159420289855\n",
      "test: loss=0.05633508880956924 acc=0.9739130434782609\n",
      "EPOCH=1887\n",
      "train: loss=0.05017253735346429 acc=0.9739130434782609\n",
      "test: loss=0.05089323532577462 acc=0.9695652173913043\n",
      "EPOCH=1888\n",
      "train: loss=0.045634823215407995 acc=0.972463768115942\n",
      "test: loss=0.05287442213165669 acc=0.9710144927536232\n",
      "EPOCH=1889\n",
      "train: loss=0.05297888068741572 acc=0.972463768115942\n",
      "test: loss=0.05428746290354852 acc=0.9782608695652174\n",
      "EPOCH=1890\n",
      "train: loss=0.02956305240941794 acc=0.9869565217391304\n",
      "test: loss=0.052279798990653034 acc=0.9782608695652174\n",
      "EPOCH=1891\n",
      "train: loss=0.033378200814198 acc=0.981159420289855\n",
      "test: loss=0.05099276084485896 acc=0.9739130434782609\n",
      "EPOCH=1892\n",
      "train: loss=0.03160286668483734 acc=0.9855072463768116\n",
      "test: loss=0.051540944864118816 acc=0.9782608695652174\n",
      "EPOCH=1893\n",
      "train: loss=0.036882928032770664 acc=0.9768115942028985\n",
      "test: loss=0.05260629545761785 acc=0.9753623188405797\n",
      "EPOCH=1894\n",
      "train: loss=0.03486407969070803 acc=0.9797101449275363\n",
      "test: loss=0.05385469571297071 acc=0.9739130434782609\n",
      "EPOCH=1895\n",
      "train: loss=0.05497077541101318 acc=0.9666666666666667\n",
      "test: loss=0.05230029870703741 acc=0.972463768115942\n",
      "EPOCH=1896\n",
      "train: loss=0.05469660226976779 acc=0.972463768115942\n",
      "test: loss=0.052012647208874704 acc=0.972463768115942\n",
      "EPOCH=1897\n",
      "train: loss=0.029531755527456124 acc=0.9840579710144928\n",
      "test: loss=0.054231099194992 acc=0.9710144927536232\n",
      "EPOCH=1898\n",
      "train: loss=0.04567868932875206 acc=0.9753623188405797\n",
      "test: loss=0.05103647275676137 acc=0.9753623188405797\n",
      "EPOCH=1899\n",
      "train: loss=0.03766775058943026 acc=0.9826086956521739\n",
      "test: loss=0.04908909254099552 acc=0.9710144927536232\n",
      "EPOCH=1900\n",
      "train: loss=0.020370193322585096 acc=0.9884057971014493\n",
      "test: loss=0.05329210370109173 acc=0.9739130434782609\n",
      "EPOCH=1901\n",
      "train: loss=0.02039956031334094 acc=0.991304347826087\n",
      "test: loss=0.05232550374608667 acc=0.9710144927536232\n",
      "EPOCH=1902\n",
      "train: loss=0.05075307542910621 acc=0.9753623188405797\n",
      "test: loss=0.05129947806258705 acc=0.972463768115942\n",
      "EPOCH=1903\n",
      "train: loss=0.05059841166080209 acc=0.9695652173913043\n",
      "test: loss=0.05461040490002429 acc=0.9768115942028985\n",
      "EPOCH=1904\n",
      "train: loss=0.056105061716323124 acc=0.9710144927536232\n",
      "test: loss=0.05062774712641699 acc=0.9753623188405797\n",
      "EPOCH=1905\n",
      "train: loss=0.0386286534446553 acc=0.9768115942028985\n",
      "test: loss=0.05039971065604812 acc=0.9768115942028985\n",
      "EPOCH=1906\n",
      "train: loss=0.03583487145866962 acc=0.981159420289855\n",
      "test: loss=0.0576121281708644 acc=0.972463768115942\n",
      "EPOCH=1907\n",
      "train: loss=0.047277199998520765 acc=0.9753623188405797\n",
      "test: loss=0.05392043300682941 acc=0.9739130434782609\n",
      "EPOCH=1908\n",
      "train: loss=0.06974489082782827 acc=0.9637681159420289\n",
      "test: loss=0.054266110298111056 acc=0.9695652173913043\n",
      "EPOCH=1909\n",
      "train: loss=0.03457767841199666 acc=0.9869565217391304\n",
      "test: loss=0.05838038599316713 acc=0.9739130434782609\n",
      "EPOCH=1910\n",
      "train: loss=0.07073657508984141 acc=0.9652173913043478\n",
      "test: loss=0.053469052272595935 acc=0.9739130434782609\n",
      "EPOCH=1911\n",
      "train: loss=0.04339208113199155 acc=0.9797101449275363\n",
      "test: loss=0.052035299427841346 acc=0.9768115942028985\n",
      "EPOCH=1912\n",
      "train: loss=0.04293854472867158 acc=0.9797101449275363\n",
      "test: loss=0.053850883244378284 acc=0.972463768115942\n",
      "EPOCH=1913\n",
      "train: loss=0.031389825845023756 acc=0.9855072463768116\n",
      "test: loss=0.0530891201092511 acc=0.9753623188405797\n",
      "EPOCH=1914\n",
      "train: loss=0.0473057701407331 acc=0.9739130434782609\n",
      "test: loss=0.05226302971575602 acc=0.9739130434782609\n",
      "EPOCH=1915\n",
      "train: loss=0.036392676952303936 acc=0.9797101449275363\n",
      "test: loss=0.053492182334339396 acc=0.9710144927536232\n",
      "EPOCH=1916\n",
      "train: loss=0.03371415685996899 acc=0.9884057971014493\n",
      "test: loss=0.05185650140611823 acc=0.9710144927536232\n",
      "EPOCH=1917\n",
      "train: loss=0.0559974242777014 acc=0.9710144927536232\n",
      "test: loss=0.050688935825422936 acc=0.9768115942028985\n",
      "EPOCH=1918\n",
      "train: loss=0.0827186779854015 acc=0.9608695652173913\n",
      "test: loss=0.05147928946212139 acc=0.9681159420289855\n",
      "EPOCH=1919\n",
      "train: loss=0.06692566862684564 acc=0.9652173913043478\n",
      "test: loss=0.0488897141036413 acc=0.9753623188405797\n",
      "EPOCH=1920\n",
      "train: loss=0.0412845259999234 acc=0.9782608695652174\n",
      "test: loss=0.04976157821679586 acc=0.9695652173913043\n",
      "EPOCH=1921\n",
      "train: loss=0.02450864834031421 acc=0.9855072463768116\n",
      "test: loss=0.052572323478689 acc=0.9753623188405797\n",
      "EPOCH=1922\n",
      "train: loss=0.05167967982434493 acc=0.9739130434782609\n",
      "test: loss=0.05182449106380396 acc=0.972463768115942\n",
      "EPOCH=1923\n",
      "train: loss=0.037102566432828714 acc=0.9840579710144928\n",
      "test: loss=0.05235059201336631 acc=0.9739130434782609\n",
      "EPOCH=1924\n",
      "train: loss=0.050769638029325485 acc=0.972463768115942\n",
      "test: loss=0.05265900490220868 acc=0.9797101449275363\n",
      "EPOCH=1925\n",
      "train: loss=0.03501626661781581 acc=0.9797101449275363\n",
      "test: loss=0.05207452843178851 acc=0.972463768115942\n",
      "EPOCH=1926\n",
      "train: loss=0.04994920309195799 acc=0.9753623188405797\n",
      "test: loss=0.05373901369624445 acc=0.972463768115942\n",
      "EPOCH=1927\n",
      "train: loss=0.037567484121844824 acc=0.9768115942028985\n",
      "test: loss=0.04959189165835456 acc=0.9739130434782609\n",
      "EPOCH=1928\n",
      "train: loss=0.037839498951687006 acc=0.981159420289855\n",
      "test: loss=0.05152017471743533 acc=0.9739130434782609\n",
      "EPOCH=1929\n",
      "train: loss=0.060408478061638876 acc=0.9710144927536232\n",
      "test: loss=0.05183010532086035 acc=0.9753623188405797\n",
      "EPOCH=1930\n",
      "train: loss=0.06490977817090268 acc=0.9608695652173913\n",
      "test: loss=0.049980972962252596 acc=0.9768115942028985\n",
      "EPOCH=1931\n",
      "train: loss=0.06269114564261127 acc=0.9623188405797102\n",
      "test: loss=0.049350589501707495 acc=0.972463768115942\n",
      "EPOCH=1932\n",
      "train: loss=0.04920673066864684 acc=0.9739130434782609\n",
      "test: loss=0.057009946712252506 acc=0.9739130434782609\n",
      "EPOCH=1933\n",
      "train: loss=0.052249083932100635 acc=0.9782608695652174\n",
      "test: loss=0.050606767977001484 acc=0.9768115942028985\n",
      "EPOCH=1934\n",
      "train: loss=0.056790183117661285 acc=0.9739130434782609\n",
      "test: loss=0.04963705129275059 acc=0.9768115942028985\n",
      "EPOCH=1935\n",
      "train: loss=0.04167020622244387 acc=0.9739130434782609\n",
      "test: loss=0.0520262459385173 acc=0.972463768115942\n",
      "EPOCH=1936\n",
      "train: loss=0.04549315353333528 acc=0.9797101449275363\n",
      "test: loss=0.05063636656361825 acc=0.9739130434782609\n",
      "EPOCH=1937\n",
      "train: loss=0.03177007598117382 acc=0.981159420289855\n",
      "test: loss=0.05156191468321034 acc=0.9739130434782609\n",
      "EPOCH=1938\n",
      "train: loss=0.036997523498332935 acc=0.9797101449275363\n",
      "test: loss=0.05377114810326731 acc=0.9753623188405797\n",
      "EPOCH=1939\n",
      "train: loss=0.05128000844523345 acc=0.972463768115942\n",
      "test: loss=0.04887404157302749 acc=0.9753623188405797\n",
      "EPOCH=1940\n",
      "train: loss=0.048147572887271436 acc=0.9753623188405797\n",
      "test: loss=0.05303290967790998 acc=0.972463768115942\n",
      "EPOCH=1941\n",
      "train: loss=0.050726457301796105 acc=0.9695652173913043\n",
      "test: loss=0.05272293686361985 acc=0.972463768115942\n",
      "EPOCH=1942\n",
      "train: loss=0.028045094435947403 acc=0.9840579710144928\n",
      "test: loss=0.05019398223108912 acc=0.9753623188405797\n",
      "EPOCH=1943\n",
      "train: loss=0.03559599188139192 acc=0.9768115942028985\n",
      "test: loss=0.05305334041908381 acc=0.9753623188405797\n",
      "EPOCH=1944\n",
      "train: loss=0.030132601447946078 acc=0.9884057971014493\n",
      "test: loss=0.0499928688074365 acc=0.972463768115942\n",
      "EPOCH=1945\n",
      "train: loss=0.04828821582540014 acc=0.9739130434782609\n",
      "test: loss=0.051540040464677195 acc=0.9753623188405797\n",
      "EPOCH=1946\n",
      "train: loss=0.04535916179717622 acc=0.9768115942028985\n",
      "test: loss=0.051658082684664555 acc=0.9710144927536232\n",
      "EPOCH=1947\n",
      "train: loss=0.028850884740094052 acc=0.9884057971014493\n",
      "test: loss=0.05148782436504688 acc=0.972463768115942\n",
      "EPOCH=1948\n",
      "train: loss=0.06250224803936487 acc=0.9666666666666667\n",
      "test: loss=0.05394169404982609 acc=0.9768115942028985\n",
      "EPOCH=1949\n",
      "train: loss=0.08240222207829005 acc=0.9536231884057971\n",
      "test: loss=0.05176864643025223 acc=0.9739130434782609\n",
      "EPOCH=1950\n",
      "train: loss=0.042495850264743076 acc=0.9797101449275363\n",
      "test: loss=0.05101908656231213 acc=0.9695652173913043\n",
      "EPOCH=1951\n",
      "train: loss=0.04274951966495317 acc=0.9753623188405797\n",
      "test: loss=0.05100377124426377 acc=0.9695652173913043\n",
      "EPOCH=1952\n",
      "train: loss=0.0558188171758126 acc=0.972463768115942\n",
      "test: loss=0.0502558955768188 acc=0.9753623188405797\n",
      "EPOCH=1953\n",
      "train: loss=0.0514646910756317 acc=0.9710144927536232\n",
      "test: loss=0.053331053967111505 acc=0.9753623188405797\n",
      "EPOCH=1954\n",
      "train: loss=0.07202415893923027 acc=0.9652173913043478\n",
      "test: loss=0.052551100558807885 acc=0.972463768115942\n",
      "EPOCH=1955\n",
      "train: loss=0.07831548820336451 acc=0.9623188405797102\n",
      "test: loss=0.04938413525008181 acc=0.972463768115942\n",
      "EPOCH=1956\n",
      "train: loss=0.03848339909645753 acc=0.9826086956521739\n",
      "test: loss=0.05079306247137311 acc=0.9739130434782609\n",
      "EPOCH=1957\n",
      "train: loss=0.05112199329760297 acc=0.972463768115942\n",
      "test: loss=0.05092026079256126 acc=0.972463768115942\n",
      "EPOCH=1958\n",
      "train: loss=0.03194680313605285 acc=0.9855072463768116\n",
      "test: loss=0.051008346511682075 acc=0.9782608695652174\n",
      "EPOCH=1959\n",
      "train: loss=0.04115905488413821 acc=0.9797101449275363\n",
      "test: loss=0.04884891082459666 acc=0.9753623188405797\n",
      "EPOCH=1960\n",
      "train: loss=0.05838817678066394 acc=0.9681159420289855\n",
      "test: loss=0.05343438865500661 acc=0.972463768115942\n",
      "EPOCH=1961\n",
      "train: loss=0.040709127465538224 acc=0.9826086956521739\n",
      "test: loss=0.051367986501440546 acc=0.972463768115942\n",
      "EPOCH=1962\n",
      "train: loss=0.04873300434629932 acc=0.9782608695652174\n",
      "test: loss=0.05084115294685093 acc=0.9753623188405797\n",
      "EPOCH=1963\n",
      "train: loss=0.03945717284361977 acc=0.9782608695652174\n",
      "test: loss=0.050593654624580466 acc=0.9768115942028985\n",
      "EPOCH=1964\n",
      "train: loss=0.039957146646020944 acc=0.9826086956521739\n",
      "test: loss=0.05184167425079388 acc=0.9695652173913043\n",
      "EPOCH=1965\n",
      "train: loss=0.04964374118526157 acc=0.9710144927536232\n",
      "test: loss=0.05353135042078011 acc=0.972463768115942\n",
      "EPOCH=1966\n",
      "train: loss=0.03697396932352903 acc=0.9855072463768116\n",
      "test: loss=0.05605247903277461 acc=0.972463768115942\n",
      "EPOCH=1967\n",
      "train: loss=0.0559794096373078 acc=0.972463768115942\n",
      "test: loss=0.05421218485801292 acc=0.9710144927536232\n",
      "EPOCH=1968\n",
      "train: loss=0.03450012235107236 acc=0.981159420289855\n",
      "test: loss=0.04934062438725329 acc=0.9739130434782609\n",
      "EPOCH=1969\n",
      "train: loss=0.06161682003877402 acc=0.9739130434782609\n",
      "test: loss=0.05278048035509895 acc=0.9753623188405797\n",
      "EPOCH=1970\n",
      "train: loss=0.06004374759422655 acc=0.9637681159420289\n",
      "test: loss=0.05190664537968215 acc=0.9695652173913043\n",
      "EPOCH=1971\n",
      "train: loss=0.028826037081786618 acc=0.9884057971014493\n",
      "test: loss=0.052177847899151544 acc=0.972463768115942\n",
      "EPOCH=1972\n",
      "train: loss=0.038866058566888226 acc=0.9840579710144928\n",
      "test: loss=0.05200159533546513 acc=0.9739130434782609\n",
      "EPOCH=1973\n",
      "train: loss=0.048894084957111526 acc=0.9768115942028985\n",
      "test: loss=0.051662284779961945 acc=0.9768115942028985\n",
      "EPOCH=1974\n",
      "train: loss=0.064557002498889 acc=0.9637681159420289\n",
      "test: loss=0.049988207942021655 acc=0.9710144927536232\n",
      "EPOCH=1975\n",
      "train: loss=0.03621367144805011 acc=0.9840579710144928\n",
      "test: loss=0.050369913944921955 acc=0.9768115942028985\n",
      "EPOCH=1976\n",
      "train: loss=0.03406512383709435 acc=0.9826086956521739\n",
      "test: loss=0.049188507461527926 acc=0.9739130434782609\n",
      "EPOCH=1977\n",
      "train: loss=0.04751947609718073 acc=0.972463768115942\n",
      "test: loss=0.05041033128004847 acc=0.9768115942028985\n",
      "EPOCH=1978\n",
      "train: loss=0.05235109867024809 acc=0.9710144927536232\n",
      "test: loss=0.05336837597693044 acc=0.9739130434782609\n",
      "EPOCH=1979\n",
      "train: loss=0.05655821474385239 acc=0.9710144927536232\n",
      "test: loss=0.05167926616677395 acc=0.9753623188405797\n",
      "EPOCH=1980\n",
      "train: loss=0.06326712785348254 acc=0.9652173913043478\n",
      "test: loss=0.05193211851016308 acc=0.9753623188405797\n",
      "EPOCH=1981\n",
      "train: loss=0.06616481715940654 acc=0.9637681159420289\n",
      "test: loss=0.054471255280692264 acc=0.972463768115942\n",
      "EPOCH=1982\n",
      "train: loss=0.04884656526757187 acc=0.972463768115942\n",
      "test: loss=0.05210392420365006 acc=0.9681159420289855\n",
      "EPOCH=1983\n",
      "train: loss=0.039397980472013934 acc=0.9797101449275363\n",
      "test: loss=0.050233450807086306 acc=0.9695652173913043\n",
      "EPOCH=1984\n",
      "train: loss=0.033297059821899434 acc=0.9869565217391304\n",
      "test: loss=0.05211423171536578 acc=0.9753623188405797\n",
      "EPOCH=1985\n",
      "train: loss=0.07187560485474165 acc=0.9666666666666667\n",
      "test: loss=0.05015853747543583 acc=0.9753623188405797\n",
      "EPOCH=1986\n",
      "train: loss=0.0406209900917218 acc=0.9782608695652174\n",
      "test: loss=0.05080946734812145 acc=0.9739130434782609\n",
      "EPOCH=1987\n",
      "train: loss=0.05228294923121533 acc=0.9695652173913043\n",
      "test: loss=0.0510656989637469 acc=0.9753623188405797\n",
      "EPOCH=1988\n",
      "train: loss=0.030809818036312824 acc=0.9855072463768116\n",
      "test: loss=0.0512245097876815 acc=0.9710144927536232\n",
      "EPOCH=1989\n",
      "train: loss=0.050101235348847396 acc=0.9695652173913043\n",
      "test: loss=0.05016171767175014 acc=0.9710144927536232\n",
      "EPOCH=1990\n",
      "train: loss=0.042327455144025805 acc=0.9782608695652174\n",
      "test: loss=0.04993626079039783 acc=0.9695652173913043\n",
      "EPOCH=1991\n",
      "train: loss=0.052840506432762335 acc=0.972463768115942\n",
      "test: loss=0.0513179553408627 acc=0.9782608695652174\n",
      "EPOCH=1992\n",
      "train: loss=0.05416092332155006 acc=0.9695652173913043\n",
      "test: loss=0.05097795620388642 acc=0.9753623188405797\n",
      "EPOCH=1993\n",
      "train: loss=0.04693251366639169 acc=0.9768115942028985\n",
      "test: loss=0.05063406857036821 acc=0.9753623188405797\n",
      "EPOCH=1994\n",
      "train: loss=0.037451312179224316 acc=0.9826086956521739\n",
      "test: loss=0.05308588954399379 acc=0.9782608695652174\n",
      "EPOCH=1995\n",
      "train: loss=0.038736864275198506 acc=0.9797101449275363\n",
      "test: loss=0.051956944040760046 acc=0.9739130434782609\n",
      "EPOCH=1996\n",
      "train: loss=0.050144773720234175 acc=0.9710144927536232\n",
      "test: loss=0.04886575165347207 acc=0.9739130434782609\n",
      "EPOCH=1997\n",
      "train: loss=0.04362202361345417 acc=0.9797101449275363\n",
      "test: loss=0.04981245170896972 acc=0.9753623188405797\n",
      "EPOCH=1998\n",
      "train: loss=0.04038878441685418 acc=0.9782608695652174\n",
      "test: loss=0.05003503827663928 acc=0.9753623188405797\n",
      "EPOCH=1999\n",
      "train: loss=0.04411130637372087 acc=0.9797101449275363\n",
      "test: loss=0.049990685880699735 acc=0.972463768115942\n",
      "EPOCH=2000\n",
      "train: loss=0.03852561101639098 acc=0.9855072463768116\n",
      "test: loss=0.05084116382297688 acc=0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "metrics_names = ['loss_train','loss_test','acc_train','acc_test']\n",
    "losses = []\n",
    "\n",
    "net.to(DEVICE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'EPOCH={epoch + 1}')\n",
    "    for X, y in train_dl:\n",
    "        X = X.float().to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = net(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "    train_ll, train_acc = evaluate_model(net, train_dl)\n",
    "    test_ll, test_acc = evaluate_model(net, test_dl)\n",
    "    \n",
    "    \n",
    "    print(f'train: loss={train_ll} acc={train_acc}')\n",
    "    print(f'test: loss={test_ll} acc={test_acc}')\n",
    "          \n",
    "    metrics.append([train_ll, test_ll, train_acc, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.DataFrame(metrics, columns=['train-loss','test-loss','train-acc','test-acc']).plot(subplots=False, figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './model/sequence_lstm_classifier.pt')\n",
    "\n",
    "# m_state_dict = torch.load('mymodule.pt')\n",
    "# new_m = MyModule()\n",
    "# new_m.load_state_dict(m_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr-dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
