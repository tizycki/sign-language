{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.images_from_video import video_to_images\n",
    "\n",
    "from utils.keypoints import read_keypoints, read_rescale_keypoints_list, rescale_keypoints_sequence, rescale_keypoints\n",
    "from model.transforms import KeypointsSequencePadding\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from model.models import StopPoseNet, SequenceRecognitionNet, SequenceRecognitionNetLSTM\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video params\n",
    "INPUT_VIDEO_PATH = './data/video_raw/internal_resource_2333.MOV'\n",
    "VIDEO_ROTATE = True\n",
    "VIDEO_OUTPUT_FPS = 10\n",
    "VIDEO_START_FRAME = 40\n",
    "VIDEO_END_FRAME = 520\n",
    "\n",
    "# Working directory params\n",
    "OUTPUT_FOLDER = './data/video_translation/'\n",
    "TEMP_BASE_FOLDER_NAME = 'tmp'\n",
    "TEMP_FRAMES_FOLDER_NAME = 'frames'\n",
    "TEMP_KEYPOINTS_FOLDER_NAME = 'keypoints'\n",
    "TEMP_VIDEO_RAW_FOLDER_NAME = 'sequences'\n",
    "TEMP_VIDEO_RENDERED_FOLDER_NAME = 'video_rendered'\n",
    "OUTPUT_BASE_FOLDER_NAME = 'output'\n",
    "\n",
    "# Model paths\n",
    "STOP_POSE_NET_PATH = './model/stop_pose_detector.pt'\n",
    "SEQUENCE_NET_PATH = './model/sequence_classifier.pt'\n",
    "\n",
    "# Model params\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# Transformer params\n",
    "SEQUENCE_LENGTH_MAX = 50\n",
    "\n",
    "# Pytorch params\n",
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing content of directory:./data/video_translation/tmp/internal_resource_2343/frames\n",
      "Removing content of directory:./data/video_translation/tmp/internal_resource_2343/keypoints\n",
      "Removing content of directory:./data/video_translation/tmp/internal_resource_2343/sequences\n",
      "Removing content of directory:./data/video_translation/tmp/internal_resource_2343/video_rendered\n"
     ]
    }
   ],
   "source": [
    "# Prepare directories\n",
    "video_file_name = os.path.basename(INPUT_VIDEO_PATH).split('.')[0]\n",
    "\n",
    "for dir_path in [TEMP_FRAMES_FOLDER_NAME, TEMP_KEYPOINTS_FOLDER_NAME, TEMP_VIDEO_RAW_FOLDER_NAME, TEMP_VIDEO_RENDERED_FOLDER_NAME]:\n",
    "    dir_path = os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, dir_path)\n",
    "    \n",
    "    if not os.path.exists(dir_path):\n",
    "        print(f'Creating directory: {dir_path}')\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        print(f'Removing content of directory:{dir_path}')\n",
    "        dir_content = glob.glob(f'{dir_path}/*')\n",
    "        for file in dir_content:\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. It took 112.56163263320923 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Split video into images\n",
    "start_time = time.time()\n",
    "video_to_images(\n",
    "    video_path=INPUT_VIDEO_PATH,\n",
    "    output_dir=os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, TEMP_FRAMES_FOLDER_NAME),\n",
    "    file_name_prefix=video_file_name,\n",
    "    output_fps=VIDEO_OUTPUT_FPS,\n",
    "    rotate=VIDEO_ROTATE,\n",
    "    create_subdir=False,\n",
    "    start_frame=VIDEO_START_FRAME,\n",
    "    end_frame=VIDEO_END_FRAME\n",
    ")\n",
    "clear_output()\n",
    "print(f'Done. It took {time.time() - start_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker run -it --rm --mount type=bind,source=/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343,target=/sign-language/data --mount type=bind,source=/home/tom/Desktop/projects/sign-language/utils,target=/sign-language/utils --net=host -e DISPLAY --runtime=nvidia openpose-custom\n",
      "\n",
      "python3 ./utils/openpose_wrapper.py --image_dir ./data/frames --write_json ./data/keypoints --write_images ./data/rendered_images\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press enter when you finished with docker execution \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform images into keypoints\n",
    "\n",
    "print(\n",
    "    f\"\"\"docker run -it --rm --mount type=bind,source={os.path.realpath(os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name))},target=/sign-language/data --mount type=bind,source={os.path.realpath(os.path.join('.', 'utils'))},target=/sign-language/utils --net=host -e DISPLAY --runtime=nvidia openpose-custom\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\\npython3 ./utils/openpose_wrapper.py --image_dir ./data/frames --write_json ./data/keypoints --write_images ./data/rendered_images\"\"\"\n",
    ")\n",
    "\n",
    "input('\\nPress enter when you finished with docker execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 521\n"
     ]
    }
   ],
   "source": [
    "# Load list of keypoints\n",
    "keypoints_dir_path = os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, TEMP_KEYPOINTS_FOLDER_NAME)\n",
    "keypoints_paths = glob.glob(f'{keypoints_dir_path}/*.json')\n",
    "keypoints_paths.sort()\n",
    "\n",
    "print(f'Number of frames: {len(keypoints_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521/521 [00:00<00:00, 10779.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load and rescale keypoints\n",
    "keypoints_all = read_rescale_keypoints_list(keypoints_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (521,)\n",
      "Number of sequences: 1\n",
      "\n",
      "Done. It took 0.02443981170654297 seconds.\n"
     ]
    }
   ],
   "source": [
    "### Get keypoints sequences\n",
    "start_time = time.time()\n",
    "\n",
    "# Load to tensor\n",
    "keypoints_all_tensor = torch.FloatTensor(keypoints_all).to(DEVICE)\n",
    "\n",
    "# Load stop_pose model\n",
    "state_dict = torch.load(STOP_POSE_NET_PATH)\n",
    "stop_pose_net = StopPoseNet().to(DEVICE)\n",
    "stop_pose_net.load_state_dict(state_dict)\n",
    "stop_pose_net.eval()\n",
    "\n",
    "# Make predictions\n",
    "stop_pose_preds = np.zeros((keypoints_all_tensor.shape[0], 2))\n",
    "\n",
    "stop_pose_preds[:] = stop_pose_net(keypoints_all_tensor[:]).sigmoid().detach().cpu().numpy()\n",
    "test = np.copy(stop_pose_preds)\n",
    "stop_pose_preds = np.argmax(stop_pose_preds, axis=1)\n",
    "print(f'Predictions shape: {stop_pose_preds.shape}')\n",
    "\n",
    "# Get sequences\n",
    "def extract_sequences(stop_pose_preds):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    is_one = np.concatenate(([0], np.equal(stop_pose_preds, 0).view(np.int8), [0]))\n",
    "    diff_abs = np.abs(np.diff(is_one))\n",
    "    \n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(diff_abs == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "sequences_idx = extract_sequences(stop_pose_preds)\n",
    "print(f'Number of sequences: {sequences_idx.shape[0]}')\n",
    "\n",
    "print(f'\\nDone. It took {time.time() - start_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99566615e-01, 4.12081921e-04],\n",
       "       [9.96641755e-01, 3.32559855e-03],\n",
       "       [8.91152680e-01, 1.00755297e-01],\n",
       "       ...,\n",
       "       [7.66190529e-01, 2.07118630e-01],\n",
       "       [7.59080708e-01, 2.14096904e-01],\n",
       "       [7.69138038e-01, 2.05261633e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.54s/it]\n"
     ]
    }
   ],
   "source": [
    "### Generate video of raw sequences\n",
    "\n",
    "# Set output video params\n",
    "video_output_dir = os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, 'sequences')\n",
    "fps = 10\n",
    "\n",
    "# Create df with frames\n",
    "df_frames = pd.DataFrame(data=keypoints_paths, columns=['file_path'])\n",
    "df_frames.file_path = df_frames.file_path.apply(lambda x: os.path.realpath(x.replace('_keypoints.json', '.jpg').replace('keypoints', 'frames')))\n",
    "df_frames = df_frames.sort_values('file_path')\n",
    "\n",
    "# Generate video for each sequence\n",
    "for i, sequence_range in tqdm(enumerate(sequences_idx)):\n",
    "    video_name = f'internal_resource_{sequence_range[0]}_{sequence_range[1]}.avi'\n",
    "    video_path = os.path.join(os.path.realpath(video_output_dir), video_name)\n",
    "    \n",
    "    images = df_frames.iloc[sequence_range[0]: sequence_range[1]]['file_path'].to_numpy().tolist()\n",
    "    frame = cv2.imread(images[0])\n",
    "    height, width, layers = frame.shape\n",
    "    \n",
    "    video = cv2.VideoWriter(video_path, 0, fps, (width,height))\n",
    "\n",
    "    for image_path in images:\n",
    "        video.write(cv2.imread(image_path))\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000070.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000071.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000072.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000073.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000074.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000586.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000587.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000588.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000589.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>/home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000590.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>521 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               file_path\n",
       "0    /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000070.jpg\n",
       "1    /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000071.jpg\n",
       "2    /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000072.jpg\n",
       "3    /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000073.jpg\n",
       "4    /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000074.jpg\n",
       "..                                                                                                                                   ...\n",
       "516  /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000586.jpg\n",
       "517  /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000587.jpg\n",
       "518  /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000588.jpg\n",
       "519  /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000589.jpg\n",
       "520  /home/tom/Desktop/projects/sign-language/data/video_translation/tmp/internal_resource_2343/frames/internal_resource_2343_000590.jpg\n",
       "\n",
       "[521 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([521, 57, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create sequences tensor array\n",
    "sequences_tensor_list = []\n",
    "\n",
    "for sequence_range in sequences_idx:\n",
    "    sequence_tensor = keypoints_all_tensor[sequence_range[0]:sequence_range[1]]\n",
    "    sequences_tensor_list.append(sequence_tensor)\n",
    "    print(sequence_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 57, 2])\n"
     ]
    }
   ],
   "source": [
    "### Predict sequences\n",
    "\n",
    "# Load stop_pose model\n",
    "state_dict = torch.load(SEQUENCE_NET_PATH)\n",
    "sequence_net = SequenceRecognitionNet(NUM_CLASSES)\n",
    "sequence_net.load_state_dict(state_dict)\n",
    "sequence_net.eval()\n",
    "\n",
    "# Create sequence padding transformer\n",
    "sequence_padder = KeypointsSequencePadding(SEQUENCE_LENGTH_MAX)\n",
    "\n",
    "# Sequence iteration - padding and prediction\n",
    "y_preds_proba = []\n",
    "y_preds = []\n",
    "for sequence_tensor in sequences_tensor_list:\n",
    "    if sequence_tensor.shape[0] < 5:\n",
    "        continue\n",
    "    \n",
    "    # Perform last-frame padding\n",
    "    X = torch.Tensor(sequence_padder(sequence_tensor)).unsqueeze(0).float().to(DEVICE)\n",
    "    print(X.shape)\n",
    "    y_pred_proba = F.softmax(sequence_net(X), dim=1)\n",
    "    y_pred_proba = y_pred_proba.detach().cpu().numpy()\n",
    "    y_pred = y_pred_proba.argmax(axis=1)[0]\n",
    "    \n",
    "    y_preds_proba.append(y_pred_proba)\n",
    "    y_preds.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence: 1, label=You\n",
      "Meet: 0.00004011\n",
      "Name: 0.05519281\n",
      "Good day: 0.01193496\n",
      "See you around: 0.00001312\n",
      "Thank you: 0.00001720\n",
      "Hello: 0.00151290\n",
      "Bye bye: 0.00000637\n",
      "Tom: 0.02031478\n",
      "Nice: 0.00028716\n",
      "You: 0.91066819\n",
      "My: 0.00001241\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'Meet',\n",
    "    1: 'Name',\n",
    "    2: 'Good day',\n",
    "    3: 'See you around',\n",
    "    4: 'Thank you',\n",
    "    5: 'Hello',\n",
    "    6: 'Bye bye',\n",
    "    7: 'Tom',\n",
    "    8: 'Nice',\n",
    "    9: 'You',\n",
    "    10: 'My'\n",
    "} \n",
    "\n",
    "for row_index, pred_label in enumerate(y_preds):\n",
    "    print(f'\\nSequence: {row_index + 1}, label={label_map[pred_label]}')\n",
    "    for col_index in range(len(label_map)):\n",
    "        print(f'{label_map[col_index]}: {y_preds_proba[row_index][0][col_index]:.08f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "521it [00:34, 15.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Video ensemble based on rendered images with subtitles\n",
    "# Set output video params\n",
    "y_preds = [5, 2, 10, 1, 7, 0, 9, 8, 4, 3, 6]\n",
    "video_output_dir = os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, TEMP_VIDEO_RENDERED_FOLDER_NAME)\n",
    "fps = 10\n",
    "\n",
    "# Load list of rendered images\n",
    "rendered_images_dir_path = os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, 'rendered_images')\n",
    "rendered_images_paths = glob.glob(f'{rendered_images_dir_path}/*.png')\n",
    "rendered_images_paths.sort()\n",
    "\n",
    "df_frames = pd.DataFrame(data=rendered_images_paths, columns=['rendered_image_path'])\n",
    "\n",
    "# Generate video for each sequence\n",
    "video_name = f'{video_file_name}_skeleton_translation.avi'\n",
    "video_path = os.path.join(os.path.realpath(video_output_dir), video_name)\n",
    "\n",
    "images = df_frames['rendered_image_path'].to_numpy().tolist()\n",
    "height, width, layers = 1920, int(1080 * 2), 3 # frame.shape\n",
    "\n",
    "fourcc=cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')\n",
    "video = cv2.VideoWriter(video_path, fourcc, fps, (width,height))\n",
    "\n",
    "# Get unscaled keypoints\n",
    "keypoints_all_unscaled = []\n",
    "for keypoints_path in keypoints_paths:\n",
    "    keypoints_all_unscaled.append(read_keypoints(keypoints_path))\n",
    "keypoints_all_unscaled = np.array(keypoints_all_unscaled)\n",
    "\n",
    "# Get bounding box coordinates and size\n",
    "boxes_min = np.zeros([len(keypoints_all_unscaled), 2])\n",
    "boxes_max = np.zeros([len(keypoints_all_unscaled), 2])\n",
    "\n",
    "for sequence_range in sequences_idx:\n",
    "    if (sequence_range[1] - sequence_range[0]) < 5:\n",
    "        continue\n",
    "    keypoints_unscaled_sequence = keypoints_all_unscaled[sequence_range[0]:sequence_range[1]]\n",
    "    \n",
    "    box_min = np.min(np.min(keypoints_unscaled_sequence, axis=1), axis=0).reshape(1, -1)\n",
    "    box_max = np.max(np.max(keypoints_unscaled_sequence, axis=1), axis=0).reshape(1, -1)\n",
    "    boxes_min[sequence_range[0]: sequence_range[1]] = box_min\n",
    "    boxes_max[sequence_range[0]: sequence_range[1]] = box_max\n",
    "\n",
    "# Bound labels to image\n",
    "image_labels = np.zeros([len(keypoints_all_unscaled), 1])\n",
    "image_labels[:] = np.array([-1])\n",
    "\n",
    "seq_iter = 0\n",
    "for sequence_range in sequences_idx:\n",
    "    if (sequence_range[1] - sequence_range[0]) < 5:\n",
    "        continue\n",
    "    image_labels[sequence_range[0]:sequence_range[1]] = y_preds[seq_iter]\n",
    "    seq_iter += 1\n",
    "    \n",
    "for i, image_path in tqdm(enumerate(images)):\n",
    "    \n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    top = 0\n",
    "    bottom = 0\n",
    "    left = int(1 * image.shape[1])\n",
    "    right = 0\n",
    "    \n",
    "    new_image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)\n",
    "        \n",
    "    # Add bounding box\n",
    "    cv2.rectangle(\n",
    "        new_image,\n",
    "        (int(boxes_min[i][0] + 1080), int(boxes_min[i][1])),\n",
    "        (int(boxes_max[i][0] + 1080), int(boxes_max[i][1])),\n",
    "        (0,255,0),\n",
    "        (5)\n",
    "    )\n",
    "    cv2.rectangle(\n",
    "        new_image,\n",
    "        (0, 900),\n",
    "        (1080, 1300),\n",
    "        (0,0,0),\n",
    "        (-1)\n",
    "    )\n",
    "\n",
    "    # Add text\n",
    "    font=cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "    stop_pose_label = stop_pose_preds[i]\n",
    "    stop_pose_label = 'Yes' if stop_pose_label == 1 else 'No'\n",
    "    \n",
    "    frame_label = image_labels[i][0]\n",
    "    if frame_label == -1:\n",
    "        frame_label = '<ignored sequence>'\n",
    "    else:\n",
    "        frame_label = label_map[image_labels[i][0]]\n",
    "    \n",
    "    cv2.putText(new_image, f\"Frame: {i}/{len(images)}\", (250,900), font, 5, (255,255,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(new_image, f\"FPS: {VIDEO_OUTPUT_FPS}\", (250,975), font, 5, (255,255,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(new_image,f\"Stop pose: {stop_pose_label}\", (250,1200), font, 5, (255,255,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(new_image, f\"{frame_label}\", (250,1300), font, 5, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    # Write frame to video\n",
    "    video.write(np.uint8(new_image))\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "521it [00:26, 19.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Video ensemble based on rendered images with subtitles\n",
    "# Set output video params\n",
    "video_output_dir = os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, TEMP_VIDEO_RENDERED_FOLDER_NAME)\n",
    "fps = 10\n",
    "\n",
    "# Load list of rendered images\n",
    "rendered_images_dir_path = os.path.join(OUTPUT_FOLDER, TEMP_BASE_FOLDER_NAME, video_file_name, 'rendered_images')\n",
    "rendered_images_paths = glob.glob(f'{rendered_images_dir_path}/*.png')\n",
    "rendered_images_paths.sort()\n",
    "    \n",
    "df_frames = pd.DataFrame(data=rendered_images_paths, columns=['rendered_image_path'])\n",
    "\n",
    "# Generate video for each sequence\n",
    "video_name = f'{video_file_name}_skeleton.avi'\n",
    "video_path = os.path.join(os.path.realpath(video_output_dir), video_name)\n",
    "\n",
    "images = df_frames['rendered_image_path'].to_numpy().tolist()\n",
    "frame = cv2.imread(images[0])\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(video_path, 0, fps, (width,height))\n",
    "\n",
    "for i, image_path in tqdm(enumerate(images)):\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Write frame to video\n",
    "    video.write(image)\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
